import random
"""This module deals with the neuron structure and functions"""
import time
from pathlib import Path
from typing import List, Optional, Union
from dataclasses import dataclass
import imageio.v2 as iio
import os
import yaml
from neurorosettes.physics import normalize_vector  # This import is causing issues
import numpy as np
from vedo import ProgressBar, merge, Cylinder, write
from vedo import Plotter  # If using vedo
from vedo import Line, Plotter as VedoPlotter 

from ripser import ripser
from persim import plot_diagrams
import numpy as np
from ripser import ripser
from persim import plot_diagrams
import numpy as np

plotter = Plotter()
from vedo import Plotter as VedoPlotter, Line, Axes, Box

class Plotter:
    def __init__(self):
        self.vp = VedoPlotter()
        self.lines = []  # Store lines for later rendering

        # Add 3D coordinate axes
        axes = Axes(self.vp, xtitle='X', ytitle='Y', ztitle='Z')
        self.vp += axes

        # Optional: add a light bounding box
        bounding_box = Box(pos=(0, 0, 0), length=150, width=150, height=2, c="lightgray", alpha=0.2)
        self.vp += bounding_box

    def addLine(self, start, end, c='red'):
        self.lines.append({'start': start, 'end': end, 'color': c})
        print(f"üñçÔ∏è Added line from {start} to {end} with color {c}")

    def render(self):
        for line_data in self.lines:
            start = line_data['start']
            end = line_data['end']
            color = line_data['color']
            self.vp += Line(start, end, c=color, lw=40)
        self.vp.show(interactive=True)

# class Plotter:
#     def __init__(self):
#         self.lines = []  # Store lines for visualization

#     def addLine(self, start, end, c='red'):
#         """Adds a line to the visualization from start to end."""
#         self.lines.append({'start': start, 'end': end, 'color': c})
#         print(f"üñçÔ∏è Added line from {start} to {end} with color {c}")
#     def render(self):
#         for line_data in self.lines:
#             start = line_data['start']
#             end = line_data['end']
#             color = line_data['color']
#             self.vp += Line(start, end, c=color, lw=40)  # Add line to the scene
#         self.vp.show(interactive=True)

import numpy as np

def align_cells_to_lumen(self, neurons):
    """
    Forces cells into a **ring structure**, ensuring elongated axes face inward.
    """
    cluster_center = np.mean([n.cell.position for n in neurons], axis=0)

    for neuron in neurons:
        direction_to_center = normalize_vector(cluster_center - neuron.cell.position)
        if np.linalg.norm(direction_to_center) == 0:
            print(f"‚ö†Ô∏è Warning: Neuron {neuron} is at the exact cluster center, skipping normalization.")
            continue
        direction_to_center = normalize_vector(direction_to_center)

        # üîÑ **Enforce orientation & elongation in correct direction**
        neuron.cell.sphere.orientation(direction_to_center)
        neuron.cell.sphere.scale([1.8, 0.6, 1.0])  # **Stronger elongation for lumen clarity**

def get_lumen_contributors(self):
    """
    Filters out only the neurons **directly involved in lumen formation**.
    Removes excess outer cells.
    """
    cluster_center = np.mean([n.cell.position for n in self.neurons], axis=0)
    lumen_neurons = []

    for neuron in self.neurons:
        distance = np.linalg.norm(neuron.cell.position - cluster_center)

        # ‚úÖ **Only keep neurons within the correct ring distance**
        if 5.0 < distance < 15.0:  
            lumen_neurons.append(neuron)

    return lumen_neurons
def compute_persistent_holes(neurons, maxdim=1):
    from ripser import ripser
    from persim import plot_diagrams
    import numpy as np
    from ripser import ripser
    from persim import plot_diagrams
    import numpy as np

    """
    Computes persistent homology (H0, H1) from 2D projections of neuron positions.

    Parameters:
    - neurons: list of neuron objects with .cell.position
    - maxdim: max dimension of homology to compute (default=1)

    Returns:
    - diagrams: list of persistence diagrams (H0 and H1)
    """
    points = np.array([n.cell.position[:2] for n in neurons])  # Project to 2D
    diagrams = ripser(points, maxdim=maxdim)['dgms']
    return diagrams

def detect_rosette_structure(diagrams, hole_threshold=0.5):
    """
    Detects rosette-like features based on persistent H1 holes with long lifetimes.

    Parameters:
    - diagrams: list of persistence diagrams from ripser
    - hole_threshold: minimum lifetime of a hole to be considered persistent

    Returns:
    - (bool, int): whether a rosette is detected, and count of persistent holes
    """
    holes = diagrams[1]  # H1: 1D holes
    persistent_holes = [d for d in holes if (d[1] - d[0]) > hole_threshold]
    return len(persistent_holes) > 0, len(persistent_holes)

def analyze_and_visualize_rosette(neurons, visualize=False):
    """
    Runs persistent homology on neuron layout and optionally displays diagram.

    Parameters:
    - neurons: list of neuron objects
    - visualize: if True, show persistence diagram

    Returns:
    - rosette_detected (bool)
    - hole_count (int)
    """
    diagrams = compute_persistent_holes(neurons)
    rosette_detected, hole_count = detect_rosette_structure(diagrams)
    if visualize:
        plot_diagrams(diagrams, show=True)
    return rosette_detected, hole_count
def compute_clustering_coefficient(neurons):
    """Measures clustering of neurons: higher = more packed, lower = more spread."""

    if not neurons:  # üèóÔ∏è If neuron list is empty, return default clustering value
        print("‚ö†Ô∏è Warning: No neurons found. Returning default clustering coefficient = 0.")
        return 0.0  # Default clustering coefficient when no data

    # Convert neuron positions to a NumPy array
    positions = np.array([n.cell.position for n in neurons])

    # Ensure positions is a valid (N,3) shape
    if positions.shape[0] == 0 or positions.shape[1] != 3:
        print(f"‚ö†Ô∏è Warning: Expected positions to have shape (N, 3), but got {positions.shape}. Returning 0.")
        return 0.0

    # ‚úÖ Compute pairwise distances correctly
    distances = np.linalg.norm(positions[:, None, :] - positions[None, :, :], axis=2)  # (N, N) distance matrix
    
    avg_distance = np.mean(distances[distances > 0])  # Ignore self-distance
    return 1 / (1 + avg_distance)  # Normalized inverse distance
def compute_cluster_centroid(cluster):
    """Returns the centroid of a cluster of neurons."""
    positions = np.array([n.cell.position for n in cluster])
    return np.mean(positions, axis=0)

def compute_average_cluster_coefficient(clusters, repulsion_threshold=0.200, repulsion_strength=1.05, attraction_strength = 0.7):
    """
    Computes the average clustering coefficient across multiple clusters.
    
    Parameters
    ----------
    clusters : List[List[Neuron]]
        A list of clusters, where each cluster is a list of Neuron objects.

    Returns
    -------
    float
        The average clustering coefficient across all non-empty clusters.
    """
    if not clusters:
        print("‚ö†Ô∏è No clusters found. Returning 0.")
        return 0.0

    valid_coefficients = []
    for i, cluster in enumerate(clusters):
        if not cluster:
            continue
        coef = compute_clustering_coefficient(cluster)
        for i, cluster in enumerate(clusters):
            if not cluster:
                continue

            coef = compute_clustering_coefficient(cluster)

            # ‚§¥Ô∏è Redirect small or undefined clusters toward nearby lone neurons
            if len(cluster) < 3 or coef == 0.0:
                for neuron in cluster:
                    lone_neighbors = [n for c in clusters if len(c) < 3 and c != cluster for n in c
                                    if np.linalg.norm(neuron.cell.position - n.cell.position) < 15.0]
                    
                    if lone_neighbors:
                        average_target = np.mean([n.cell.position for n in lone_neighbors], axis=0)
                        direction = average_target - neuron.cell.position
                        norm = np.linalg.norm(direction)
                        if norm > 1e-3:
                            # neuron.cell.position += (direction / norm) * 0.41
                            neuron.cell.position += 0.51 * (average_target - neuron.cell.position)
                            neuron.cell.position += np.random.uniform(-0.01, 0.01, size=3)
                continue  # Skip rest of loop since cluster isn‚Äôt valid

            print(f"üìä Cluster {i+1} Clustering Coefficient: {coef:.4f}")
            valid_coefficients.append(coef)

    if not valid_coefficients:
        print("‚ö†Ô∏è No valid clusters for coefficient calculation.")
        return 0.0

    average_coef = sum(valid_coefficients) / len(valid_coefficients)
    # neuron.cell.position += np.random.uniform(-0.01, 0.01, size=3)
    print(f"üîó Average Clustering Coefficient Across Clusters: {average_coef:.4f}")
    
    # üí´ Intra-cluster attraction toward centroid
    if coef > 0.115:
        for cluster in clusters:
            centroid = compute_cluster_centroid(cluster)
            for neuron in cluster:
                to_center = neuron.cell.position - centroid
                neuron.cell.position += to_center * 2*attraction_strength

                for other in cluster:
                    if other is neuron:
                        continue
                    delta = neuron.cell.position - other.cell.position
                    dist = np.linalg.norm(delta)
                    if dist < 10.0 and dist > 1e-3:
                        neuron.cell.position += (delta / dist) * -0.01
    if coef >= repulsion_threshold:
        print("üí• Repulsion triggered due to high clustering coefficient.")

        # Compute cluster centroids
    centroids = [compute_cluster_centroid(cluster) for cluster in clusters]

    for i, cluster in enumerate(clusters):
        for j, other_centroid in enumerate(centroids):
            if i == j:
                continue
            direction = centroids[i] - other_centroid
            distance = np.linalg.norm(direction)
            if distance < 25.0 and distance > 0:  # Avoid divide-by-zero, was 15 before 10
                direction = direction / distance  # Normalize
                repulsion_vector = direction * repulsion_strength
                for neuron in cluster:
                    neuron.cell.position += repulsion_vector
                print(f"‚ÜîÔ∏è Applied repulsion to Cluster {i+1} away from Cluster {j+1}")
    return average_coef
# def compute_average_cluster_coefficient(clusters, repulsion_threshold=0.200, repulsion_strength=2.5, attraction_strength=0.5):
#     """
#     Computes the average clustering coefficient across multiple clusters.
#     Applies repulsion between nearby clusters if coefficient gets too high.
#     Applies gentle attraction toward each cluster's own center.
    
#     Parameters
#     ----------
#     clusters : List[List[Neuron]]
#         A list of clusters, where each cluster is a list of Neuron objects.
#     repulsion_threshold : float
#         Threshold above which inter-cluster repulsion is triggered.
#     repulsion_strength : float
#         Strength of repulsion (scales the vector).
#     attraction_strength : float
#         Strength of intra-cluster attraction.
        
#     Returns
#     -------
#     float
#         The average clustering coefficient across all non-empty clusters.
#     """
#     if not clusters:
#         print("‚ö†Ô∏è No clusters found. Returning 0.")
#         return 0.0

#     # üîü Limit to 10 largest clusters
#     clusters = sorted(clusters, key=len, reverse=True)[:10]

#     valid_coefficients = []
#     for i, cluster in enumerate(clusters):
#         if not cluster:
#             continue
#         coef = compute_clustering_coefficient(cluster)
#         print(f"üìä Cluster {i+1} Clustering Coefficient: {coef:.4f}")
#         valid_coefficients.append(coef)

#     if not valid_coefficients:
#         print("‚ö†Ô∏è No valid clusters for coefficient calculation.")
#         return 0.0

#     average_coef = sum(valid_coefficients) / len(valid_coefficients)
#     print(f"üîó Average Clustering Coefficient Across Clusters: {average_coef:.4f}")

#     # üí´ Intra-cluster attraction toward centroid
#     for cluster in clusters:
#         centroid = compute_cluster_centroid(cluster)
#         for neuron in cluster:
#             to_center = centroid - neuron.cell.position
#             neuron.cell.position += to_center * attraction_strength

#     # üí• Inter-cluster repulsion if clustering is too high
#     if average_coef >= repulsion_threshold:
#         print("üí• Repulsion triggered due to high clustering coefficient.")

#         centroids = [compute_cluster_centroid(cluster) for cluster in clusters]

#         for i, cluster in enumerate(clusters):
#             for j, other_centroid in enumerate(centroids):
#                 if i == j:
#                     continue
#                 direction = centroids[i] - other_centroid
#                 distance = np.linalg.norm(direction)
#                 if distance < 20.0 and distance > 1e-3:
#                     repulsion_vector = (direction / distance) * repulsion_strength
#                     for neuron in cluster:
#                         neuron.cell.position += repulsion_vector
#                     print(f"‚ÜîÔ∏è Applied repulsion to Cluster {i+1} away from Cluster {j+1}")

#     return average_coef

def compute_nearest_neighbor_variance(neurons):
    """Measures uniformity of spacing: higher = more uneven spacing, lower = ideal spacing."""

    if not neurons:  # üèóÔ∏è If neuron list is empty, return default variance
        print("‚ö†Ô∏è Warning: No neurons found. Returning default nearest neighbor variance = 0.")
        return 0.0  # Default variance when no data

    positions = np.array([n.cell.position for n in neurons])

    if positions.shape[0] == 0 or positions.shape[1] != 3:
        print(f"‚ö†Ô∏è Warning: Expected positions to have shape (N, 3), but got {positions.shape}. Returning 0.")
        return 0.0

    # ‚úÖ Compute pairwise distances correctly
    distances = np.linalg.norm(positions[:, None, :] - positions[None, :, :], axis=2)  # (N, N) distance matrix

    nearest_distances = np.sort(distances, axis=1)[:, 1]  # Exclude self-distance
    return np.var(nearest_distances)  # Variance of spacing

def compute_orientation_entropy(neurons):
    """Measures randomness in cell orientation: lower entropy = better polarization."""
    orientations = np.array([n.outgrowth_axis for n in neurons])
    dot_products = np.dot(orientations, orientations.T)
    alignment_scores = np.mean(dot_products)
    entropy = -alignment_scores * np.log2(alignment_scores + 1e-9)  # Avoid log(0)
    return entropy

def compute_aspect_ratio(neurons):
    """Measures cell elongation."""
    
    if not neurons:  # üèóÔ∏è Handle empty neuron lists
        print("‚ö†Ô∏è Warning: No neurons found. Returning default aspect ratio = 1.0")
        return 1.0  # Default aspect ratio (round cells)

    scales = np.array([n.cell.sphere.scale() for n in neurons])

    # ‚úÖ Ensure scales is at least 2D
    if scales.ndim == 1:
        print(f"‚ö†Ô∏è Warning: Expected (N, 3) array, but got {scales.shape}. Reshaping...")
        scales = scales.reshape(-1, 3)  # Convert (N,) to (N,3)

    return np.mean(scales[:, 0] / scales[:, 1])  # X-axis elongation relative to Y

def compute_radial_symmetry(neurons):
    """Measures how well cells form a circular rosette around a lumen."""
    
    if not neurons:  # üèóÔ∏è Handle empty neuron lists
        print("‚ö†Ô∏è Warning: No neurons found. Returning default radial symmetry = 0.")
        return 0.0  

    positions = np.array([n.cell.position for n in neurons])

    # ‚úÖ Ensure positions is at least 2D (N,3)
    if positions.ndim == 1:
        print(f"‚ö†Ô∏è Warning: Expected (N, 3) array, but got {positions.shape}. Reshaping...")
        positions = positions.reshape(-1, 3)  # Convert (N,) to (N,3)

    center = np.mean(positions, axis=0)  # Compute cluster center
    distances = np.linalg.norm(positions - center, axis=1)  # Compute radial distances

    return np.std(distances)  # Lower std means better symmetry

# from neurorosettes.config import ConfigParser
# from neurorosettes.clocks import ClocksFactory
# from neurorosettes.physics import (
#     ContactFactory,
#     PotentialsFactory,
#     SimpleFactory,
#     get_cylinder_intersection,
# )
# from neurorosettes.subcellular import CellBody, Neurite, ObjectFactory
# from neurorosettes.neurons import Neuron, NeuronFactory
# from neurorosettes.utilities import Animator, get_random_unit_vector
# from neurorosettes.grid import UniformGrid, CellDensityCheck

from config import ConfigParser
from clocks import ClocksFactory
from physics import (
    ContactFactory,
    PotentialsFactory,
    SimpleFactory,
    get_cylinder_intersection,
)
from subcellular import CellBody, Neurite, ObjectFactory
from neurons import Neuron, NeuronFactory
from utilities import Animator, get_random_unit_vector
from grid import UniformGrid, CellDensityCheck


def ccw(A, B, C):
    return (C[1] - A[1]) * (B[0] - A[0]) > (B[1] - A[1]) * (C[0] - A[0])


# Return true if line segments AB and CD intersect
def intersect(A, B, C, D):
    return ccw(A, C, D) != ccw(B, C, D) and ccw(A, B, C) != ccw(A, B, D)


@dataclass
class Timer:
    """Class to store the simulation time data."""

    total_time: float
    """The total time of a simulation (in minutes)."""
    step: float = 0.3
    """The time between simulation points (in minutes)."""
    current_time: float = 0.0
    """The current time point of the simulation."""

    def get_progress_bar(self) -> ProgressBar:
        """Returns a progress bar with the simulation time"""
        return ProgressBar(0, self.total_time / self.step, c="r")


class SimulationContainer:
    """
    Class that represents the environment where neurons exist.

    Parameters
    ----------
    grid
        The grid where simulation objects will be stored,
        to improve neighbor interactions.
    simulation_2d
        If the simulation is 2D or 3D.
    neuron_factory
        The factory object to be used to create new neurons.
    contact_factory
        The factory object to be used to create interactions.
    drag_coefficient
        The drag coefficient of the extracellular space.
    density_check
        Optional contact inhibition function to inhibit proliferation
        when the cell density is too high.
    """
    def __init__(
        self,
        grid: UniformGrid,
        simulation_2d: bool,
        neuron_factory: NeuronFactory,
        contact_factory: ContactFactory,
        timer: Timer,
        drag_coefficient: float = 10.0,
        density_check: Optional[CellDensityCheck] = None,
        plotter=None
    ) -> None:
        # Existing initialization code
        self.dynamic_proliferation_rate = 1.5
        self.timer = timer
        self.grid = grid
        self.simulation_2d = simulation_2d
        self.sphere_int = contact_factory.get_sphere_sphere_interactions()
        self.sphere_cylinder_int = contact_factory.get_sphere_cylinder_interactions()
        self.cylinder_int = contact_factory.get_cylinder_cylinder_interactions()
        self.neuron_factory = neuron_factory
        self.contact_factory = contact_factory
        self.object_factory = self.neuron_factory.objects_factory
        self.drag_coefficient = drag_coefficient
        self.density_check = density_check
        self.animator = Animator()
        from vedo import Axes
        print(f"üß≠ Plotter type: {type(self.animator.plotter)}")
        axes = Axes(
            xrange=(-100, 100),
            yrange=(-100, 100),
            zrange=(-10, 10),
            xtitle='X',
            ytitle='Y',
            ztitle='Z'
        )
        self.animator.plotter += axes
        self.animator.set_camera(height=500)  # Try larger height
        self.neurons = []
        self.rosette_formed = False  # Add a flag to track rosette formation
        self.indicator_color = "blue"  # Initial color for indicator
        self.locked_clusters = set()
        self.clustering_complete = False
        self.constriction_complete = False
        self.polarization_complete = False  # Tracks if clustering phase has completed
        self.constriction_complete = False
        self.polarization_complete = False
        self.elongation_complete = False 
        self.lumenation_complete = False # üõ†Ô∏è Initialize elongation tracking!
        self.plotter = plotter
        self.final_expansion_clusters = []
        self.final_expansion_progress = {}
        self.final_expansion_started = False
        self.scaling_cluster_index = 0  # Track which cluster to scale next
        self.scaling_cluster_index = 0
        self.last_scaling_time = 0.0
        self.has_scaled = False  # Flag to cap elongation




        

        if self.simulation_2d:
            self.animator.add_grid(
                self.grid.representation_grid_values,
                self.grid.representation_grid_values,
            )
    def remove_neurites(self):
            """
            Removes all neurites from neurons and clears their visual representation.
            """
            for neuron in self.neurons:
                neuron.neurites.clear()  # Remove neurites from neuron structure

            # Remove all springs visually (assuming self.springs stores neurite representations)
            for spring in self.animator.springs:
                self.animator.plotter.remove(spring)

            # Clear the stored springs list to prevent further references
            self.animator.springs.clear()

            print("Neurites removed.")  # Debugging
            
  
    def apply_clustering_force(self, clustering_strength=0.01, attraction_probability=0.3, resistance_probability=0.23):
            """
            Moves neurons toward the cluster center with a **chance** of getting pulled in.
            
            Parameters:
            - clustering_strength: How strongly neurons move toward the center (default: slow pull).
            - attraction_probability: Chance that a neuron **starts moving** toward the cluster.
            - resistance_probability: Chance a neuron **actively resists** and stays put.
            """
            if len(self.neurons) < 2:
                return  # No clustering possible with one neuron

            cluster_center = np.mean([n.cell.position for n in self.neurons], axis=0)  # Compute cluster center

            for neuron in self.neurons:
                if np.random.rand() < resistance_probability:  # Some neurons **actively resist** clustering
                    continue  

                if np.random.rand() < attraction_probability:  # Some neurons get pulled in
                    direction_to_center = cluster_center - neuron.cell.position
                    neuron.cell.position += direction_to_center * clustering_strength  # Controlled gradual movement
                    neuron.cell.sphere.color("blue")  # Keep them **blue** while moving
    def apply_post_polarization_constriction(self, constriction_strength=0.02, attraction_probability=0.5, resistance_probability=0.15):
        """
        Gradually pulls neurons **tighter into their respective clusters** after polarization.

        - `constriction_strength`: How strongly neurons are pulled in (default: moderate pull).
        - `attraction_probability`: Chance that a neuron **starts moving** toward the cluster center.
        - `resistance_probability`: Chance a neuron **resists** and stays put.
        """

        # Identify cluster centers
        cluster_centers = self.identify_cluster_centers()

        if not cluster_centers:
            print("‚ö†Ô∏è No valid clusters detected for constriction.")
            return

        for neuron in self.neurons:
            # Determine which cluster it belongs to
            distances = [np.linalg.norm(neuron.cell.position - center) for center in cluster_centers]
            cluster_index = np.argmin(distances)
            cluster_center = cluster_centers[cluster_index]

            # Some neurons resist the constriction
            if np.random.rand() < resistance_probability:
                continue  

            # Some neurons get pulled toward the cluster center
            if np.random.rand() < attraction_probability:
                direction_to_center = cluster_center - neuron.cell.position
                neuron.cell.position += direction_to_center * constriction_strength  # Controlled movement
                neuron.cell.sphere.color("blue")  # Keep them **blue** while moving

        print("üîÑ Post-polarization constriction applied: Clusters tightening.")

    def assign_vertex_colors(self, sphere, apical_vector, apical_color, basal_color):
        """
        Manually assigns vertex colors based on their orientation relative to apical_vector.
        
        ‚úÖ Ensures **apical side is red** and **basal side is blue**.
        """
        vertices = sphere.points()  # Get all vertex positions
        colors = []
        print(f"üß¨ Vertex count: {len(vertices)} | Assigned color count: {len(colors)}")
        center = np.mean(vertices, axis=0)  # Manually compute the center
        

        for vertex in vertices:
            # Compute dot product to check alignment with apical vector
            alignment = np.dot(normalize_vector(vertex - center), apical_vector)

            # Assign color: Red if close to apical direction, Blue otherwise
            if alignment > 0:
                colors.append(apical_color)  # Apical (red)
            else:
                colors.append(basal_color)  # Basal (blue)

        print(f"üß¨ Vertex count: {len(vertices)} | Assigned color count: {len(colors)}")
        return colors
    
    def apply_continuous_repulsion(self, min_distance=3.5, strength=0.4):
        """
        Applies a repulsive force between all pairs of neurons to prevent overlap.
        Runs every timestep to keep cells separated.
        """
        for i, n1 in enumerate(self.neurons):
            for j, n2 in enumerate(self.neurons):
                if i >= j:
                    continue
                delta = n1.cell.position - n2.cell.position
                dist = np.linalg.norm(delta)
                if dist < min_distance and dist > 1e-3:
                    direction = delta / dist
                    force = strength * (min_distance - dist) * direction
                    n1.cell.position += force
                    n2.cell.position -= force

    # def apply_density_aware_clustering_force(self, clustering_strength=0.08, base_attraction_prob=0.5, resistance_probability=0.1):
    #     """
    #     Gradually attracts neurons toward denser local regions.

    #     Improvements:
    #     - Uses normalized direction
    #     - Boosts movement with scaled force from density
    #     - Makes clusters stable and tighter
    #     """
    #     if len(self.neurons) < 2:
    #         return

    #     neighbor_radius = 20.0
    #     density_map = {}

    #     for neuron in self.neurons:
    #         close_neighbors = [
    #             n for n in self.neurons 
    #             if n is not neuron and np.linalg.norm(neuron.cell.position - n.cell.position) < neighbor_radius
    #         ]
    #         density_map[neuron] = close_neighbors

    #     max_density = max(len(nlist) for nlist in density_map.values()) if density_map else 1

    #     for neuron, neighbors in density_map.items():
    #         if np.random.rand() < resistance_probability or not neighbors:
    #             continue

    #         local_density = len(neighbors)
    #         attraction_prob = min(1.0, base_attraction_prob + (local_density / max_density) ** 2 * 0.5)

    #         if np.random.rand() < attraction_prob:
    #             local_center = np.mean([n.cell.position for n in neighbors], axis=0)
    #             direction = local_center - neuron.cell.position
    #             direction = direction / (np.linalg.norm(direction) + 1e-8)  # prevent divide-by-zero
    #             step_size = clustering_strength * (local_density / max_density)
    #             neuron.cell.position += direction * step_size


    def apply_density_aware_clustering_force(self, clustering_strength=0.03, base_attraction_prob=0.4, resistance_probability=0.15):
        """
        Emphasizes repulsion more than clustering to prevent clumping.
        Gently attracts neurons to local clusters, but strongly repels close neighbors.
        """
        if len(self.neurons) < 2:
            return

        neighbor_radius = 20.0
        density_map = {}

        # Step 1: Map density
        for neuron in self.neurons:
            close_neighbors = [
                n for n in self.neurons 
                if n is not neuron and np.linalg.norm(neuron.cell.position - n.cell.position) < neighbor_radius
            ]
            density_map[neuron] = close_neighbors

        max_density = max(len(nlist) for nlist in density_map.values()) if density_map else 1

        # Step 2: Gentle clustering
        for neuron, neighbors in density_map.items():
            if np.random.rand() < resistance_probability or not neighbors:
                continue

            local_density = len(neighbors)
            attraction_prob = min(1.0, base_attraction_prob + (local_density / max_density) ** 2 * 0.5)

            if np.random.rand() < attraction_prob:
                local_center = np.mean([n.cell.position for n in neighbors], axis=0)
                direction = local_center - neuron.cell.position
                direction = direction / (np.linalg.norm(direction) + 1e-8)
                step_size = clustering_strength * (local_density / max_density)
                neuron.cell.position += direction * step_size

        # Step 3: Stronger repulsion for close neighbors
        for i, n1 in enumerate(self.neurons):
            for j, n2 in enumerate(self.neurons):
                if i >= j:
                    continue
                dist = np.linalg.norm(n1.cell.position - n2.cell.position)
                if dist < 4.5:
                    force = 0.1 * (1.0 - dist / 4.5)  # Stronger, wider radius
                    direction = n1.cell.position - n2.cell.position
                    direction = direction / (np.linalg.norm(direction) + 1e-8)
                    repulsion = direction * force
                    n1.cell.position += repulsion
                    n2.cell.position -= repulsion

    # def apply_density_aware_clustering_force(self, clustering_strength=0.5, base_attraction_prob=0.50, resistance_probability=0.25):
    #     """
    #     Moves neurons toward denser local regions to form emergent clusters from initial positions.

    #     Parameters:
    #     - clustering_strength: Base strength for attraction movement.
    #     - base_attraction_prob: Starting probability to get attracted to a dense region.
    #     - resistance_probability: Probability a neuron resists movement.
    #     """
    #     # if len(self.neurons) < 2:
    #     #     return

    #     # Compute local densities using a simple proximity-based heuristic
    #     neighbor_radius = 25.0  # Adjustable radius to define local density
    #     density_map = {}
    #     for neuron in self.neurons:
    #         close_neighbors = [n for n in self.neurons if np.linalg.norm(neuron.cell.position - n.cell.position) < neighbor_radius]
    #         density_map[neuron] = len(close_neighbors)

    #     max_density = max(density_map.values()) if density_map else 1

    #     for neuron in self.neurons:
    #         if np.random.rand() < resistance_probability:
    #             continue

    #         # Higher attraction probability in denser regions
    #         local_density = density_map[neuron]
    #         attraction_prob = base_attraction_prob + (local_density / max_density) ** 2 * 0.5  # boost probability

    #         if np.random.rand() < attraction_prob:
    #             # Move neuron toward local center of density
    #             neighbors = [n for n in self.neurons if np.linalg.norm(neuron.cell.position - n.cell.position) < neighbor_radius]
    #             if not neighbors:
    #                 continue
    #             local_center = np.mean([n.cell.position for n in neighbors], axis=0)
    #             direction = local_center - neuron.cell.position
    #             neuron.cell.position += direction * clustering_strength
    #             # neuron.cell.sphere.color("blue")
                
    def remove_outlier_neurons(self, removal_threshold=7.5, min_cluster_size=10, spread_distance=75.0):
        """
        Keeps the three largest clusters and removes all others.
        Then spreads the top 3 clusters apart in space for clarity.

        - `removal_threshold`: Optional legacy param (not used).
        - `min_cluster_size`: Minimum size for a cluster to be retained.
        - `spread_distance`: Distance between each cluster's new center.
        """
        if len(self.neurons) < 2:
            return

        clusters = self.find_clusters(distance_threshold=4.5)

        if not clusters:
            return

        # Sort clusters by descending size
        sorted_clusters = sorted(clusters, key=len, reverse=True)

        # Take only the top 3 that meet min size
        clusters_to_keep = sorted_clusters[:3]

        if not clusters_to_keep:
            print("üö® No clusters large enough to retain.")
            return

        # Flatten kept neurons
        neurons_to_keep = [n for cluster in clusters_to_keep for n in cluster]
        neurons_to_remove = [n for n in self.neurons if n not in neurons_to_keep]

        print(f"\nüîç Found {len(clusters)} clusters.")
        for i, c in enumerate(clusters_to_keep):
            print(f"‚úÖ Cluster {i+1} size: {len(c)}")
        print(f"üóë Removing {len(neurons_to_remove)} neurons outside top 3 clusters.")

        # Hide neurons that are being removed
        for neuron in neurons_to_remove:
            if hasattr(neuron.cell, "sphere"):
                neuron.cell.sphere.alpha(0)

        # üåê Spread the top 3 clusters apart
        cluster_offsets = [
            np.array([-spread_distance, 0, 0]),
            np.array([spread_distance, 0, 0]),
            np.array([0, spread_distance, 0])
        ]
        for cluster_index, cluster in enumerate(clusters_to_keep):
            offset = cluster_offsets[cluster_index]
            cluster_center = np.mean([n.cell.position for n in cluster], axis=0)
            
            for neuron in cluster:
                if not hasattr(neuron, "spread_target"):
                    neuron.spread_target = neuron.cell.position + (offset - cluster_center)

        # for cluster_index, cluster in enumerate(clusters_to_keep):
        #     offset = cluster_offsets[cluster_index]   
        #     cluster_center = np.mean([n.cell.position for n in cluster], axis=0)

        #     for neuron in cluster:
        #         neuron.cell.position += (offset - cluster_center)

        # ‚úÖ Finalize the neuron list
        self.neurons = neurons_to_keep
    def apply_spring_repulsion(self, spring_constant: float = 10.0):
        """
        For each pair of neurons, if they touch or overlap (distance <= sum of radii),
        apply a repulsive displacement along the line joining their centers.
        
        Parameters
        ----------
        spring_constant : float
            The Hookean spring constant (stiffer springs => stronger push for the same overlap).
        """
        import numpy as np

        # Iterate over all unique pairs
        n = len(self.neurons)
        for i in range(n):
            n1 = self.neurons[i]
            pos1 = np.array(n1.cell.position)
            r1   = getattr(n1, "radius", getattr(n1.cell, "radius", 4.0))

            for j in range(i+1, n):
                n2 = self.neurons[j]
                pos2 = np.array(n2.cell.position)
                r2   = getattr(n2, "radius", getattr(n2.cell, "radius", 4.0))

                # Vector between centers
                delta = pos1 - pos2
                dist  = np.linalg.norm(delta)
                if dist == 0:
                    continue  # perfectly overlapping centers‚Äîskip to avoid NaN

                # Compute overlap (positive if touching or overlapping)
                overlap = (r1 + r2) - dist
                if overlap >= 0:
                    # Normal direction from n2 ‚Üí n1
                    normal = delta / dist

                    # Hooke‚Äôs law: F = k * overlap
                    # We'll use that directly as a displacement magnitude
                    disp = spring_constant * overlap

                    # Move each cell half the distance (push apart)
                    n1.cell.position = list(pos1 + 0.5 * disp * normal)
                    n2.cell.position = list(pos2 - 0.5 * disp * normal)


    def apply_spring_dynamics(self, dt: float, spring_constant: float = 10.0):
        """
        Computes spring-based repulsive forces for overlapping neurons,
        converts them to velocities, and updates positions over time dt.

        Parameters
        ----------
        dt : float
            The simulation time step.
        spring_constant : float
            Hookean spring constant for overlap handling.
        """
        from typing import List
        import numpy as np
        # Initialize total force on each neuron
        for neuron in self.neurons:
            neuron.total_force = np.zeros(3, dtype=float)

        # Compute pairwise spring forces
        n = len(self.neurons)
        for i in range(n):
            n1 = self.neurons[i]
            pos1 = np.array(n1.cell.position, dtype=float)
            r1 = getattr(n1, 'radius', getattr(n1.cell, 'radius', 4.0))
            for j in range(i+1, n):
                n2 = self.neurons[j]
                pos2 = np.array(n2.cell.position, dtype=float)
                r2 = getattr(n2, 'radius', getattr(n2.cell, 'radius', 4.0))

                delta = pos1 - pos2
                dist = np.linalg.norm(delta)
                overlap = (r1 + r2) - dist

                if overlap > 0:
                    normal = delta / (dist + 1e-8)
                    force = spring_constant * overlap * normal
                    n1.total_force += force
                    n2.total_force -= force

        # Update velocities and positions
        for neuron in self.neurons:
            # velocity from force and drag
            v = neuron.total_force / self.drag_coefficient
            # Displacement over time dt
            displacement = v * dt
            neuron.cell.position += displacement

            # Flatten onto XY plane if running 2D
            if self.simulation_2d:
                neuron.cell.position[2] = 0.0
    # def apply_mechanical_forces(self,
    #                             dt: float,
    #                             repulsion_k: float = 10.0,
    #                             adhesion_k: float = 2.0,
    #                             adhesion_dist: float = 15.0):
    #     """
    #     Applies pairwise mechanical forces between neurons:
    #     - Repulsion when overlapping (distance < r1+r2)
    #     - Adhesion (attraction) when within adhesion_dist but not overlapping
    #     Updates velocities & positions over time dt.

    #     Parameters
    #     ----------
    #     dt : float
    #         Simulation timestep.
    #     repulsion_k : float
    #         Spring constant for repulsive overlap.
    #     adhesion_k : float
    #         Spring constant for adhesive attraction.
    #     adhesion_dist : float
    #         Maximum distance to apply adhesive force.
    #     """
    #     import numpy as np

    #     # 1Ô∏è‚É£ Initialize total_force on each neuron
    #     for neuron in self.neurons:
    #         neuron.total_force = np.zeros(3, dtype=float)

    #     # 2Ô∏è‚É£ Compute pairwise forces
    #     n = len(self.neurons)
    #     for i in range(n):
    #         n1 = self.neurons[i]
    #         p1 = np.array(n1.cell.position, dtype=float)
    #         r1 = getattr(n1, 'radius',
    #                     getattr(n1.cell, 'radius', 4.0))
    #         for j in range(i+1, n):
    #             n2 = self.neurons[j]
    #             p2 = np.array(n2.cell.position, dtype=float)
    #             r2 = getattr(n2, 'radius',
    #                         getattr(n2.cell, 'radius', 4.0))

    #             delta = p1 - p2
    #             dist = np.linalg.norm(delta)
    #             if dist == 0:
    #                 continue  # skip exact overlap of centers

    #             normal = delta / dist
    #             overlap = (r1 + r2) - dist

    #             if overlap > 0:
    #                 # Repulsive spring
    #                 force = repulsion_k * overlap * normal
    #             elif dist < adhesion_dist:
    #                 # Adhesive spring toward each other
    #                 # rest length is adhesion_dist
    #                 spring_len = dist - adhesion_dist
    #                 force = -adhesion_k * spring_len * normal
    #             else:
    #                 force = np.zeros(3)

    #             n1.total_force += force
    #             n2.total_force -= force

    #     # 3Ô∏è‚É£ Update velocities & positions
    #     for neuron in self.neurons:
    #         # v = F/drag ‚Üí displacement = v * dt
    #         v = neuron.total_force / self.drag_coefficient
    #         disp = v * dt
    #         neuron.cell.position += disp

    #         # Flatten in 2D if needed
    #         if self.simulation_2d:
    #             neuron.cell.position[2] = 0.0

    # def apply_mechanical_forces(self,
    #                             dt: float,
    #                             base_repulsion_k: float = 10.0,
    #                             base_adhesion_k: float = 2.0,
    #                             adhesion_dist: float = 15.0
    #                             ):
    #     """
    #     Combined repulsion + adhesion with strengths that adapt to current neuron count:
    #     - As neuron count ‚Üë, adhesion_k ‚Üë and repulsion_k ‚Üì.
    #     """
    #     import numpy as np

    #     # Compute dynamic spring constants
    #     ncount = len(self.neurons)
    #     # e.g. adhesion grows logarithmically, repulsion decays
    #     adhesion_k  = base_adhesion_k  * np.log1p(ncount+20)
    #     repulsion_k = base_repulsion_k / np.log1p(ncount)

    #     # Init forces
    #     for neuron in self.neurons:
    #         neuron.total_force = np.zeros(3, dtype=float)

    #     # Pairwise forces
    #     for i in range(ncount):
    #         n1 = self.neurons[i]
    #         p1 = np.array(n1.cell.position, dtype=float)
    #         r1 = getattr(n1, 'radius', getattr(n1.cell, 'radius', 4.0))
    #         for j in range(i+1, ncount):
    #             n2 = self.neurons[j]
    #             p2 = np.array(n2.cell.position, dtype=float)
    #             r2 = getattr(n2, 'radius', getattr(n2.cell, 'radius', 4.0))

    #             delta = p1 - p2
    #             dist  = np.linalg.norm(delta)
    #             if dist == 0:
    #                 continue
    #             normal  = delta / dist
    #             overlap = (r1 + r2) - dist

    #             if overlap > 0:
    #                 force = repulsion_k * overlap * normal
    #             elif dist < adhesion_dist:
    #                 spring_len = dist - adhesion_dist
    #                 force = -adhesion_k * spring_len * normal
    #             else:
    #                 force = np.zeros(3)

    #             n1.total_force += force
    #             n2.total_force -= force

    #     # Integrate velocity ‚Üí displacement
    #     for neuron in self.neurons:
    #         v = neuron.total_force / self.drag_coefficient
    #         disp = v * dt
    #         neuron.cell.position += disp
    #         if self.simulation_2d:
    #             neuron.cell.position[2] = 0.0
    # # Smoother Spring Dynamics with Damped Velocity Integration

    def apply_mechanical_forces(self,
                                dt: float,
                                base_repulsion_k: float = 10.0,
                                base_adhesion_k: float = 2.0,
                                adhesion_dist: float = 15.0,
                                damping: float = 0.8):
        """
        Applies combined repulsion/adhesion forces and integrates motion with velocity damping
        for smoother cell movements.

        Parameters
        ----------
        dt : float
            Simulation timestep.
        base_repulsion_k : float
            Base spring constant for overlap repulsion.
        base_adhesion_k : float
            Base spring constant for adhesion.
        adhesion_dist : float
            Max distance for adhesive attraction.
        damping : float in (0,1)
            Velocity retention factor (lower = more damping).
        """
        import numpy as np

        ncount = len(self.neurons)
        if ncount == 0:
            return

        # Dynamic k values
        adhesion_k  = base_adhesion_k * np.log1p(ncount)
        repulsion_k = base_repulsion_k / np.log1p(ncount)

        # Initialize forces and velocities
        for neuron in self.neurons:
            neuron.total_force = np.zeros(3, dtype=float)
            if not hasattr(neuron, 'velocity'):
                neuron.velocity = np.zeros(3, dtype=float)

        # Compute pairwise forces (repulsion + adhesion)
        for i in range(ncount):
            n1 = self.neurons[i]
            p1 = np.array(n1.cell.position, float)
            r1 = getattr(n1, 'radius', getattr(n1.cell, 'radius', 4.0))
            for j in range(i+1, ncount):
                n2 = self.neurons[j]
                p2 = np.array(n2.cell.position, float)
                r2 = getattr(n2, 'radius', getattr(n2.cell, 'radius', 4.0))

                delta = p1 - p2
                dist  = np.linalg.norm(delta)
                if dist == 0:
                    continue
                normal  = delta / dist
                overlap = (r1 + r2) - dist

                if overlap > 0:
                    force = repulsion_k * overlap * normal
                elif dist < adhesion_dist:
                    spring_len = dist - adhesion_dist
                    force = -adhesion_k * spring_len * normal
                else:
                    force = np.zeros(3)

                n1.total_force += force
                n2.total_force -= force

        # Integrate with damping
        for neuron in self.neurons:
            # acceleration
            acc = neuron.total_force / self.drag_coefficient
            # damped velocity update
            neuron.velocity = neuron.velocity * damping + acc * dt
            # position update
            neuron.cell.position += neuron.velocity * dt

            # flatten onto XY plane if 2D
            if self.simulation_2d:
                neuron.cell.position[2] = 0.0


    import numpy as np
    import random
    from scipy.spatial import cKDTree

    def replicate_cells_by_density(self,
                                    base_prob: float = 0.1,
                                    radius: float = 5.0,
                                    jitter_scale: float = 1.0):
        """
        For each neuron, compute its local density (# neighbors within `radius`),
        then give it a chance to replicate with probability p = base_prob / (1 + density).
        New cells are placed with small random jitter to avoid exact overlap.

        Parameters
        ----------
        base_prob : float
            Base replication probability when density = 0.
        radius : float
            The radius in which to count neighbors (local density).
        jitter_scale : float
            Standard deviation of the Gaussian jitter to apply to the new cell's position.
        """
        import numpy as np
        import random
        from scipy.spatial import cKDTree
        # build a KD-tree of all current neuron positions
        positions = np.array([n.cell.position for n in self.neurons])
        if len(positions) == 0:
            return

        tree = cKDTree(positions)
        new_neurons = []

        for i, neuron in enumerate(self.neurons):
            pos = neuron.cell.position
            # count all neighbors (including self) within `radius`
            idxs = tree.query_ball_point(pos, r=radius)
            # local_density = len(idxs) - 1  # subtract self

            # # adjust replication probability down as density rises
            # p = base_prob / (1 + local_density)
            local_density = len(idxs) - 1
            denom = 1.0 + local_density
            if denom <= 0:
                denom = 1.0
            p = base_prob / denom

            if random.random() < p:
                # pick a jittered position so we don't land exactly on top of each other
                jitter = np.random.normal(scale=jitter_scale, size=3)
                new_pos = pos + jitter
                if self.simulation_2d:
                    new_pos[2] = 0.0

                # create the new neuron
                new_neuron = self.create_new_neuron(new_pos)
                if new_neuron and hasattr(new_neuron.cell, "sphere"):
                    # differentiate color so you can see the newborns, for example
                    new_neuron.cell.sphere.color("green")

                new_neurons.append(new_neuron)

        print(f"üîÑ Density-based replication: created {len(new_neurons)} new neurons.")
        return new_neurons



    # def replicate_cells_by_density_positive(self, base_prob=0.05, radius=5.0):
    #     """
    #     Each neuron‚Äôs replication probability increases with local density:

    #         local_density = (neighbors_in_radius - 1) / area
    #         p = base_prob * (1 + local_density), clamped to [0,1]

    #     Args:
    #     base_prob:    probability at zero neighbors
    #     radius:       search radius for counting neighbors
    #     """
    #     # --- sanity checks ---
    #     import numpy as np, random
    #     from scipy.spatial import cKDTree
    #     if radius <= 0:
    #         raise ValueError("radius must be > 0")

    #     # build spatial index
    #     positions = np.array([n.cell.position for n in self.neurons], dtype=float)
    #     tree = cKDTree(positions)

    #     # precompute area
    #     area = np.pi * radius**2
    #     if area == 0:
    #         area = 1e-6

    #     for neuron in self.neurons:
    #         # find all neighbors (including itself)
    #         idxs = tree.query_ball_point(neuron.cell.position, r=radius)
    #         count = len(idxs) - 1  # subtract self

    #         local_density = count / area   # number per unit area

    #         # probability rises with density
    #         p = base_prob * (1 + local_density)
    #         p = min(p, 1.0)

    #         if random.random() < p:
    #             # jitter new cell around parent
    #             jitter = np.random.normal(scale=1.0, size=3)
    #             jitter[2] = 0.0 if self.simulation_2d else jitter[2]
    #             new_pos = neuron.cell.position + jitter
    #             if self.simulation_2d:
    #                 new_pos[2] = 0.0

    #             new_neuron = self.create_new_neuron(new_pos)
    #             if new_neuron and hasattr(new_neuron.cell, "sphere"):
    #                 new_neuron.cell.sphere.color("green")
    #             print(f"üÜï Replicated at {new_pos}, p={p:.3f}, density={local_density:.3f}")



    def replicate_cells_by_density_positive(self, base_prob=0.05, radius=5.0):
        """
        Each neuron‚Äôs replication probability increases with local density:

            local_density = (neighbors_in_radius - 1) / area
            p = base_prob * (1 + local_density), clamped to [0,1]

        Args:
        base_prob:    probability at zero neighbors
        radius:       search radius for counting neighbors
        """
        import numpy as np, random
        from scipy.spatial import cKDTree
        if radius <= 0:
            raise ValueError("radius must be > 0")

        # --- collect only valid 3-D positions ---
        valid_neurons = []
        valid_positions = []
        for n in self.neurons:
            pos = np.asarray(n.cell.position, dtype=float)
            if pos.shape == (3,) and np.all(np.isfinite(pos)):
                valid_neurons.append(n)
                valid_positions.append(pos)

        if not valid_positions:
            # nothing to do
            return

        # stack into an (N,3) array
        positions = np.vstack(valid_positions)

        # build the KD-tree
        tree = cKDTree(positions)

        # precompute search area
        area = np.pi * radius**2

        for neuron, pos in zip(valid_neurons, positions):
            # count neighbors (including itself)
            idxs = tree.query_ball_point(pos, r=radius)
            count = len(idxs) - 1  # subtract self

            local_density = count / area
            p = base_prob * (1 + local_density)
            p = min(p, 1.0)

            if random.random() < p:
                # place the daughter
                jitter = np.random.normal(scale=1.0, size=3)
                if self.simulation_2d:
                    jitter[2] = 0.0
                new_pos = pos + jitter
                if self.simulation_2d:
                    new_pos[2] = 0.0

                new_neuron = self.create_new_neuron(new_pos.tolist())
                if new_neuron and hasattr(new_neuron.cell, "sphere"):
                    new_neuron.cell.sphere.color("green")
                print(f"üÜï Replicated at {new_pos}, p={p:.3f}, density={local_density:.3f}")

    def apply_cluster_spread(self, spread_strength=0.61):
        """
        Gradually nudges neurons toward their assigned cluster offset.
        Call this repeatedly after remove_outlier_neurons().
        """
        spreading_done = True

        for neuron in self.neurons:
            if hasattr(neuron, "spread_target"):
                direction = neuron.spread_target - neuron.cell.position
                distance = np.linalg.norm(direction)

                if distance > 0.5:  # Still far from target
                    neuron.cell.position += direction * spread_strength
                    spreading_done = False

        if spreading_done:
            self.clusters_spread = True  # You can use this flag to stop further spreading
#9:43 PM - 3 clusters
    # def remove_outlier_neurons(self, removal_threshold=7.5, min_cluster_size=10):
    #     """
    #     Keeps the **three largest clusters** and removes all other neurons.

    #     Parameters:
    #     - removal_threshold: Optional, not used in this version.
    #     - min_cluster_size: Clusters smaller than this will be ignored entirely.
    #     """
    #     if len(self.neurons) < 2:
    #         return

    #     # üîπ Find all clusters using a stricter distance threshold
    #     clusters = self.find_clusters(distance_threshold=4.5)

    #     if not clusters:
    #         return

    #     # üîç Sort clusters by size (largest first)
    #     sorted_clusters = sorted(clusters, key=len, reverse=True)

    #     # ‚úÖ Keep the largest three clusters (if they meet the minimum size)
    #     clusters_to_keep = [cluster for cluster in sorted_clusters[:3] if len(cluster) >= min_cluster_size]

    #     if not clusters_to_keep:
    #         print("üö® No clusters large enough to retain.")
    #         return

    #     # üîó Flatten clusters_to_keep into one list
    #     neurons_to_keep = [n for cluster in clusters_to_keep for n in cluster]
    #     neurons_to_remove = [n for n in self.neurons if n not in neurons_to_keep]

    #     print(f"\nüîç Found {len(clusters)} clusters.")
    #     print(f"‚úÖ Keeping top 3 clusters with total {len(neurons_to_keep)} neurons.")
    #     print(f"üóë Removing {len(neurons_to_remove)} neurons outside the top clusters.")

    #     for neuron in neurons_to_remove:
    #         if hasattr(neuron.cell, "sphere"):
    #             neuron.cell.sphere.alpha(0)  # Hide the neuron
    #             # Optionally: self.animator.plotter.remove(neuron.cell.sphere)

    #     # ‚úÖ Update self.neurons to only include those we're keeping
    #     self.neurons = neurons_to_keep


# 9:36 PM: One cluster
    # def remove_outlier_neurons(self, removal_threshold=7.5, min_cluster_size=38):
    #     """
    #     Removes neurons that are too far from the **largest, densest cluster**.

    #     - `removal_threshold`: Distance beyond which neurons are **deleted**.
    #     - `min_cluster_size`: Minimum neurons required to **retain a cluster**.
    #     """
    #     if len(self.neurons) < 2:
    #         return  # No need to check if only one neuron exists

    #     # **Find all clusters**
    #     clusters = self.find_clusters(distance_threshold=4.5)  # üîπ Stricter clustering check

    #     if not clusters:
    #         return  # No valid clusters found, skip processing

    #     # **Find the most populated cluster**
    #     largest_cluster = max(clusters, key=len)

    #     # **Only keep the largest cluster if it's big enough**
    #     if len(largest_cluster) < min_cluster_size:
    #         print(f"üö® Largest cluster is too small ({len(largest_cluster)} neurons). No removal applied.")
    #         return  

    #     cluster_center = np.mean([n.cell.position for n in largest_cluster], axis=0)

    #     # **Identify neurons that are too far from the core cluster**
    #     neurons_to_remove = [n for n in self.neurons if n not in largest_cluster]
    #     # for neuron in self.neurons:
    #     #     distance_to_center = np.linalg.norm(neuron.cell.position - cluster_center)

    #     #     # üö® **Remove neuron if it's NOT in the largest cluster OR is too far**
    #     #     if neuron not in largest_cluster or distance_to_center > removal_threshold:
    #     #         neurons_to_remove.append(neuron)

    #     print(f"\nüîç Found {len(clusters)} clusters. Keeping largest ({len(largest_cluster)} neurons).")
    #     print(f"üóë Removing {len(neurons_to_remove)} neurons outside the main cluster.")

    #     # **Remove neurons properly**
    #     for neuron in neurons_to_remove:
    #         if hasattr(neuron.cell, "sphere"):
    #             # ‚úÖ Make the neuron invisible BEFORE removal (prevents errors)
    #             neuron.cell.sphere.alpha(0)

    #             # ‚úÖ Fully remove from visualization
    #             # self.animator.plotter.remove(neuron.cell.sphere)

    #     # ‚úÖ **Filter out removed neurons instead of `remove()`**
    #     self.neurons = largest_cluster



    def compact_clusters_spherically(self, attraction_strength=0.2, iterations=20):
        """
        Adjusts neuron positions to make clusters more spherical and compact before polarization.

        - `attraction_strength`: How strongly neurons are pulled toward their cluster centers.
        - `iterations`: Number of cycles to refine the cluster shape.
        """
        clusters = self.find_clusters(distance_threshold=4.5)
        sorted_clusters = sorted(clusters, key=len, reverse=True)
        clusters_to_keep = sorted_clusters[:3]  # Keep only the 3 largest clusters

        if len(clusters_to_keep) < 3:
            print("üö® Warning: Less than 3 clusters detected!")

        # Compute initial cluster centers
        cluster_centers = [np.mean([n.cell.position for n in cluster], axis=0) for cluster in clusters_to_keep]

        for _ in range(iterations):
            for cluster_idx, cluster in enumerate(clusters_to_keep):
                cluster_center = cluster_centers[cluster_idx]

                for neuron in cluster:
                    # Compute direction from neuron to cluster center
                    direction_to_center = cluster_center - neuron.cell.position
                    distance = np.linalg.norm(direction_to_center)

                    if distance > 0:  # Avoid division by zero
                        # Normalize the direction and apply force toward center
                        force = (direction_to_center / distance) * attraction_strength
                        neuron.cell.position += force  # Apply movement

                        # Optional: Add slight random jitter to avoid artificial grid-like packing
                        neuron.cell.position += np.random.uniform(-0.05, 0.05, size=3)

            # Recalculate cluster centers after each iteration
            cluster_centers = [np.mean([n.cell.position for n in cluster], axis=0) for cluster in clusters_to_keep]

        print("‚úÖ Clusters refined: More compact and spherical.")

    def apply_constriction(self, removal_threshold=5.0, min_cluster_size=10):
        """
        Removes neurons that are too far from the largest, densest cluster.
        
        - `removal_threshold`: Distance beyond which neurons are **deleted** from the simulation.
        - `min_cluster_size`: The **minimum** number of neurons needed for a cluster to be kept.
        """
        if len(self.neurons) < 2:
            return  # No need to check if only one neuron exists

        # **Find all clusters**
        clusters = self.find_clusters(distance_threshold=4.0)  # üîπ Make clustering stricter

        if not clusters:
            return  # No valid clusters found, skip processing

        # **Find the most populated cluster**
        largest_cluster = max(clusters, key=len)

        # **Only keep the largest cluster if it's big enough**
        if len(largest_cluster) < min_cluster_size:
            print(f"üö® Largest cluster is too small ({len(largest_cluster)} neurons). No constriction applied.")
            return  

        cluster_center = np.mean([n.cell.position for n in largest_cluster], axis=0)

        # **Identify neurons that are too far from the core cluster**
        neurons_to_remove = []
        for neuron in self.neurons:
            distance_to_center = np.linalg.norm(neuron.cell.position - cluster_center)

            # üö® **Mark neuron for removal if it's far from the core**
            if neuron not in largest_cluster or distance_to_center > removal_threshold:
                neurons_to_remove.append(neuron)

        print(f"\nüîç Found {len(clusters)} clusters. Keeping largest ({len(largest_cluster)} neurons).")
        print(f"üóë Removing {len(neurons_to_remove)} neurons that are too far.")

        # **Remove neurons outside the core cluster**
        for neuron in neurons_to_remove:
            self.neurons.remove(neuron)
            if hasattr(neuron.cell.sphere, "alpha"):  
                neuron.cell.sphere.alpha(0)  # Hide instead of deleting (avoids crashes)
            if hasattr(neuron.cell.sphere, "visible"):
                neuron.cell.sphere.visible(False)  
        
    def apply_sheet_formation(self):
        """Forces all neurons into a cohesive monolayer with minimal vertical displacement."""
        avg_z = np.mean([n.cell.position[2] for n in self.neurons])  # Find central sheet plane
        for neuron in self.neurons:
            neuron.cell.position[2] = avg_z  # Align all cells to same Z-plane
            neuron.cell.sphere.color("yellow")  # Mark sheet formation
    def apply_lateral_stretching(self):
        """Stretches the sheet by encouraging lateral expansion."""
        centroid = np.mean([n.cell.position for n in self.neurons], axis=0)
        for neuron in self.neurons:
            stretch_vector = normalize_vector(neuron.cell.position - centroid) * 0.02  # Weak expansion force
            neuron.cell.position += stretch_vector  
            neuron.cell.sphere.scale([1.2, 1.0, 1.0])  # Slight X-axis elongation

    def get_top_n_cluster_centers(self, n=10, distance_threshold=4.5):
        """
        Computes and returns the centers of the top `n` largest clusters.

        Parameters:
        - n (int): Number of top clusters to consider.
        - distance_threshold (float): Distance used to define clusters.

        Returns:
        - List[np.ndarray]: List of cluster center coordinates.
        """
        clusters = self.find_clusters(distance_threshold=distance_threshold)
        if not clusters:
            print("‚ö†Ô∏è No clusters found.")
            return []

        largest_clusters = sorted(clusters, key=len, reverse=True)[:n]
        cluster_centers = [np.mean([neuron.cell.position for neuron in cluster], axis=0) for cluster in largest_clusters]

        for idx, center in enumerate(cluster_centers):
            print(f"üìç Cluster {idx+1} center: {center}")

        return cluster_centers

    def apply_polarity_establishment(self, spread_factor=-0.12, alignment_duration=2000.0):
        """
        Establishes apical-basal polarity for each of the three clusters:
        ‚úÖ Each cluster forms a **uniform sphere** around its center.
        ‚úÖ Apical (red) side faces inward toward the cluster center.
        ‚úÖ Basal (blue) side faces outward.
        ‚úÖ Gradual orientation correction over `alignment_duration` time.
        ‚úÖ Adds slight outward spread for clarity.
        """
        current_time = self.timer.current_time

        if not hasattr(self, "polarization_start_time"):
            self.polarization_start_time = current_time  # Set the start time

        elapsed_time = current_time - self.polarization_start_time
        alignment_progress = min(elapsed_time / alignment_duration, 1.0)  # Normalize progress (0 ‚Üí 1)

        # print(f"\nüî¥ Polarization Debug: Time {current_time:.2f}s, Progress: {alignment_progress:.2%}")
        print(f"\nüî¥ Polarization Debug: Time {current_time / 550 + 1:.2f} days, Progress: {alignment_progress:.2%}")

        # **Find the three largest clusters**
        clusters = self.find_clusters(distance_threshold=4.5)
        sorted_clusters = sorted(clusters, key=len, reverse=True)[:5]
        clusters_to_keep = sorted_clusters
        # sorted_clusters = sorted(clusters, key=len, reverse=True)
        # clusters_to_keep = sorted_clusters[:3]  # Keep the three largest

        if len(clusters_to_keep) < 3:
            print(f"üö® Warning: Less than 3 clusters found! Only {len(clusters_to_keep)} will be polarized.")

        # **Compute the center of each cluster**
        cluster_centers = [np.mean([n.cell.position for n in cluster], axis=0) for cluster in clusters_to_keep]
        
        for i, center in enumerate(cluster_centers):
            print(f"üü¢ Cluster {i+1} center: {center}")

        # **Assign neurons to their respective clusters**
        neuron_cluster_map = {}
        for i, cluster in enumerate(clusters_to_keep):
            for neuron in cluster:
                neuron_cluster_map[neuron] = i  # Store which cluster this neuron belongs to

        for neuron in self.neurons:
            if neuron not in neuron_cluster_map:
                continue  # Skip neurons that are not in the top 3 clusters

            cluster_idx = neuron_cluster_map[neuron]
            cluster_center = cluster_centers[cluster_idx]

            if not hasattr(neuron, "polarized"):
                neuron.polarized = False  # Track the state of each neuron

            # üîÑ **Ensure uniform sphere distribution around cluster center**
            direction_to_center = normalize_vector(cluster_center - neuron.cell.position)
            print(f"üîÑ Neuron at {neuron.cell.position} ‚Üí Direction to cluster {cluster_idx+1}: {direction_to_center}")

            neuron.cell.position += -direction_to_center * spread_factor  # Slight outward spread
           
            apical_axis = normalize_vector(cluster_center - neuron.cell.position)
            # Assign red/blue based on that axis
            colors = self.assign_vertex_colors(neuron.cell.sphere, apical_axis, (1, 0, 0), (0, 0, 1))
            neuron.cell.sphere.pointColors(colors, alpha=1)
            print(f"‚úÖ Applied {len(colors)} vertex colors to sphere")

            # üî¥ Draw a red line showing the apical direction
            apical_tip = neuron.cell.position + apical_axis * 3
            if not hasattr(self, "plotter"):
                raise AttributeError("üö® Error: self.plotter is not defined in SimulationContainer!")

            if not hasattr(self.plotter, "addLine"):
                raise AttributeError("üö® Error: self.plotter does not have an addLine method!")

            print(f"üß© Debug: Type of self.plotter ‚Üí {type(self.plotter)}")
            if not hasattr(self.plotter, "addLine"):
                raise AttributeError(f"üö® Error: self.plotter exists but does not have an addLine method! Type: {type(self.plotter)}")

            self.plotter.addLine(neuron.cell.position, apical_tip, c='red')

            # ‚úÖ Debug
            print(f"üé® Neuron {neuron} - Forced Apical Axis: {apical_axis}")


            # üéØ **Gradually Align Apical Side Inward**
            adjusted_direction = (
                (1 - alignment_progress) * neuron.cell.sphere.orientation() +
                alignment_progress * direction_to_center
            )
            neuron.cell.sphere.orientation(adjusted_direction)

            print(f"üìè Neuron {neuron} - Adjusted Orientation: {adjusted_direction}")

            # ‚úÖ **Final Polarity Lock-in**
            if alignment_progress >= 1.0:
                neuron.polarized = True  # Lock polarization
                self.polarization_complete = True  # Mark simulation phase complete
            if self.polarization_complete:
                self.plotter.render()
                print(f"‚úÖ Polarization complete: {self.polarization_complete}")
    def apply_repulsion_until_separated(self, min_distance=4.0, repulsion_strength=1.5, max_iterations=150):
        """
        Applies pairwise repulsion forces between all neurons until all pairs are at least min_distance apart.
        """
        for iteration in range(max_iterations):
            adjustments_made = False
            for i, neuron in enumerate(self.neurons):
                for j in range(i + 1, len(self.neurons)):
                    other = self.neurons[j]
                    delta = neuron.cell.position - other.cell.position
                    dist = np.linalg.norm(delta)
                    if dist < 1e-6:
                        continue  # skip zero-distance to avoid division by zero
                    if dist < min_distance:
                        direction = delta / dist
                        move_amount = 0.5 * repulsion_strength * (min_distance - dist)  # Split force equally
                        neuron.cell.position += direction * move_amount
                        other.cell.position -= direction * move_amount
                        adjustments_made = True
            if not adjustments_made:
                print(f"‚úÖ All neuron pairs are at least {min_distance} units apart after {iteration+1} iterations.")
                break
        else:
            print(f"‚ö†Ô∏è Max iterations reached; some neurons may still be closer than {min_distance} units.")
    import numpy as np
    from scipy.spatial import cKDTree

    def apply_density_aware_repulsion(self, min_distance=6.0, base_repulsion=0.4, neighbor_threshold=5):
        """
        Applies targeted repulsion to clumped neurons by identifying dense regions.
        
        Parameters:
        - min_distance: Minimum distance neurons should maintain.
        - base_repulsion: Base force applied during repulsion.
        - neighbor_threshold: How many neighbors defines a "clumped" neuron.
        """
        import numpy as np
        from scipy.spatial import cKDTree
        if len(self.neurons) < 2:
            return

        positions = np.array([n.cell.position for n in self.neurons])
        tree = cKDTree(positions)

        for idx, neuron in enumerate(self.neurons):
            # Find nearby neighbors within min_distance
            neighbor_idxs = tree.query_ball_point(neuron.cell.position, r=min_distance)
            neighbor_idxs = [i for i in neighbor_idxs if i != idx]  # exclude self

            if len(neighbor_idxs) < neighbor_threshold:
                continue  # Not clumped, skip

            # Strength scales with local crowding
            local_strength = base_repulsion * (len(neighbor_idxs) / neighbor_threshold)

            for neighbor_idx in neighbor_idxs:
                other = self.neurons[neighbor_idx]
                delta = neuron.cell.position - other.cell.position
                dist = np.linalg.norm(delta)
                if dist < 1e-6:
                    continue
                direction = delta / dist
                force = 0.5 * local_strength * (min_distance - dist)
                neuron.cell.position += direction * force
                other.cell.position -= direction * force

    # def apply_polarity_establishment_n(self, spread_factor=-0.12, alignment_duration=5.0):
    #         """
    #         Establishes apical-basal polarity for the top 10 clusters:
    #         ‚úÖ Each cluster forms a uniform sphere around its center.
    #         ‚úÖ Apical (red) side faces inward; Basal (blue) side faces outward.
    #         ‚úÖ Gradual orientation correction over `alignment_duration`.
    #         ‚úÖ Adds slight outward spread for clarity.
    #         """
    #         current_time = self.timer.current_time

    #         if not hasattr(self, "polarization_start_time"):
    #             self.polarization_start_time = current_time

    #         elapsed_time = current_time - self.polarization_start_time
    #         alignment_progress = min(elapsed_time / alignment_duration, 1.0)
    #         # print(f"\nüî¥ Polarization Debug: Time {current_time:.2f}s, Progress: {alignment_progress:.2%}")
    #         print(f"\nüî¥ Polarization Debug: Time {current_time / 550 + 1:.2f} days, Progress: {alignment_progress:.2%}")
    #         # ‚úÖ Select the 10 largest clusters
    #         all_clusters = self.find_clusters(distance_threshold=7.5)
    #         if not all_clusters:
    #             print("‚ö†Ô∏è No clusters found.")
    #             return

    #         sorted_clusters = sorted(all_clusters, key=len, reverse=True)
    #         clusters_to_keep = sorted_clusters[:10]

    #         if len(clusters_to_keep) < 10:
    #             print(f"üö® Warning: Less than 10 clusters found! Only {len(clusters_to_keep)} will be polarized.")

    #         # ‚úÖ Compute centers of top 10 clusters
    #         cluster_centers = [np.mean([n.cell.position for n in cluster], axis=0) for cluster in clusters_to_keep]
    #         for i, center in enumerate(cluster_centers):
    #             print(f"üü¢ Cluster {i+1} center: {center}")

    #         # ‚úÖ Map neurons to their cluster
    #         # neuron_cluster_map = {}
    #         # for i, cluster in enumerate(clusters_to_keep):
    #         #     for neuron in cluster:
    #         #         neuron_cluster_map[neuron] = i
    #     # ‚úÖ Use radial proximity instead of cluster membership
    #         radius_threshold = 7.5  # Adjust as needed
    #         from scipy.spatial import cKDTree

    #         # Step 1: Build KDTree from all neuron positions
    #         positions = np.array([neuron.cell.position for neuron in self.neurons])
    #         tree = cKDTree(positions)

    #         # Step 2: Get all neuron positions in a list (preserves order for indexing)
    #         neuron_list = self.neurons

    #         # Step 3: Map neurons within radius of each cluster center
    #         neuron_cluster_map = {}

    #         for i, center in enumerate(cluster_centers):
    #             nearby_idxs = tree.query_ball_point(center, r=radius_threshold)
    #             for idx in nearby_idxs:
    #                 neuron = neuron_list[idx]
    #                 if neuron not in neuron_cluster_map:  # Only assign once (first cluster found)
    #                     neuron_cluster_map[neuron] = i


    #         # ‚úÖ Apply polarity logic
    #         for neuron in self.neurons:
    #             if neuron not in neuron_cluster_map:
    #                 continue  # Skip neurons not in top 10 clusters

    #             cluster_idx = neuron_cluster_map[neuron]
    #             cluster_center = cluster_centers[cluster_idx]

    #             if not hasattr(neuron, "polarized"):
    #                 neuron.polarized = False

    #             direction_to_center = normalize_vector(cluster_center - neuron.cell.position)
    #             neuron.cell.position += -direction_to_center * spread_factor

    #             apical_axis = normalize_vector(cluster_center - neuron.cell.position)
    #             colors = self.assign_vertex_colors(neuron.cell.sphere, apical_axis, (1, 0, 0), (0, 0, 1))
    #             neuron.cell.sphere.pointColors(colors, alpha=1)

    #             apical_tip = neuron.cell.position + apical_axis * 3
    #             if hasattr(self.plotter, "addLine"):
    #                 self.plotter.addLine(neuron.cell.position, apical_tip, c='red')

    #             adjusted_direction = (
    #                 (1 - alignment_progress) * neuron.cell.sphere.orientation() +
    #                 alignment_progress * direction_to_center
    #             )
    #             neuron.cell.sphere.orientation(adjusted_direction)

    #             if alignment_progress >= 1.0:
    #                 neuron.polarized = True
    #                 self.polarization_complete = True
    #                     # üß¨ Elongate the cell along the apical axis
    #                 if hasattr(neuron.cell, "sphere"):
    #                     # Compute elongation scale along apical-basal axis
    #                     elongation_vector = np.abs(apical_axis)  # Use absolute to avoid negative scaling
    #                     elongation_scale = 1.1  # Stretch factor

    #                     # Elongate mostly in the direction of the apical axis
    #                     scale_vector = 1 + (elongation_scale - 1) * elongation_vector
    #                     neuron.cell.sphere.scale(scale_vector.tolist())
    #                     print(f"üîµ Elongated neuron along axis {apical_axis} with scale {scale_vector}")
    #         if self.polarization_complete:
    #             print("‚úÖ Polarization complete.")
    #             self.plotter.render()

    # def apply_polarity_establishment_n(self, spread_factor=-0.12, alignment_duration=5.0):
    #     """
    #     Establishes apical-basal polarity for the top 10 clusters:
    #     ‚úÖ Each cluster forms a uniform sphere around its center.
    #     ‚úÖ Apical (red) side faces inward; Basal (blue) side faces outward.
    #     ‚úÖ Gradual orientation correction over `alignment_duration`.
    #     ‚úÖ Adds slight outward spread for clarity.
    #     """
    #     current_time = self.timer.current_time

    #     if not hasattr(self, "polarization_start_time"):
    #         self.polarization_start_time = current_time

    #     elapsed_time = current_time - self.polarization_start_time
    #     alignment_progress = min(elapsed_time / alignment_duration, 1.0)
    #     print(f"\nüî¥ Polarization Debug: Time {current_time / 550 + 1:.2f} days, Progress: {alignment_progress:.2%}")

    #     # ‚úÖ Select the 10 largest clusters
    #     all_clusters = self.find_clusters(distance_threshold=7.5)
    #     if not all_clusters:
    #         print("‚ö†Ô∏è No clusters found.")
    #         return

    #     sorted_clusters = sorted(all_clusters, key=len, reverse=True)
    #     clusters_to_keep = sorted_clusters[:10]

    #     if len(clusters_to_keep) < 10:
    #         print(f"üö® Warning: Less than 10 clusters found! Only {len(clusters_to_keep)} will be polarized.")

    #     # ‚úÖ Compute centers of top 10 clusters
    #     cluster_centers = [np.mean([n.cell.position for n in cluster], axis=0) for cluster in clusters_to_keep]
    #     for i, center in enumerate(cluster_centers):
    #         print(f"üü¢ Cluster {i+1} center: {center}")

    #     # ‚úÖ Use radial proximity instead of strict cluster membership
    #     radius_threshold = 7.5
    #     from scipy.spatial import cKDTree

    #     # Filter out invalid neurons
    #     valid_neurons = []
    #     valid_positions = []
    #     for neuron in self.neurons:
    #         pos = np.asarray(neuron.cell.position)
    #         if np.all(np.isfinite(pos)):
    #             valid_neurons.append(neuron)
    #             valid_positions.append(pos)
    #         else:
    #             print(f"‚ö†Ô∏è Skipping invalid neuron position: {pos}")

    #     tree = cKDTree(valid_positions)
    #     neuron_cluster_map = {}

    #     for i, center in enumerate(cluster_centers):
    #         nearby_idxs = tree.query_ball_point(center, r=radius_threshold)
    #         for idx in nearby_idxs:
    #             neuron = valid_neurons[idx]
    #             if neuron not in neuron_cluster_map:
    #                 neuron_cluster_map[neuron] = i

    #     # ‚úÖ Apply polarity logic
    #     for neuron in self.neurons:
    #         if neuron not in neuron_cluster_map:
    #             continue

    #         cluster_idx = neuron_cluster_map[neuron]
    #         cluster_center = cluster_centers[cluster_idx]

    #         if not hasattr(neuron, "polarized"):
    #             neuron.polarized = False

    #         direction_to_center = normalize_vector(cluster_center - neuron.cell.position)
    #         neuron.cell.position += -direction_to_center * spread_factor

    #         apical_axis = normalize_vector(cluster_center - neuron.cell.position)
    #         colors = self.assign_vertex_colors(neuron.cell.sphere, apical_axis, (1, 0, 0), (0, 0, 1))
    #         neuron.cell.sphere.pointColors(colors, alpha=1)

    #         apical_tip = neuron.cell.position + apical_axis * 3
    #         if hasattr(self.plotter, "addLine"):
    #             self.plotter.addLine(neuron.cell.position, apical_tip, c='red')

    #         adjusted_direction = (
    #             (1 - alignment_progress) * neuron.cell.sphere.orientation() +
    #             alignment_progress * direction_to_center
    #         )
    #         neuron.cell.sphere.orientation(adjusted_direction)

    #         if alignment_progress >= 1.0:
    #             neuron.polarized = True
    #             self.polarization_complete = True

    #             # üß¨ Elongate the cell along the apical axis
    #             if hasattr(neuron.cell, "sphere"):
    #                 elongation_vector = np.abs(apical_axis)
    #                 elongation_scale = 1.1
    #                 scale_vector = 1 + (elongation_scale - 1) * elongation_vector
    #                 neuron.cell.sphere.scale(scale_vector.tolist())
    #                 print(f"üîµ Elongated neuron along axis {apical_axis} with scale {scale_vector}")

    #     if self.polarization_complete:
    #         print("‚úÖ Polarization complete.")
    #         self.plotter.render()
    def apply_polarity_establishment_n(self, spread_factor=-0.12, alignment_duration=5.0):
        """
        Establishes apical-basal polarity for the top 10 clusters:
        ‚úÖ Each cluster forms a uniform sphere around its center.
        ‚úÖ Apical (red) side faces inward; Basal (blue) side faces outward.
        ‚úÖ Gradual orientation correction over `alignment_duration`.
        ‚úÖ Adds slight outward spread for clarity.
        """
        current_time = self.timer.current_time

        if not hasattr(self, "polarization_start_time"):
            self.polarization_start_time = current_time

        elapsed_time = current_time - self.polarization_start_time
        alignment_progress = min(elapsed_time / alignment_duration, 1.0)
        print(f"\nüî¥ Polarization Debug: Time {current_time / 550 + 1:.2f} days, Progress: {alignment_progress:.2%}")

        # ‚úÖ Select the 10 largest clusters
        all_clusters = self.find_clusters(distance_threshold=7.5)
        if not all_clusters:
            print("‚ö†Ô∏è No clusters found.")
            return

        sorted_clusters = sorted(all_clusters, key=len, reverse=True)
        clusters_to_keep = sorted_clusters[:10]

        if len(clusters_to_keep) < 10:
            print(f"üö® Warning: Less than 10 clusters found! Only {len(clusters_to_keep)} will be polarized.")

        # ‚úÖ Compute valid cluster centers
        cluster_centers = []
        valid_cluster_indices = []

        for i, cluster in enumerate(clusters_to_keep):
            positions = np.array([n.cell.position for n in cluster])
            if not np.all(np.isfinite(positions)):
                print(f"‚ö†Ô∏è Skipping cluster {i+1} due to invalid positions.")
                continue
            center = np.mean(positions, axis=0)
            if np.all(np.isfinite(center)):
                cluster_centers.append(center)
                valid_cluster_indices.append(i)
                print(f"üü¢ Cluster {len(cluster_centers)} center: {center}")
            else:
                print(f"‚ö†Ô∏è Skipping cluster {i+1} center due to NaN: {center}")

        # ‚úÖ Build KDTree with valid neurons only
        from scipy.spatial import cKDTree
        radius_threshold = 7.5
        valid_neurons = []
        valid_positions = []

        for neuron in self.neurons:
            pos = np.asarray(neuron.cell.position)
            if np.all(np.isfinite(pos)):
                valid_neurons.append(neuron)
                valid_positions.append(pos)
            else:
                print(f"‚ö†Ô∏è Skipping invalid neuron position: {pos}")

        if not valid_positions or not cluster_centers:
            print("‚ùå No valid neurons or centers to process.")
            return

        tree = cKDTree(valid_positions)
        neuron_cluster_map = {}

        for i, center in enumerate(cluster_centers):
            if not np.all(np.isfinite(center)):
                continue
            nearby_idxs = tree.query_ball_point(center, r=radius_threshold)
            for idx in nearby_idxs:
                neuron = valid_neurons[idx]
                if neuron not in neuron_cluster_map:
                    neuron_cluster_map[neuron] = i

        # ‚úÖ Apply polarity logic
        for neuron in neuron_cluster_map:
            cluster_idx = neuron_cluster_map[neuron]
            cluster_center = cluster_centers[cluster_idx]

            if not hasattr(neuron, "polarized"):
                neuron.polarized = False

            direction_to_center = normalize_vector(cluster_center - neuron.cell.position)
            neuron.cell.position += -direction_to_center * spread_factor

            apical_axis = normalize_vector(cluster_center - neuron.cell.position)
            colors = self.assign_vertex_colors(neuron.cell.sphere, apical_axis, (1, 0, 0), (0, 0, 1))
            neuron.cell.sphere.pointColors(colors, alpha=1)

            apical_tip = neuron.cell.position + apical_axis * 3
            if hasattr(self.plotter, "addLine"):
                self.plotter.addLine(neuron.cell.position, apical_tip, c='red')

            adjusted_direction = (
                (1 - alignment_progress) * neuron.cell.sphere.orientation() +
                alignment_progress * direction_to_center
            )
            neuron.cell.sphere.orientation(adjusted_direction)

            if alignment_progress >= 1.0:
                neuron.polarized = True
                self.polarization_complete = True

                # üß¨ Elongate the cell along the apical axis
                elongation_vector = np.abs(apical_axis)
                elongation_scale = 1.1
                scale_vector = 1 + (elongation_scale - 1) * elongation_vector
                neuron.cell.sphere.scale(scale_vector.tolist())
                print(f"üîµ Elongated neuron along axis {apical_axis} with scale {scale_vector}")

        if self.polarization_complete:
            print("‚úÖ Polarization complete.")
            self.plotter.render()
    def apply_polarity_to_nearest_100(self, spread_factor=-0.12, alignment_duration=5.0):
        """
        Applies polarity to the 100 neurons closest to each of the 10 largest clusters.
        If a neuron is selected by multiple clusters, it polarizes toward the nearest one.
        """
        current_time = self.timer.current_time
        if not hasattr(self, "polarization_start_time"):
            self.polarization_start_time = current_time

        elapsed_time = current_time - self.polarization_start_time
        alignment_progress = min(elapsed_time / alignment_duration, 1.0)
        print(f"\nüî¥ Polarization Debug: Time {current_time / 550 + 1:.2f} days, Progress: {alignment_progress:.2%}")

        all_clusters = self.find_clusters(distance_threshold=7.5)
        if not all_clusters:
            print("‚ö†Ô∏è No clusters found.")
            return

        sorted_clusters = sorted(all_clusters, key=len, reverse=True)[:10]
        # cluster_centers = [np.mean([n.cell.position for n in cluster], axis=0) for cluster in sorted_clusters]
        cluster_centers = []
        for i, cluster in enumerate(sorted_clusters):
            positions = [n.cell.position for n in cluster if np.all(np.isfinite(n.cell.position))]
            if len(positions) == 0:
                print(f"‚ö†Ô∏è Skipping cluster {i+1} due to invalid neuron positions.")
                continue
            center = np.asarray(np.mean(positions, axis=0))
            if center.shape != (3,) or not np.all(np.isfinite(center)):
                print(f"‚ö†Ô∏è Invalid cluster center shape or value: {center}")
                continue

            cluster_centers.append(center)
            print(f"üü¢ Cluster {i+1} center: {center}")

        from scipy.spatial import cKDTree
        valid_neurons, valid_positions = [], []
        for neuron in self.neurons:
            pos = np.asarray(neuron.cell.position)
            if np.all(np.isfinite(pos)):
                valid_neurons.append(neuron)
                valid_positions.append(pos)

        tree = cKDTree(valid_positions)
        neuron_to_best_center = {}

        for i, center in enumerate(cluster_centers):
            if center.shape != (3,) or not np.all(np.isfinite(center)):
                print(f"‚ö†Ô∏è Skipping invalid center: {center}")
                continue
            distances, indices = tree.query(center, k=min(200, len(valid_neurons)))
            if isinstance(indices, int):  # when only 1 point is returned
                indices = [indices]
                distances = [distances]
            for idx, dist in zip(indices, distances):
                neuron = valid_neurons[idx]
                if neuron not in neuron_to_best_center or dist < neuron_to_best_center[neuron][1]:
                    neuron_to_best_center[neuron] = (center, dist)

        # Apply polarity logic
        # for neuron, (cluster_center, _) in neuron_to_best_center.items():
        #     if not hasattr(neuron, "pre_polarization_scaled"):
        #         neuron.cell.sphere.scale([0.9, 0.9, 1.4])
        #         neuron.pre_polarization_scaled = True

        import math

        # Parameters for scale range
        elongated_scale = np.array([0.85, 0.85, 1.5])  # Most elongated
        base_scale = np.array([1.0, 1.0, 1.0])         # No elongation

        # Distance falloff sensitivity
        decay_rate = 0.5  # Smaller ‚Üí faster falloff (try 0.2‚Äì1.0)

        # Compute distance range
        all_dists = [dist for (_, dist) in neuron_to_best_center.values()]
        min_dist = min(all_dists)
        max_dist = max(all_dists)
        dist_range = max_dist - min_dist + 1e-6  # Avoid divide-by-zero

        # Apply polarity logic
        for neuron, (cluster_center, dist) in neuron_to_best_center.items():
            # Normalize distance to [0, 1]
            norm_dist = (dist - min_dist) / dist_range

            # Compute weight using Gaussian-like decay
            weight = math.exp(-decay_rate * norm_dist ** 2)  # Closer ‚Üí weight ~ 1

            # Interpolate scale
            scale_vector = base_scale * (1 - weight) + elongated_scale * weight

            if not hasattr(neuron, "pre_polarization_scaled"):
                neuron.cell.sphere.scale(scale_vector.tolist())
                neuron.pre_polarization_scaled = True

                            
            if not hasattr(neuron, "polarized"):
                neuron.polarized = False
            direction_to_center = normalize_vector(cluster_center - neuron.cell.position)
            neuron.cell.position += -direction_to_center * spread_factor

            apical_axis = normalize_vector(cluster_center - neuron.cell.position)
            colors = self.assign_vertex_colors(neuron.cell.sphere, apical_axis, (1, 0, 0), (0, 0, 1))
            neuron.cell.sphere.pointColors(colors, alpha=1)

            apical_tip = neuron.cell.position + apical_axis * 3
            if hasattr(self.plotter, "addLine"):
                self.plotter.addLine(neuron.cell.position, apical_tip, c='red')

            adjusted_direction = (
                (1 - alignment_progress) * neuron.cell.sphere.orientation() +
                alignment_progress * direction_to_center
            )
            neuron.cell.sphere.orientation(adjusted_direction)

            if alignment_progress >= 1.0:
                neuron.polarized = True
                self.polarization_complete = True
                elongation_vector = np.abs(apical_axis)
                elongation_scale = 1.1
                scale_vector = 1 + (elongation_scale - 1) * elongation_vector
                neuron.cell.sphere.scale(scale_vector.tolist())
                print(f"üîµ Elongated neuron along axis {apical_axis} with scale {scale_vector}")

                distance_to_center = np.linalg.norm(neuron.cell.position - cluster_center)
                if distance_to_center < 3.0:  # Change threshold as needed
                    push_vector = normalize_vector(neuron.cell.position - cluster_center)
                    push_strength = 1.9  # Change how far it moves outward
                    neuron.cell.position += push_vector * push_strength
                    neuron.cell.position[2] = 0.0  # üîΩ Flatten to XY plane
                    print(f"üö® Pushed neuron away from cluster center by {push_strength} units")
        if self.polarization_complete:
            print("‚úÖ Polarization complete.")
            self.plotter.render()
    # def apply_sequential_polarity(self, spread_factor=-0.12, alignment_duration=5.0):
    #     import math
    #     """
    #     Applies polarity to one cluster at a time, allowing already polarized neurons to gradually morph.
    #     """
    #     current_time = self.timer.current_time

    #     if not hasattr(self, "polarization_start_time"):
    #         self.polarization_start_time = current_time
    #     if not hasattr(self, "current_cluster_index"):
    #         self.current_cluster_index = 0

    #     elapsed_time = current_time - self.polarization_start_time
    #     clusters_to_polarize = 10
    #     duration_per_cluster = alignment_duration / clusters_to_polarize

    #     cluster_idx = int(elapsed_time / duration_per_cluster)
    #     if cluster_idx >= clusters_to_polarize:
    #         print("‚úÖ All clusters have completed polarity.")
    #         self.polarization_complete = True
    #         return

    #     if cluster_idx > self.current_cluster_index:
    #         self.current_cluster_index = cluster_idx

    #     alignment_progress = min((elapsed_time - cluster_idx * duration_per_cluster) / duration_per_cluster, 1.0)
    #     print(f"\nüî¥ Polarizing cluster {cluster_idx+1}/10, Progress: {alignment_progress:.2%}")

    #     # Get clusters and centers
    #     clusters = self.find_clusters(distance_threshold=7.5)
    #     sorted_clusters = sorted(clusters, key=len, reverse=True)[:clusters_to_polarize]
    #     cluster_centers = [
    #         np.mean([n.cell.position for n in cluster if np.all(np.isfinite(n.cell.position))], axis=0)
    #         for cluster in sorted_clusters
    #     ]

    #     # Prepare neuron ‚Üí (closest_center, distance)
    #     from scipy.spatial import cKDTree
    #     valid_neurons = [n for n in self.neurons if np.all(np.isfinite(n.cell.position))]
    #     positions = [n.cell.position for n in valid_neurons]
    #     tree = cKDTree(positions)

    #     global_map = {}
    #     for i, center in enumerate(cluster_centers):
    #         dists, idxs = tree.query(center, k=min(100, len(valid_neurons)))
    #         if isinstance(idxs, int): idxs, dists = [idxs], [dists]
    #         for idx, dist in zip(idxs, dists):
    #             neuron = valid_neurons[idx]
    #             if neuron not in global_map or dist < global_map[neuron][1]:
    #                 global_map[neuron] = (center, dist, i)

    #     elongated_scale = np.array([0.85, 0.85, 1.5])
    #     base_scale = np.array([1.0, 1.0, 1.0])
    #     decay_rate = 0.5
    #     all_dists = [v[1] for v in global_map.values()]
    #     min_d, max_d = min(all_dists), max(all_dists)
    #     dist_range = max_d - min_d + 1e-6

    #     for neuron, (center, dist, assigned_cluster) in global_map.items():
    #         norm_d = (dist - min_d) / dist_range
    #         weight = math.exp(-decay_rate * norm_d ** 2)
    #         scale_vec = base_scale * (1 - weight) + elongated_scale * weight

    #         # Apply polarity only for current cluster
    #         if assigned_cluster == cluster_idx:
    #             if not hasattr(neuron, "pre_polarization_scaled"):
    #                 neuron.cell.sphere.scale(scale_vec.tolist())
    #                 neuron.pre_polarization_scaled = True

    #             if not hasattr(neuron, "polarized"):
    #                 neuron.polarized = False

    #             dir_to_center = normalize_vector(center - neuron.cell.position)
    #             neuron.cell.position += -dir_to_center * spread_factor

    #             apical_axis = normalize_vector(center - neuron.cell.position)
    #             colors = self.assign_vertex_colors(neuron.cell.sphere, apical_axis, (1, 0, 0), (0, 0, 1))
    #             neuron.cell.sphere.pointColors(colors, alpha=1)

    #             if hasattr(self.plotter, "addLine"):
    #                 self.plotter.addLine(neuron.cell.position, neuron.cell.position + apical_axis * 3, c='red')

    #             orientation = (
    #                 (1 - alignment_progress) * neuron.cell.sphere.orientation() +
    #                 alignment_progress * dir_to_center
    #             )
    #             neuron.cell.sphere.orientation(orientation)

    #             if alignment_progress >= 1.0:
    #                 neuron.polarized = True

    #         # Gradual morphing for previously polarized neurons
    #         elif hasattr(neuron, "polarized") and neuron.polarized:
    #             refine = normalize_vector(center - neuron.cell.position)
    #             updated = 0.9 * neuron.cell.sphere.orientation() + 0.1 * refine
    #             neuron.cell.sphere.orientation(updated)

    #     if self.polarization_complete:
    #         self.plotter.render()

    from typing import List, Tuple
    import numpy as np
    from scipy.spatial import cKDTree
    def rank_cluster_centers_by_local_density(

        neurons: List, 
        cluster_centers: List[np.ndarray], 
        radius: float
        # from typing import List, Tuple
        # import numpy as np
        # from scipy.spatial import cKDTree
    ) -> List[Tuple[np.ndarray, int]]:
        """
        Given a list of neuron objects and cluster center coordinates, 
        returns the cluster centers ranked by the number of neurons 
        within `radius` of each center.

        Parameters
        ----------
        neurons : list
            List of neuron objects, each with `cell.position` as a 3D coordinate.
        cluster_centers : list of np.ndarray
            List of 3-element arrays defining cluster center coordinates.
        radius : float
            Radial distance threshold for counting nearby neurons.

        Returns
        -------
        List of (center, count) tuples sorted by count descending.
        """
        # Extract positions of all neurons
        from typing import List, Tuple
        import numpy as np
        from scipy.spatial import cKDTree
        positions = np.array([n.cell.position for n in neurons])
        # Build KD-tree for efficient neighbor queries
        tree = cKDTree(positions)

        center_counts = []
        for center in cluster_centers:
            # Find all neuron indices within radius of the center
            idxs = tree.query_ball_point(center, r=radius)
            center_counts.append((center, len(idxs)))

        # Sort centers by descending count
        center_counts.sort(key=lambda x: x[1], reverse=True)
        return center_counts

    # Example usage:
    # neurons = self.neurons
    # centers = cluster_centers  # list of np.array([x,y,z])
    # ranked = rank_cluster_centers_by_local_density(neurons, centers, radius=7.5)
    # for center, count in ranked:
    #     print(f"Center {center} has {count} cells within radius 7.5")
    # def mark_cluster_centers(self, cluster_centers):
    #     """
    #     Spawn one small red neuron at each of the given 3-D cluster_centers.
    #     Uses create_new_neuron so they show up just like your other cells.
    #     """
    #     # only do this once
    #     if getattr(self, "_centers_marked", False):
    #         return
    #     self._centers_marked = True

    #     for center in cluster_centers:
    #         # ensure we‚Äôre in the right format
    #         pos = list(center)
    #         if self.simulation_2d:
    #             pos[2] = 0.0

    #         # create it
    #         marker = self.create_new_neuron(pos)
    #         if marker and hasattr(marker.cell, "sphere"):
    #             marker.cell.sphere.color("red")           # make it bright red
    #             marker.cell.sphere.scale([2.5, 2.5, 2.5])  # shrink it down

    import numpy as np
    import math

    def mark_cluster_centers(self,
                            cluster_centers: List[np.ndarray],
                            cluster_sizes: List[int],
                            action_radius: float = 7.5):
        """
        Spawn one small red neuron at each of the given cluster_centers,
        scaling each marker between 1.0 and 1.5 according to its local density.

        Parameters
        ----------
        cluster_centers : List[np.ndarray]
            A list of 3-vectors (x,y,z) giving each cluster‚Äôs center.
        cluster_sizes : List[int]
            The number of cells in each cluster (must match cluster_centers order).
        action_radius : float
            Radius (in world units) used to compute the area = œÄ*r¬≤.
        """
        import numpy as np
        import math
        # only do this once per run
        if getattr(self, "_centers_marked", False):
            return
        self._centers_marked = True

        # compute the per-cluster densities (#cells / area)
        area = math.pi * action_radius**2
        densities = [size / area for size in cluster_sizes]

        # map densities ‚Üí [0,1]
        min_d, max_d = min(densities), max(densities)
        range_d = max_d - min_d if max_d > min_d else 1.0

        for center, dens in zip(cluster_centers, densities):
            # normalized 0..1
            w = (dens - min_d) / range_d

            # scale linearly between 1.0 (sparsest) and 1.5 (densest)
            scale_val = 1.0 + 0.5 * w

            # drop a red neuron into the world
            pos = list(center)
            if self.simulation_2d:
                pos[2] = 0.0

            marker = self.create_new_neuron(pos)
            if marker and hasattr(marker.cell, "sphere"):
                marker.cell.sphere.color("white")
                marker.cell.sphere.scale([scale_val, scale_val, scale_val])
                marker.cell.sphere.alpha(10) 
    # def mark_cluster_centers(self,
    #                         cluster_centers: List[np.ndarray],
    #                         cluster_sizes: List[int],
    #                         action_radius: float = 7.5):
    #     """
    #     Place a flat red disk at each cluster center, scaling each disk between
    #     radius 1.0 and 1.5 based on local density (#cells / œÄ¬∑r¬≤).
    #     """
    #     import math
    #     from vedo.shapes import Disk
    #     # Only do once
    #     if getattr(self, "_centers_marked", False):
    #         return
    #     self._centers_marked = True

    #     # Compute densities
    #     area = math.pi * action_radius**2
    #     densities = [sz / area for sz in cluster_sizes]
    #     min_d, max_d = min(densities), max(densities)
    #     range_d = max_d - min_d if max_d > min_d else 1.0

    #     for center, dens in zip(cluster_centers, densities):
    #         # normalize 0‚Üí1
    #         w = (dens - min_d) / range_d
    #         # radius 1.0‚Üí1.5
    #         r = 1.0 + 0.5 * w

    #         # force z=0 in 2D
    #         x, y, z = center
    #         if self.simulation_2d:
    #             z = 0.0

    #         # draw a flat disk in the XY plane
    #         #   pt: center point, r: radius,
    #         #   normal=(0,0,1): flat in XY,
    #         #   res=24: circle resolution
    #         disk = Disk(pos=(x,y,z),
    #             r=r,
    #             normal=(0,0,1),
    #             c="red",
    #             alpha=1.0,
    #             res=24)
    #         self.plotter.add(disk)
    #     # re-render once
    #     self.plotter.render()

    
    # def mark_cluster_centers(self,
    #                             cluster_centers: List[np.ndarray],
    #                             cluster_sizes: List[int],
    #                             action_radius: float = 7.5):
    #     """
    #     Draw a flat 2D circle at each cluster center, scaled from 1.0 up to 1.5
    #     according to how many cells are in that cluster.
    #     """
    #     import math

    #     # only once
    #     if getattr(self, "_centers_marked", False):
    #         return
    #     self._centers_marked = True

    #     # compute densities
    #     area = math.pi * action_radius**2
    #     densities = [size / area for size in cluster_sizes]
    #     min_d, max_d = min(densities), max(densities)
    #     span = max_d - min_d if max_d > min_d else 1.0

    #     for center, dens in zip(cluster_centers, densities):
    #         w = (dens - min_d) / span                # 0‚Üí1
    #         scale_val = 1.0 + 0.5 * w                # 1.0‚Üí1.5

    #         # project to 2D
    #         x, y, _ = center
    #         z = 0.0

    #         # draw a flat circle in the XY plane
    #         if hasattr(self.plotter, "addCircle"):
    #             self.plotter.addCircle(
    #                 pos=(x, y, z),
    #                 r=action_radius * scale_val * 0.1,   # tweak radius multiplier
    #                 normal=(0, 0, 1),
    #                 c="red",
    #                 alpha=1.0,
    #             )
    #         else:
    #             # fallback: create a flattened sphere
    #             marker = self.create_new_neuron([x, y, z])
    #             if marker and hasattr(marker.cell, "sphere"):
    #                 marker.cell.sphere.color("red")
    #                 # flatten in Z to make it a disk
    #                 marker.cell.sphere.scale([scale_val, scale_val, 0.01])
    def push_nearest_neurons_outward(self,
                                     cluster_centers: List[np.ndarray],
                                     radius: float = 5.0,
                                     max_push: float = 1.0):
        """
        After polarization, find every neuron within `radius` of each
        cluster center and push it radially outward by up to `max_push`.
        Closer neurons get pushed more, farther ones less.
        """
        from scipy.spatial import cKDTree

        # collect only the neurons with valid positions
        valid_neurons = []
        positions = []
        for n in self.neurons:
            pos = np.asarray(n.cell.position)
            if np.all(np.isfinite(pos)) and pos.shape == (3,):
                valid_neurons.append(n)
                positions.append(pos)
        if not positions:
            return

        tree = cKDTree(positions)

        for center in cluster_centers:
            # find all neurons within 'radius'
            idxs = tree.query_ball_point(center, r=radius)
            for idx in idxs:
                neuron = valid_neurons[idx]
                pos = np.asarray(neuron.cell.position)
                # compute normalized direction from center ‚Üí neuron
                offset = pos - center
                dist = np.linalg.norm(offset)
                # if dist < 1e-6:
                #     continue  # skip exact center
                direction = offset / dist

                # push amount falls off linearly: at dist=0 ‚Üí push=max_push; at dist=radius ‚Üí push=0
                push_amt = max_push * (1.0 - np.clip(dist / radius, 0.0, 1.0))

                # apply the displacement
                neuron.cell.position += direction * push_amt

                # if you're in 2D, keep z = 0
                if self.simulation_2d:
                    neuron.cell.position[2] = 0.0


    # def fan_out_cluster(self,
    #                     center: np.ndarray,
    #                     radius: float = 7.5,
    #                     max_push: float = 2.0):
    #     """
    #     After a cluster has finished polarizing, push its nearby neurons
    #     *perpendicular* to the radial vector, fanning them out.

    #     Parameters
    #     ----------
    #     center : np.ndarray, shape (3,)
    #         The XYZ position of the just‚Äêfinished cluster center.
    #     radius : float
    #         Only neurons within this distance get pushed.
    #     max_push : float
    #         The maximum distance any neuron at the very center will be moved.
    #     """
    #     from scipy.spatial import cKDTree
    #     import numpy as np
    #     # make sure center is 3‚Äêvector
    #     c = np.array(center, dtype=float)
    #     c[2] = 0.0 if self.simulation_2d else c[2]

    #     # build KD‚Äêtree of all neuron positions
    #     positions = np.array([n.cell.position for n in self.neurons])
    #     tree = cKDTree(positions)

    #     # find all neurons within `radius`
    #     idxs = tree.query_ball_point(c, r=radius)

    #     for idx in idxs:
    #         neuron = self.neurons[idx]
    #         pos = neuron.cell.position.copy()
    #         pos[2] = 0.0 if self.simulation_2d else pos[2]

    #         # radial vector from center ‚Üí neuron
    #         radial = pos - c
    #         dist = np.linalg.norm(radial)
    #         if dist == 0:
    #             continue

    #         radial_unit = radial / dist

    #         # compute a perp in the XY‚Äêplane: rotate 90¬∞
    #         perp = np.array([-radial_unit[1], radial_unit[0], 0.0])
    #         norm_perp = np.linalg.norm(perp)
    #         if norm_perp == 0:
    #             continue
    #         perp_unit = perp / norm_perp

    #         # weight fall‚Äêoff: at center weight=1, at radius weight=0
    #         weight = max(0.0, (radius - dist) / radius)

    #         # actual push amount
    #         push_amount = max_push * weight

    #         # apply the push
    #         neuron.cell.position += perp_unit * push_amount

    #     # (optional) mark that you‚Äôve fanned this center already if you only want it once:
    #     # self._fanned_clusters.add(cluster_idx)

    from scipy.spatial import cKDTree
    import numpy as np
    import math

    def fan_out_cluster(self,
                        center: np.ndarray,
                        radius: float = 7.5,
                        max_push: float = 2.0):
        """
        After a cluster has finished polarizing, push its nearby neurons
        *perpendicular* to the radial vector, fanning them out and drawing
        a little yellow line so you can see it.
        """
        from scipy.spatial import cKDTree
        import numpy as np
        import math
        # one‚Äêtime guard per center
        if not hasattr(self, "_fanned_centers"):
            self._fanned_centers = set()

        # use tuple so it‚Äôs hashable
        key = tuple(np.round(center, 3).tolist())
        if key in self._fanned_centers:
            return

        print(f"üöÄ fan_out_cluster: pushing neurons around center {center}  radius={radius}, max_push={max_push}")

        # Build a KD‚Äêtree of current neuron positions
        positions = np.array([n.cell.position for n in self.neurons])
        tree = cKDTree(positions)

        # Find all neurons within `radius` of this center
        idxs = tree.query_ball_point(center, r=radius)

        for idx in idxs:
            neuron = self.neurons[idx]
            pos = neuron.cell.position.copy()
            # flatten to 2D if needed
            if self.simulation_2d:
                pos[2] = 0.0

            # compute radial unit vector
            radial = pos - center
            dist = np.linalg.norm(radial)
            if dist < 1e-6:
                continue
            radial_u = radial / dist

            # get a perp in the XY‚Äêplane
            perp = np.array([-radial_u[1], radial_u[0], 0.0])
            perp_norm = np.linalg.norm(perp)
            if perp_norm < 1e-6:
                continue
            perp_u = perp / perp_norm

            # fall‚Äêoff weight: 1 at center ‚Üí 0 at radius
            w = max(0.0, (radius - dist) / radius)
            push_amt = max_push * w

            # actually move the neuron
            neuron.cell.position += perp_u * push_amt

            # draw a little yellow line so you see it
            if hasattr(self.plotter, "addLine"):
                start = neuron.cell.position
                end = start + perp_u * (radius * 0.2)
                self.plotter.addLine(start, end, c="yellow")

        # mark done and render
        self._fanned_centers.add(key)
        print(f"‚úÖ fan_out_cluster: done for {key}, rendered.")
        self.plotter.render()

    def apply_sequential_polarity(self, spread_factor=-11.2, alignment_duration=5.0):  
        import math
        if not hasattr(self, "_fanned_clusters"):
            self._fanned_clusters = set()
        """
        Applies polarity to one cluster at a time, allowing already polarized neurons to gradually morph.
        """
        current_time = self.timer.current_time

        if not hasattr(self, "polarization_start_time"):
            self.polarization_start_time = current_time
        if not hasattr(self, "current_cluster_index"):
            self.current_cluster_index = 0

        elapsed_time = current_time - self.polarization_start_time
        clusters_to_polarize = 10
        duration_per_cluster = alignment_duration / clusters_to_polarize

        cluster_idx = int(elapsed_time / duration_per_cluster)
        if cluster_idx >= clusters_to_polarize:
            print("‚úÖ All clusters have completed polarity.")
            self.polarization_complete = True
            return

        if cluster_idx > self.current_cluster_index:
            self.current_cluster_index = cluster_idx

        alignment_progress = min((elapsed_time - cluster_idx * duration_per_cluster) / duration_per_cluster, 1.0)
        print(f"\nüî¥ Polarizing cluster {cluster_idx+1}/10, Progress: {alignment_progress:.2%}")

        # Get clusters and centers
        clusters = self.find_clusters(distance_threshold=2.5)
        # sorted_clusters = sorted(clusters, key=len, reverse=True)[:clusters_to_polarize]
        import math

        # your action radius (same as in find_clusters)
        action_radius = 6.5
        # area of that circle
        circle_area = math.pi * action_radius**2

        # compute density = #cells / area
        clusters_with_density = [(cl, len(cl)/circle_area) for cl in clusters]
        # sort by descending density and take the top N
        sorted_clusters = [
            cl for cl, dens in sorted(clusters_with_density, key=lambda x: x[1], reverse=True)
        ][:clusters_to_polarize]

        # cluster_centers = [
        #     np.mean([n.cell.position for n in cluster if np.all(np.isfinite(n.cell.position))], axis=0)
        #     for cluster in sorted_clusters
        # ]
        cluster_centers = []
        for i, cluster in enumerate(sorted_clusters):
            pts = [n.cell.position for n in cluster
                if np.all(np.isfinite(n.cell.position)) and len(n.cell.position)==3]
            if not pts:
                # no valid positions in this cluster
                continue
            center = np.mean(pts, axis=0)
            # make sure it's really a 3-vector
            if center.shape == (3,) and np.all(np.isfinite(center)):
                cluster_centers.append(center)
            else:
                # skip any weird ones
                continue
        # cluster_sizes  = [ len(cl) for cl in sorted_clusters ]
    # #     # ‚îÄ‚îÄ VISUALIZE CLUSTER CENTERS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # #     # drop two tiny red spheres around each center so you can see them in the plotter
    # #     for center in cluster_centers:
    # #        # offsets so the markers don‚Äôt sit exactly on top of each other
    # #        for off in [np.array([2.0, 0.0, 0.0]), np.array([0.0, 2.0, 0.0])]:
    # #            marker_pos = center + off
    # #            if hasattr(self.plotter, "addSphere"):
    # #                # r is radius, c is color, alpha is opacity
    # #                 self.plotter.addSphere(marker_pos, r=1.5, c="red", alpha=1.0)
    # #  # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    #         # ‚îÄ‚îÄ SPAWN RED MARKER-NEURONS AT EACH CLUSTER CENTER ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    #         if not hasattr(self, "_cluster_center_markers"):
    #             self._cluster_center_markers = []
    #             for center in cluster_centers:
    #                 # actually create a little neuron at the exact center
    #                 marker = self.create_new_neuron(coordinates=center.tolist())
    #                 if marker and hasattr(marker.cell, "sphere"):
    #                 # color it bright red and make it small
    #                     marker.cell.sphere.color("red")
    #                     marker.cell.sphere.scale([3.5, 3.5, 3.5])
    #                 # keep track so you only do this once
    #                 self._cluster_center_markers.append(marker)
    #         # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # self.mark_cluster_centers(cluster_centers)
        ranked_centers = self.rank_cluster_centers_by_local_density

        # Prepare neuron ‚Üí (closest_center, distance)
        from scipy.spatial import cKDTree
        valid_neurons = [n for n in self.neurons if np.all(np.isfinite(n.cell.position))]
        positions = [n.cell.position for n in valid_neurons]
        tree = cKDTree(positions)

        global_map = {}
        for i, center in enumerate(cluster_centers):
            dists, idxs = tree.query(center, k=min(200, len(valid_neurons)))
            if isinstance(idxs, int): idxs, dists = [idxs], [dists]
            for idx, dist in zip(idxs, dists):
                neuron = valid_neurons[idx]
                if neuron not in global_map or dist < global_map[neuron][1]:
                    global_map[neuron] = (center, dist, i)

        elongated_scale = np.array([0.75, 0.75, 1.4]) #before was 0.85, 0.85, 1.5
        base_scale = np.array([1.0, 1.0, 1.0])
        decay_rate = 0.5
        all_dists = [v[1] for v in global_map.values()]
        min_d, max_d = min(all_dists), max(all_dists)
        dist_range = max_d - min_d + 1e-6

        for neuron, (center, dist, assigned_cluster) in global_map.items():
            norm_d = (dist - min_d) / dist_range
            weight = math.exp(-decay_rate * norm_d ** 2)
            scale_vec = base_scale * (1 - weight) + elongated_scale * weight

            # Apply polarity only for current cluster
            if assigned_cluster == cluster_idx:
                if not hasattr(neuron, "pre_polarization_scaled"):
                    neuron.cell.sphere.scale(scale_vec.tolist())
                    neuron.pre_polarization_scaled = True

                if not hasattr(neuron, "polarized"):
                    neuron.polarized = False
                
                # if alignment_progress >= 0.9 and hasattr(neuron, "polarized"):
                #     if dist <= 5.0:
                #         perp = np.array([-dir_to_center[1], dir_to_center[0], 0.0])
                #         perp = perp / np.linalg.norm(perp)  # normalize, optional

                #         orientation = (
                #             (1 - alignment_progress) * neuron.cell.sphere.orientation() +
                #             alignment_progress * perp
                #         )
                #         print(f"üöÄ Fanning out cluster ")
                #         neuron.cell.sphere.orientation(orientation)

                dir_to_center = normalize_vector(center - neuron.cell.position)
                # neuron.cell.position += -dir_to_center * spread_factor
                # 1) Compute distance from center
                dist = np.linalg.norm(neuron.cell.position - center)
                # 2) Normalize by your action radius (same as when finding clusters)
                action_radius = 8.5
                norm = np.clip(dist / action_radius, 0.0, 1.0)

                # 3) Weight spread: closer ‚Üí weight near 1; farther ‚Üí weight near 0
                weight = 1.0 - norm

                # 4) Compute actual push amount
                push = spread_factor * weight

                # 5) Apply outward
                neuron.cell.position += -dir_to_center * push

                apical_axis = normalize_vector(center - neuron.cell.position)
                colors = self.assign_vertex_colors(neuron.cell.sphere, apical_axis, (1, 0, 0), (0, 0, 1))
                neuron.cell.sphere.pointColors(colors, alpha=1)

                if hasattr(self.plotter, "addLine"):
                    self.plotter.addLine(neuron.cell.position, neuron.cell.position + apical_axis * 3, c='red')
                orientation = (
                    (1 - alignment_progress) * neuron.cell.sphere.orientation() +
                    alignment_progress * dir_to_center
                )
                neuron.cell.sphere.orientation(orientation)

                if alignment_progress >= 0.8:
                    # if dist <= 10.0:
                    #     perp = np.array([-dir_to_center[1], dir_to_center[0], 0.0])
                    #     perp = perp / np.linalg.norm(perp)  # normalize, optional

                    #     orientation = (
                    #         (1 - alignment_progress) * neuron.cell.sphere.orientation() +
                    #         alignment_progress * perp
                    #     )
                    #     print(f"üöÄ Fanning out cluster ")
                    #     neuron.cell.sphere.orientation(orientation)
                    #     neuron.cell.sphere.scale([1, 1, 1])
                        neuron.polarized = True
                # if alignment_progress >= 0.8 and dist <= 10.0:
                #     # 1) build outward unit vector
                #     outward = -dir_to_center

                #     # 2) compute a push magnitude (stronger the closer you are)
                #     #    you can tune `max_push` to your liking
                #     max_push = 200.0
                #     push_mag = max_push * (1.0 - dist/10.0)

                #     # 3) move the neuron away
                #     neuron.cell.position += outward * push_mag

                #     # 4) (optional) re‚Äêorient the sphere to face outward
                #     neuron.cell.sphere.orientation(outward)

                #     # 5) mark done
                #     neuron.polarized = True
                #     print(f"üöÄ Pushed neuron at dist={dist:.1f} out by {push_mag:.2f}")
                # if alignment_progress >= 0.8 and dist <= 10.0:
                #     # 1) 3D outward vector
                #     outward3d = -dir_to_center

                #     # 2) project into XY plane & renormalize
                #     outward_xy = np.array([outward3d[0], outward3d[1], 0.0])
                #     norm_xy = np.linalg.norm(outward_xy)
                #     if norm_xy > 1e-6:
                #         outward_xy /= norm_xy
                #     else:
                #         # fallback if exactly above/below center
                #         outward_xy = np.array([1.0, 0.0, 0.0])

                #     # 3) compute push magnitude (strongest at center, zero at 10u)
                #     max_push = 20.0
                #     push_mag = max_push * (1.0 - dist/10.0)

                #     # 4) apply only in XY
                #     neuron.cell.position += outward_xy * push_mag

                #     # 5) re‚Äêorient sphere in the XY plane
                #     neuron.cell.sphere.orientation(outward_xy)

                #     neuron.polarized = True
                #     print(f"üöÄ Pushed (XY) neuron at dist={dist:.1f} out by {push_mag:.2f}")
            # Gradual morphing for previously polarized neurons
            elif hasattr(neuron, "polarized") and neuron.polarized:
                refine = normalize_vector(center - neuron.cell.position)
                updated = 0.9 * neuron.cell.sphere.orientation() + 0.1 * refine
            #     # perp = np.array([-dir_to_center[1], dir_to_center[0], 0.0])
            #     # perp = perp / np.linalg.norm(perp)  # normalize, optional
            #     # updated = (
            #     #     (1 - alignment_progress) * neuron.cell.sphere.orientation() +
            #     #     alignment_progress * perp
            #     # )
                neuron.cell.sphere.orientation(updated)
        # if alignment_progress >= 0.8 and cluster_idx not in self._pushed_clusters:
        #     # only push around *this* cluster_center
        #     this_center = cluster_centers[cluster_idx]
        #     # push neighbors out of radius=7.5 by up to 2.0 units
        #     self.push_nearest_neurons_outward([this_center],
        #                                       radius=7.5,
        #                                       max_push=200.0)
        #     self._pushed_clusters.add(cluster_idx)
        # if alignment_progress >= 0.8 and cluster_idx not in self._fanned_clusters:
        #         this_center = cluster_centers[cluster_idx]
        #         # 1) grab only the neurons within a small fan_radius around the center
        #         print(f"üöÄ Fanning out cluster #{cluster_idx}")
        #         self.fan_out_cluster(this_center,
        #                      radius=7.5,
        #                      max_push=2.0)
        #         self._fanned_clusters.add(cluster_idx)
                # fan_radius = 13.0   # tune this to pick ‚Äújust the ring‚Äù you want
                # close_neurons = [
                #     n for n in self.neurons
                #     if np.linalg.norm(n.cell.position - this_center) <= fan_radius
                # ]

                # # 2) compute perpendicular in XY for each of those and assign
                # for neuron in close_neurons:
                #     radial = normalize_vector(this_center - neuron.cell.position)
                #     perp = np.array([-radial[1], radial[0], 0.0])
                #     neuron.cell.sphere.orientation(perp)

                # self._fanned_clusters.add(cluster_idx)
        if alignment_progress >= 0.8 and dist <= 10.0:
            # 1) 3D outward vector
            outward3d = -dir_to_center

            # 2) project into XY plane & renormalize
            outward_xy = np.array([outward3d[0], outward3d[1], 0.0])
            norm_xy = np.linalg.norm(outward_xy)
            if norm_xy > 1e-6:
                outward_xy /= norm_xy
            else:
                # fallback if exactly above/below center
                outward_xy = np.array([1.0, 0.0, 0.0])

            # 3) compute push magnitude (strongest at center, zero at 10u)
            max_push = 2.0
            push_mag = max_push * (1.0 - dist/10.0)

            # 4) apply only in XY
            neuron.cell.position += outward_xy * push_mag

            # 5) re‚Äêorient sphere in the XY plane
            neuron.cell.sphere.orientation(outward_xy)

            neuron.polarized = True
            print(f"üöÄ Pushed (XY) neuron at dist={dist:.1f} out by {push_mag:.2f}")
        if self.polarization_complete:
            # self.fan_out_cluster(this_center,
            #                 radius=7.5,
            #                 max_push=20.0)
            # self._fanned_clusters.add(cluster_idx)
            # self.mark_cluster_centers(cluster_centers, cluster_sizes, action_radius=2.5)
            self.plotter.render()
        if self.polarization_complete:
            self.plotter.render()

    # def apply_sequential_polarity(self, spread_factor=-11.2, alignment_duration=5.0):
    #     import math
    #     """
    #     Applies polarity to one cluster at a time, allowing already polarized neurons to gradually morph.
    #     """
    #     current_time = self.timer.current_time

    #     if not hasattr(self, "polarization_start_time"):
    #         self.polarization_start_time = current_time
    #     if not hasattr(self, "current_cluster_index"):
    #         self.current_cluster_index = 0

    #     elapsed_time = current_time - self.polarization_start_time
    #     clusters_to_polarize = 10
    #     duration_per_cluster = alignment_duration / clusters_to_polarize

    #     cluster_idx = int(elapsed_time / duration_per_cluster)
    #     if cluster_idx >= clusters_to_polarize:
    #         print("‚úÖ All clusters have completed polarity.")
    #         self.polarization_complete = True
    #         return

    #     if cluster_idx > self.current_cluster_index:
    #         self.current_cluster_index = cluster_idx

    #     alignment_progress = min((elapsed_time - cluster_idx * duration_per_cluster) / duration_per_cluster, 1.0)
    #     print(f"\nüî¥ Polarizing cluster {cluster_idx+1}/10, Progress: {alignment_progress:.2%}")

    #     # Get clusters and centers
    #     clusters = self.find_clusters(distance_threshold=2.5)
    #     # sorted_clusters = sorted(clusters, key=len, reverse=True)[:clusters_to_polarize]
    #     import math

    #     # your action radius (same as in find_clusters)
    #     action_radius = 8.5
    #     # area of that circle
    #     circle_area = math.pi * action_radius**2

    #     # compute density = #cells / area
    #     clusters_with_density = [(cl, len(cl)/circle_area) for cl in clusters]
    #     # sort by descending density and take the top N
    #     sorted_clusters = [
    #         cl for cl, dens in sorted(clusters_with_density, key=lambda x: x[1], reverse=True)
    #     ][:clusters_to_polarize]

    #     # cluster_centers = [
    #     #     np.mean([n.cell.position for n in cluster if np.all(np.isfinite(n.cell.position))], axis=0)
    #     #     for cluster in sorted_clusters
    #     # ]
    #     cluster_centers = []
    #     for i, cluster in enumerate(sorted_clusters):
    #         pts = [n.cell.position for n in cluster
    #             if np.all(np.isfinite(n.cell.position)) and len(n.cell.position)==3]
    #         if not pts:
    #             # no valid positions in this cluster
    #             continue
    #         center = np.mean(pts, axis=0)
    #         # make sure it's really a 3-vector
    #         if center.shape == (3,) and np.all(np.isfinite(center)):
    #             cluster_centers.append(center)
    #         else:
    #             # skip any weird ones
    #             continue
    #     cluster_sizes  = [ len(cl) for cl in sorted_clusters ]
    #     if not hasattr(self, "_marked_clusters"):
    #             self._marked_clusters = set()

    def apply_sequential_polarity_one(self, spread_factor=-11.2, alignment_duration=5.0):
        import math
        import numpy as np
        from scipy.spatial import cKDTree

        """
        Applies polarity to one cluster at a time, allowing already polarized neurons
        to gradually morph; once a cluster is fully aligned, mark its center and push
        its neighbors outward before moving on.
        """
        # ‚îÄ‚îÄ setup timing & one-time sets ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        current_time = self.timer.current_time
        if not hasattr(self, "polarization_start_time"):
            self.polarization_start_time = current_time
        if not hasattr(self, "current_cluster_index"):
            self.current_cluster_index = 0
        if not hasattr(self, "_pushed_clusters"):
            self._pushed_clusters = set()

        elapsed = current_time - self.polarization_start_time
        clusters_to_polarize = 10
        duration = alignment_duration / clusters_to_polarize
        cluster_idx = int(elapsed // duration)

        # if we‚Äôre done with all clusters, finalize & render
        if cluster_idx >= clusters_to_polarize:
            print("‚úÖ All clusters have completed polarity.")
            self.polarization_complete = True
            self.plotter.render()
            return

        alignment_progress = min((elapsed - cluster_idx * duration) / duration, 1.0)
        print(f"\nüî¥ Polarizing cluster {cluster_idx+1}/{clusters_to_polarize}, {alignment_progress:.0%}")

        # ‚îÄ‚îÄ find + rank clusters by density ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        raw = self.find_clusters(distance_threshold=7.5)
        action_radius = 7.5
        area = math.pi * action_radius**2
        dens_list = [(cl, len(cl)/area) for cl in raw]
        topN = sorted(dens_list, key=lambda x: x[1], reverse=True)[:clusters_to_polarize]
        sorted_clusters = [cl for cl,_ in topN]
        cluster_sizes   = [len(cl) for cl in sorted_clusters]
        cluster_centers = []
        for cl in sorted_clusters:
            pts = [n.cell.position for n in cl if np.all(np.isfinite(n.cell.position))]
            cluster_centers.append(np.mean(pts, axis=0))

        # ‚îÄ‚îÄ build a KD-tree of all neurons ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        valid = [n for n in self.neurons if np.all(np.isfinite(n.cell.position))]
        positions = [n.cell.position for n in valid]
        tree = cKDTree(positions)

        # map each neuron ‚Üí (closest_center, dist, cluster_idx)
        global_map = {}
        for idx, C in enumerate(cluster_centers):
            dists, idxs = tree.query(C, k=min(100, len(valid)))
            if isinstance(idxs, int):
                idxs, dists = [idxs], [dists]
            for i_n, dist in zip(idxs, dists):
                neuron = valid[i_n]
                if neuron not in global_map or dist < global_map[neuron][1]:
                    global_map[neuron] = (C, dist, idx)

        # ‚îÄ‚îÄ polarize & elongate only neurons in this one cluster_idx ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        elongated_scale = np.array([0.75, 0.75, 1.4])
        base_scale     = np.array([1.0, 1.0, 1.0])
        decay_rate     = 0.5
        all_dists      = [v[1] for v in global_map.values()]
        min_d, max_d   = min(all_dists), max(all_dists)
        span_d         = max_d - min_d + 1e-6

        for neuron, (C, dist, assigned) in global_map.items():
            if assigned != cluster_idx:
                continue

            # compute persistence weight ‚Üí scale
            norm_d = (dist - min_d) / span_d
            w = math.exp(-decay_rate * norm_d**2)
            scale_vec = base_scale * (1-w) + elongated_scale * w

            # scale sphere once
            if not hasattr(neuron, "pre_polarization_scaled"):
                neuron.cell.sphere.scale(scale_vec.tolist())
                neuron.pre_polarization_scaled = True
            if not hasattr(neuron, "polarized"):
                neuron.polarized = False

            # push outwards from center
            dir_to_C = (C - neuron.cell.position)
            dir_to_C = dir_to_C / (np.linalg.norm(dir_to_C) + 1e-6)
            neuron.cell.position += -dir_to_C * spread_factor

            # recolor & draw a guide line
            apical = (C - neuron.cell.position); apical /= (np.linalg.norm(apical)+1e-6)
            cols = self.assign_vertex_colors(neuron.cell.sphere, apical, (1,0,0),(0,0,1))
            neuron.cell.sphere.pointColors(cols, alpha=1)
            if hasattr(self.plotter, "addLine"):
                self.plotter.addLine(
                    neuron.cell.position,
                    neuron.cell.position + apical*3,
                    c="red"
                )

            # interpolate sphere orientation
            old_o = neuron.cell.sphere.orientation()
            new_o = (1-alignment_progress)*old_o + alignment_progress*dir_to_C
            neuron.cell.sphere.orientation(new_o)

            if alignment_progress >= 1.0:
                neuron.polarized = True

        # ‚îÄ‚îÄ once this cluster is fully aligned, mark & push ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if alignment_progress >= 1.0 and cluster_idx not in self._pushed_clusters:
            # drop a red ‚Äúmarker‚Äù cell at that center
            self.mark_cluster_centers(
                [ cluster_centers[cluster_idx] ],
                [ cluster_sizes  [cluster_idx] ],
                action_radius=action_radius
            )
            # push neighbors out by up to 2.0 world‚Äêunits
            self.push_nearest_neurons_outward(
                [ cluster_centers[cluster_idx] ],
                radius=action_radius,
                max_push=2.0
            )
            # immediate redraw
            self.plotter.render()
            self._pushed_clusters.add(cluster_idx)

        # ‚îÄ‚îÄ final render when all done ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if hasattr(self, "polarization_complete") and self.polarization_complete:
            self.plotter.render()


    # #     # ‚îÄ‚îÄ VISUALIZE CLUSTER CENTERS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # #     # drop two tiny red spheres around each center so you can see them in the plotter
    # #     for center in cluster_centers:
    # #        # offsets so the markers don‚Äôt sit exactly on top of each other
    # #        for off in [np.array([2.0, 0.0, 0.0]), np.array([0.0, 2.0, 0.0])]:
    # #            marker_pos = center + off
    # #            if hasattr(self.plotter, "addSphere"):
    # #                # r is radius, c is color, alpha is opacity
    # #                 self.plotter.addSphere(marker_pos, r=1.5, c="red", alpha=1.0)
    # #  # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    #         # ‚îÄ‚îÄ SPAWN RED MARKER-NEURONS AT EACH CLUSTER CENTER ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    #         if not hasattr(self, "_cluster_center_markers"):
    #             self._cluster_center_markers = []
    #             for center in cluster_centers:
    #                 # actually create a little neuron at the exact center
    #                 marker = self.create_new_neuron(coordinates=center.tolist())
    #                 if marker and hasattr(marker.cell, "sphere"):
    #                 # color it bright red and make it small
    #                     marker.cell.sphere.color("red")
    #                     marker.cell.sphere.scale([3.5, 3.5, 3.5])
    #                 # keep track so you only do this once
    #                 self._cluster_center_markers.append(marker)
    #         # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # self.mark_cluster_centers(cluster_centers)
        ranked_centers = self.rank_cluster_centers_by_local_density

        # Prepare neuron ‚Üí (closest_center, distance)
        from scipy.spatial import cKDTree
        valid_neurons = [n for n in self.neurons if np.all(np.isfinite(n.cell.position))]
        positions = [n.cell.position for n in valid_neurons]
        tree = cKDTree(positions)

        global_map = {}
        for i, center in enumerate(cluster_centers):
            dists, idxs = tree.query(center, k=min(200, len(valid_neurons)))
            if isinstance(idxs, int): idxs, dists = [idxs], [dists]
            for idx, dist in zip(idxs, dists):
                neuron = valid_neurons[idx]
                if neuron not in global_map or dist < global_map[neuron][1]:
                    global_map[neuron] = (center, dist, i)

        elongated_scale = np.array([0.75, 0.75, 1.4]) #before was 0.85, 0.85, 1.5
        base_scale = np.array([1.0, 1.0, 1.0])
        decay_rate = 0.5
        all_dists = [v[1] for v in global_map.values()]
        min_d, max_d = min(all_dists), max(all_dists)
        dist_range = max_d - min_d + 1e-6

        for neuron, (center, dist, assigned_cluster) in global_map.items():
            norm_d = (dist - min_d) / dist_range
            weight = math.exp(-decay_rate * norm_d ** 2)
            scale_vec = base_scale * (1 - weight) + elongated_scale * weight

            # Apply polarity only for current cluster
            if assigned_cluster == cluster_idx:
                if not hasattr(neuron, "pre_polarization_scaled"):
                    neuron.cell.sphere.scale(scale_vec.tolist())
                    neuron.pre_polarization_scaled = True

                if not hasattr(neuron, "polarized"):
                    neuron.polarized = False

                dir_to_center = normalize_vector(center - neuron.cell.position)
                # neuron.cell.position += -dir_to_center * spread_factor
                # 1) Compute distance from center
                dist = np.linalg.norm(neuron.cell.position - center)

                # 2) Normalize by your action radius (same as when finding clusters)
                action_radius = 7.5
                norm = np.clip(dist / action_radius, 0.0, 1.0)

                # 3) Weight spread: closer ‚Üí weight near 1; farther ‚Üí weight near 0
                weight = 1.0 - norm

                # 4) Compute actual push amount
                push = spread_factor * weight

                # 5) Apply outward
                neuron.cell.position += -dir_to_center * push

                apical_axis = normalize_vector(center - neuron.cell.position)
                colors = self.assign_vertex_colors(neuron.cell.sphere, apical_axis, (1, 0, 0), (0, 0, 1))
                neuron.cell.sphere.pointColors(colors, alpha=1)

                if hasattr(self.plotter, "addLine"):
                    self.plotter.addLine(neuron.cell.position, neuron.cell.position + apical_axis * 3, c='red')

                orientation = (
                    (1 - alignment_progress) * neuron.cell.sphere.orientation() +
                    alignment_progress * dir_to_center
                )
                neuron.cell.sphere.orientation(orientation)

                if alignment_progress >= 1.0:
                    neuron.polarized = True

            # Gradual morphing for previously polarized neurons
            elif hasattr(neuron, "polarized") and neuron.polarized:
                refine = normalize_vector(center - neuron.cell.position)
                updated = 0.9 * neuron.cell.sphere.orientation() + 0.1 * refine
                neuron.cell.sphere.orientation(updated)
        if alignment_progress >= 1.0 and cluster_idx not in self._pushed_clusters:
            # only push around *this* cluster_center
            this_center = cluster_centers[cluster_idx]
            # push neighbors out of radius=7.5 by up to 2.0 units
            self.push_nearest_neurons_outward([this_center],
                                            radius=7.5,
                                            max_push=200000.0)
            # spawn a marker at this cluster center only once
            self.mark_cluster_centers(
                [ cluster_centers[cluster_idx] ],
                [ cluster_sizes  [cluster_idx] ],
                action_radius=7.5
            )
            self._pushed_clusters.add(cluster_idx)
            self.plotter.render()
        if self.polarization_complete:
            self.plotter.render()
        # if self.polarization_complete:
        #     self.plotter.render()


    def apply_polarity_establishment_n_incremental(self, spread_factor=-0.12, alignment_duration=5.0):
        """
        Establishes apical-basal polarity for the top 10 clusters incrementally:
        ‚úÖ Each cluster gradually elongates starting at slightly different times.
        ‚úÖ Visuals are updated incrementally.
        """
        current_time = self.timer.current_time

        if not hasattr(self, "polarization_start_time"):
            self.polarization_start_time = current_time

        elapsed_time = current_time - self.polarization_start_time
        alignment_progress = min(elapsed_time / alignment_duration, 1.0)
        print(f"\nüî¥ Polarization Debug: Time {current_time:.2f}s, Progress: {alignment_progress:.2%}")

        all_clusters = self.find_clusters(distance_threshold=4.5)
        if not all_clusters:
            print("‚ö†Ô∏è No clusters found.")
            return

        sorted_clusters = sorted(all_clusters, key=len, reverse=True)
        clusters_to_keep = sorted_clusters[:10]

        if len(clusters_to_keep) < 10:
            print(f"üö® Warning: Less than 10 clusters found! Only {len(clusters_to_keep)} will be polarized.")

        cluster_centers = [np.mean([n.cell.position for n in cluster], axis=0) for cluster in clusters_to_keep]
        for i, center in enumerate(cluster_centers):
            print(f"üü¢ Cluster {i+1} center: {center}")

        neuron_cluster_map = {}
        for i, cluster in enumerate(clusters_to_keep):
            for neuron in cluster:
                neuron_cluster_map[neuron] = i

        # ‚úÖ Incremental elongation per cluster
        elongation_delays = [i * 0.3 for i in range(len(clusters_to_keep))]  # Cluster i starts at i*0.3s

        for cluster_idx, cluster in enumerate(clusters_to_keep):
            if elapsed_time < elongation_delays[cluster_idx]:
                continue  # Skip this cluster until its delay passes

            cluster_center = cluster_centers[cluster_idx]

            for neuron in cluster:
                if not hasattr(neuron, "polarized"):
                    neuron.polarized = False

                direction_to_center = normalize_vector(cluster_center - neuron.cell.position)
                neuron.cell.position += -direction_to_center * spread_factor

                apical_axis = normalize_vector(cluster_center - neuron.cell.position)
                colors = self.assign_vertex_colors(neuron.cell.sphere, apical_axis, (1, 0, 0), (0.6, 0.8, 1))
                neuron.cell.sphere.pointColors(colors, alpha=1)

                apical_tip = neuron.cell.position + apical_axis * 3
                # if hasattr(self.plotter, "addLine"):
                #     self.plotter.addLine(neuron.cell.position, apical_tip, c='red')

                adjusted_direction = (
                    (1 - alignment_progress) * neuron.cell.sphere.orientation() +
                    alignment_progress * direction_to_center
                )
                neuron.cell.sphere.orientation(adjusted_direction)

                # Elongate the cell gradually
                if hasattr(neuron.cell, "sphere"):
                    elongation_vector = np.abs(apical_axis)
                    elongation_scale = 1.5
                    scale_vector = 1 + (elongation_scale - 1) * alignment_progress * elongation_vector
                    neuron.cell.sphere.scale(scale_vector.tolist())
                    print(f"üîµ Elongating neuron in cluster {cluster_idx} with scale {scale_vector}")

        if alignment_progress >= 1.0:
            for neuron in neuron_cluster_map:
                neuron.polarized = True
            self.polarization_complete = True
            print("‚úÖ Full polarization complete.")

        # üîÅ Always update visuals every cycle
        # self.plotter.render()


    def apply_global_polarity_establishment(self, spread_factor=-0.12, alignment_duration=5.0, scale_dims=(0.7, 0.7, 2.2)):
        """
        Applies apical-basal polarity toward the global cluster center, with fixed scale dimensions.

        Parameters:
        - spread_factor: Slight outward shift to reveal apical axis visually.
        - alignment_duration: Time over which the polarity is established.
        - scale_dims: Tuple specifying exact scaling factors in (x, y, z).
        """
        current_time = self.timer.current_time
        if not hasattr(self, "global_polarization_start_time"):
            self.global_polarization_start_time = current_time

        elapsed_time = current_time - self.global_polarization_start_time
        alignment_progress = min(elapsed_time / alignment_duration, 1.0)
        print(f"\nüß≠ Global Polarity Debug: Time {current_time / 550 + 1:.2f} days, Progress: {alignment_progress:.2%}")

        if len(self.neurons) < 2:
            return

        global_center = np.mean([neuron.cell.position for neuron in self.neurons], axis=0)
        print(f"üåê Global cluster center: {global_center}")

        for neuron in self.neurons:
            if not hasattr(neuron.cell, "sphere"):
                continue

            direction_to_center = normalize_vector(global_center - neuron.cell.position)
            neuron.cell.position += -direction_to_center * spread_factor

            apical_axis = normalize_vector(global_center - neuron.cell.position)
            colors = self.assign_vertex_colors(neuron.cell.sphere, apical_axis, (1, 0, 0), (0, 0, 1))
            neuron.cell.sphere.pointColors(colors, alpha=1)

            apical_tip = neuron.cell.position + apical_axis * 3
            if hasattr(self.plotter, "addLine"):
                self.plotter.addLine(neuron.cell.position, apical_tip, c='red')

            adjusted_direction = (
                (1 - alignment_progress) * neuron.cell.sphere.orientation() +
                alignment_progress * direction_to_center
            )
            neuron.cell.sphere.orientation(adjusted_direction)

            # Direct scale dims, interpolated over time
            start_scale = [1.0, 1.0, 1.0]
            target_scale = scale_dims
            interpolated_scale = [
                start + alignment_progress * (target - start)
                for start, target in zip(start_scale, target_scale)
            ]
            neuron.cell.sphere.scale(interpolated_scale)

        if alignment_progress >= 1.0:
            self.global_polarization_complete = True
            print("‚úÖ Global polarity and elongation complete.")
            self.plotter.render()

    # def apply_global_polarity_establishment(self, spread_factor=-0.12, alignment_duration=5.0):
    #     """
    #     Establishes apical-basal polarity toward the global cluster center for all neurons:
    #     ‚úÖ Apical (red) side faces inward; Basal (blue) side faces outward.
    #     ‚úÖ Gradual orientation and elongation over `alignment_duration`.
    #     ‚úÖ Optional outward spread for spacing clarity.
    #     """

    #     current_time = self.timer.current_time

    #     if not hasattr(self, "global_polarization_start_time"):
    #         self.global_polarization_start_time = current_time

    #     elapsed_time = current_time - self.global_polarization_start_time
    #     alignment_progress = min(elapsed_time / alignment_duration, 1.0)
    #     print(f"\nüß≠ Global Polarity Debug: Time {current_time / 550 + 1:.2f} days, Progress: {alignment_progress:.2%}")

    #     if len(self.neurons) < 2:
    #         return

    #     # ‚úÖ Compute global center
    #     global_center = np.mean([neuron.cell.position for neuron in self.neurons], axis=0)
    #     print(f"üåê Global cluster center: {global_center}")

    #     for neuron in self.neurons:
    #         if not hasattr(neuron.cell, "sphere"):
    #             continue

    #         # Direction toward global center
    #         direction_to_center = normalize_vector(global_center - neuron.cell.position)
    #         neuron.cell.position += -direction_to_center * spread_factor

    #         # Apical axis is toward the global center
    #         apical_axis = normalize_vector(global_center - neuron.cell.position)
    #         colors = self.assign_vertex_colors(neuron.cell.sphere, apical_axis, (1, 0, 0), (0, 0, 1))
    #         neuron.cell.sphere.pointColors(colors, alpha=1)

    #         # Show apical tip
    #         apical_tip = neuron.cell.position + apical_axis * 3
    #         if hasattr(self.plotter, "addLine"):
    #             self.plotter.addLine(neuron.cell.position, apical_tip, c='red')

    #         # Gradual rotation toward apical axis
    #         adjusted_direction = (
    #             (1 - alignment_progress) * neuron.cell.sphere.orientation() +
    #             alignment_progress * direction_to_center
    #         )
    #         neuron.cell.sphere.orientation(adjusted_direction)

    #         # Gradual elongation over time
    #         elongation_vector = np.abs(apical_axis)
    #         elongation_scale = 1.5
    #         scale_vector = 1 + alignment_progress * (elongation_scale - 1) * elongation_vector
    #         neuron.cell.sphere.scale(scale_vector.tolist())

    #     if alignment_progress >= 1.0:
    #         self.global_polarization_complete = True
    #         print("‚úÖ Global polarity and elongation complete.")
    #         self.plotter.render()

    def spread_apart_clusters(self, spread_distance=30.0, spread_duration=2.0, spread_speed=0.05):
        """
        Gradually spreads the top 3 clusters apart into a triangle to prevent interference during lumenation.

        Parameters
        ----------
        spread_distance : float
            How far apart to move the clusters.
        spread_duration : float
            Total time (seconds) over which to complete the spreading.
        spread_speed : float
            Scale factor controlling how fast neurons move per frame.
        """
        current_time = self.timer.current_time

        # Initialize start time if not already set
        if not hasattr(self, "cluster_spread_start_time"):
            self.cluster_spread_start_time = current_time

        elapsed_time = current_time - self.cluster_spread_start_time
        progress = min(elapsed_time / spread_duration, 1.0)  # Clamped between 0 and 1

        clusters = self.find_clusters(distance_threshold=4.5)
        if not clusters:
            print("‚ö†Ô∏è No clusters found")
            return

        num_clusters = len(clusters)

        # Arrange clusters in a circle for even spreading
        cluster_targets = []
        for i in range(num_clusters):
            angle = (2 * np.pi * i) / num_clusters
            x = spread_distance * np.cos(angle)
            y = spread_distance * np.sin(angle)
            cluster_targets.append(np.array([x, y, 0]))

        cluster_centers = [np.mean([n.cell.position for n in cluster], axis=0) for cluster in clusters]

        print(f"\nüì¶ Spreading Clusters: Progress = {progress:.1%}")
        for idx, (cluster, current_center, target_center) in enumerate(zip(clusters, cluster_centers, cluster_targets)):
            shift_vector = (target_center - current_center) * spread_speed * progress
            for neuron in cluster:
                neuron.cell.position += shift_vector

    def project_clusters_onto_plane_gradually(self, projection_duration=2.0):
        """
        Gradually projects each cluster's neurons onto a local plane orthogonal to
        the cluster's principal axis, and scales each cell to 0.8 x 0.8 x 2.0 
        relative to that cluster's orientation.

        Parameters
        ----------
        projection_duration : float
            Time (in seconds) over which the projection completes.
        """
        current_time = self.timer.current_time

        if not hasattr(self, "projection_start_time"):
            self.projection_start_time = current_time

        elapsed = current_time - self.projection_start_time
        progress = min(elapsed / projection_duration, 1.0)

        clusters = self.find_clusters(distance_threshold=4.5)
        if not clusters:
            print("‚ö†Ô∏è No clusters found.")
            return

        print(f"\nüì¶ Projecting {len(clusters)} clusters to their local planes...")
        print(f"‚è±Ô∏è Elapsed: {elapsed:.2f}s / {projection_duration:.2f}s ‚Üí Progress: {progress:.2%}")

        for c_idx, cluster in enumerate(clusters):
            if len(cluster) < 3:
                continue  # skip tiny clusters

            # Step 1: Compute PCA manually
            positions = np.array([n.cell.position for n in cluster])
            cluster_center = np.mean(positions, axis=0)
            centered = positions - cluster_center

            cov = np.cov(centered, rowvar=False)
            eigvals, eigvecs = np.linalg.eigh(cov)  # eigvecs: columns

            sorted_indices = np.argsort(eigvals)[::-1]
            eigvecs = eigvecs[:, sorted_indices]
            R = eigvecs  # local ‚Üí world axes

            print(f"\nüîπ Cluster {c_idx+1}:")
            print(f"   - Center: {np.round(cluster_center, 2)}")
            print(f"   - Main axis (1st PC): {np.round(R[:,0], 3)}")

            # Step 2: Loop over neurons and project + scale
            for i, neuron in enumerate(cluster):
                pos = neuron.cell.position.copy()

                if not hasattr(neuron, "original_position_for_projection"):
                    neuron.original_position_for_projection = pos.copy()

                original_pos = neuron.original_position_for_projection.copy()
                rel_vector = original_pos - cluster_center

                # Projection: remove component along first principal axis
                projection_length = np.dot(rel_vector, R[:,0])
                projection_vector = projection_length * R[:,0]
                target_pos = original_pos - projection_vector

                interpolated_pos = (1 - progress) * original_pos + progress * target_pos
                neuron.cell.position = interpolated_pos

                # Step 3: Scale gradually toward 0.8 √ó 0.8 √ó 2.0 in cluster-local frame
                target_local_scale = np.array([0.8, 0.8, 2.0])
                S = np.diag(target_local_scale)
                world_scale_matrix = R @ S @ R.T

                interpolated_scale_matrix = np.eye(3) + progress * (world_scale_matrix - np.eye(3))
                scale_vector = interpolated_scale_matrix @ np.ones(3)

                neuron.cell.sphere.scale(scale_vector)

                # üß† Debug info
                print(f"   üß¨ Neuron {i+1}")
                print(f"      - Original Pos:   {np.round(original_pos, 2)}")
                print(f"      - Projected Pos:  {np.round(target_pos, 2)}")
                print(f"      - Final Pos:      {np.round(interpolated_pos, 2)}")
                print(f"      - Scale Vector:   {np.round(scale_vector, 3)}")


    def split_elongated_cells(self, elongation_threshold=2.0, split_distance=3.5):
        """
        Splits neurons that are sufficiently elongated by duplicating them along the apical-basal axis.
        Doubles the number of cells post-elongation.

        Parameters:
        - elongation_threshold: Minimum Z-axis scale to qualify for division.
        - split_distance: Distance between the original and new cell along the apical-basal axis.
        """
        new_neurons = []

        # Find the three largest clusters again to calculate consistent centers
        clusters = self.find_clusters(distance_threshold=4.5)
        sorted_clusters = sorted(clusters, key=len, reverse=True)
        clusters_to_keep = sorted_clusters[:3]
        cluster_centers = [np.mean([n.cell.position for n in cluster], axis=0) for cluster in clusters_to_keep]

        # Map neurons to cluster index (assumes similar clustering from polarization)
        neuron_cluster_map = {}
        for i, cluster in enumerate(clusters_to_keep):
            for neuron in cluster:
                neuron_cluster_map[neuron] = i

        for neuron in self.neurons:
            if neuron not in neuron_cluster_map:
                continue

            # Only split elongated cells
            if not hasattr(neuron.cell.sphere, "scale"):
                continue

            scale = neuron.cell.sphere.scale()
            if scale[2] < elongation_threshold:
                continue

            cluster_idx = neuron_cluster_map[neuron]
            cluster_center = cluster_centers[cluster_idx]

            # Apical-basal axis is center-to-cell
            ab_axis = normalize_vector(cluster_center - neuron.cell.position)
            split_offset = ab_axis * split_distance

            # New position along apical-basal axis
            new_position = neuron.cell.position + split_offset
            new_neuron = self.create_new_neuron(new_position)
            new_neuron.cell.sphere.scale(scale)  # Copy scale

            # Optional: apply a little jitter for visual realism
            jitter = np.random.uniform(-0.3, 0.3, size=3)
            new_neuron.cell.position += jitter

            new_neurons.append(new_neuron)

        self.neurons.extend(new_neurons)
        self.update_drawings()
        print(f"üß¨ Split {len(new_neurons)} new neurons along apical-basal axis.")
        
    def apply_lumenation(self, lumen_factor=0.1):
        """
        Gently pushes neurons outward from their respective cluster centers.
        """
        clusters = self.find_clusters(distance_threshold=6.0)

        for cluster in clusters:
            cluster_center = np.mean([n.cell.position for n in cluster], axis=0)

            for neuron in cluster:
                direction_outward = normalize_vector(neuron.cell.position - cluster_center)
                neuron.cell.position += direction_outward * lumen_factor
                if hasattr(neuron.cell, "sphere"):
                    neuron.cell.sphere.pos(neuron.cell.position)

        self.lumenation_complete = True
        print("üåü Lumenation complete.")

    def apply_elongation(self, elongation_factor=0.05):
        """
        Slightly grows cells with pointed ends aligned radially
        (toward/away from their assigned cluster center).
        """
        # Get all clusters
        clusters = self.find_clusters(distance_threshold=6.0)

        for cluster in clusters:
            cluster_center = np.mean([n.cell.position for n in cluster], axis=0)

            for neuron in cluster:
                if not hasattr(neuron, "original_vertices"):
                    neuron.original_vertices = np.copy(neuron.cell.sphere.points())

                cell_center = neuron.cell.position
                direction_to_center = normalize_vector(cell_center - cluster_center)
                new_vertices = []

                for vertex in neuron.original_vertices:
                    direction = normalize_vector(vertex - cell_center)
                    projection = np.dot(direction, direction_to_center)
                    displacement = direction * (elongation_factor * projection)
                    new_vertices.append(vertex + displacement)

                neuron.cell.sphere.points(np.array(new_vertices), transformed=True)

        self.elongation_complete = True
        print("‚úÖ Elongation complete! Moving to lumenation.")

    # def replicate_lone_cells_spread_out(self, target_lone_count=6, spread_distance=1.8):
    #     """
    #     Replicates lone (non-clustered) neurons up to a target count.
    #     Each replicated cell is spread out and marked green.
    #     """
    #     if len(self.neurons) < 3:
    #         return

    #     # Get clustered neurons
    #     clusters = self.find_clusters(distance_threshold=4.5)
    #     clustered = set(n for cluster in clusters for n in cluster)

    #     # Identify lone neurons
    #     lone_neurons = [n for n in self.neurons if n not in clustered]

    #     if not lone_neurons:
    #         print("üü© No lone neurons found for replication.")
    #         return

    #     replicates = 0
    #     while replicates < target_lone_count and lone_neurons:
    #         source = np.random.choice(lone_neurons)
    #         jitter = np.random.normal(scale=1.0, size=3)
    #         direction = np.random.uniform(-1, 1, size=3)
    #         direction /= np.linalg.norm(direction) + 1e-8

    #         new_position = source.cell.position + direction * spread_distance + jitter
    #         new_neuron = self.create_new_neuron(new_position)

    #         if new_neuron and hasattr(new_neuron.cell, "sphere"):
    #             new_neuron.cell.sphere.color("green")
    #             print(f"üÜï Green lone neuron replicated at {new_position}")
    #             replicates += 1

    def replicate_lone_cells_spread_out(self, target_lone_count=6, spread_distance=1.8):
        """
        Replicates neurons that are spatially isolated from all others.
        These lone neurons are duplicated in a spread-out fashion and marked green.
        """
        if len(self.neurons) < 1:
            return

        # Define "lone" as: no neighbor within 5.0 units
        lone_neurons = []
        distance_threshold = 1.0

        for neuron in self.neurons:
            is_lone = True
            for other in self.neurons:
                if neuron is not other:
                    dist = np.linalg.norm(neuron.cell.position - other.cell.position)
                    if dist < distance_threshold:
                        is_lone = False
                        break
            if is_lone:
                lone_neurons.append(neuron)

        if not lone_neurons:
            print("üü© No truly isolated neurons found for replication.")
            return

        replicates = 0
        while replicates < target_lone_count and lone_neurons:
            source = np.random.choice(lone_neurons)
            jitter = np.random.normal(scale=0.2, size=3)
            direction = np.random.uniform(-1, 1, size=3)
            direction /= np.linalg.norm(direction) + 1e-8

            new_position = source.cell.position + direction * spread_distance + jitter
            if self.simulation_2d:
                new_position[2] = 0.0

            new_neuron = self.create_new_neuron(new_position)

            if new_neuron and hasattr(new_neuron.cell, "sphere"):
                new_neuron.cell.sphere.color("green")
                # new_neuron.cell.sphere.scale([0.75, 0.75, 0.75])
                
                # Draw line from source to new neuron
                if hasattr(self, "plotter"):
                    from vedo import Line
                    connection = Line(source.cell.position, new_neuron.cell.position, c="green", lw=0.3)
                    self.plotter.vp += connection

                print(f"üÜï Green lone neuron replicated at {new_position}")
                replicates += 1

            # if new_neuron and hasattr(new_neuron.cell, "sphere"):
            #     new_neuron.cell.sphere.color("green")
            #     print(f"üÜï Green lone neuron replicated at {new_position}")
            #     replicates += 1


#4:30 AM
    # def apply_elongation(self, elongation_factor=1.5, alignment_duration=10.0):
    #     """
    #     Gradually elongates each neuron along its apical-basal axis over `alignment_duration` seconds.

    #     Instead of using `.scale()`, we move the vertices of the sphere mesh along the identified
    #     apical-basal direction to ensure stability in Vedo.
    #     """
    #     current_time = self.timer.current_time

    #     if not hasattr(self, "elongation_start_time"):
    #         self.elongation_start_time = current_time  # Set the start time

    #     elapsed_time = current_time - self.elongation_start_time
    #     elongation_progress = min(elapsed_time / alignment_duration, 1.0)  # Normalize (0 ‚Üí 1)

    #     for neuron in self.neurons:
    #         if not hasattr(neuron, "elongated"):
    #             neuron.elongated = False  # Track elongation state

    #         # **Determine the apical-basal axis** (already randomized in polarization step)
    #         apical_dir = normalize_vector(neuron.cell.position)  # Assuming it's defined
    #         basal_dir = -apical_dir  # Opposite direction

    #         # **Store original positions for smooth elongation**
    #         if not hasattr(neuron, "original_vertices"):
    #             neuron.original_vertices = np.copy(neuron.cell.sphere.points())

    #         # **Apply elongation over time**
    #         new_vertices = []
    #         for vertex in neuron.original_vertices:
    #             # Compute how far to stretch the vertex along the apical-basal axis
    #             displacement = apical_dir * (elongation_progress * elongation_factor)

    #             # Stretch based on whether the vertex is on the apical or basal side
    #             if np.dot(vertex - neuron.cell.position, apical_dir) > 0:
    #                 new_vertex = vertex + displacement  # Move in apical direction
    #             else:
    #                 new_vertex = vertex - displacement  # Move in basal direction
                
    #             new_vertices.append(new_vertex)

    #         # Update the cell's mesh vertices smoothly
    #         neuron.cell.sphere.points(np.array(new_vertices), transformed=True)

    #         # ‚úÖ Mark as fully elongated when the transition completes
    #         if elongation_progress >= 1.0:
    #             neuron.elongated = True  # Lock elongation
    #             self.elongation_complete = True  # Mark simulation phase complete
    #             print(f"‚úÖ Elongation **complete**! Moving to lumenation.")

    def induce_rosette_folding(self):
        """Apically constricts cells, initiating inward curvature."""
        lumen_center = np.mean([n.cell.position for n in self.neurons], axis=0)
        for neuron in self.neurons:
            neuron.cell.position = 0.8 * neuron.cell.position + 0.2 * lumen_center  # Pull toward center
            neuron.cell.sphere.scale([0.8, 0.8, 1.0])  # Y-axis elongation
            neuron.cell.sphere.color("purple")  # Mark rosette forming


    def find_clusters(self, distance_threshold: float) -> List[List[Neuron]]:
        """
        Groups neurons into clusters based on a distance threshold.

        Parameters
        ----------
        distance_threshold : float
            The maximum distance between neurons to be considered part of the same cluster.

        Returns
        -------
        List[List[Neuron]]
            A list of clusters, where each cluster is a list of neurons.
        """
        clusters = []
        visited = set()

        for neuron in self.neurons:
            if neuron not in visited:
                cluster = self._dfs_cluster(neuron, distance_threshold, visited)
                clusters.append(cluster)
        return clusters

    def _dfs_cluster(self, neuron: Neuron, distance_threshold: float, visited: set) -> List[Neuron]:
        """
        Performs a depth-first search to find all neurons in the same cluster.

        Parameters
        ----------
        neuron : Neuron
            The starting neuron for the search.
        distance_threshold : float
            The maximum distance between neurons to be considered part of the same cluster.
        visited : set
            A set to keep track of visited neurons.

        Returns
        -------
        List[Neuron]
            A list of neurons in the same cluster as the starting neuron.
        """
        stack = [neuron]
        cluster = []

        while stack:
            current = stack.pop()
            if current not in visited:
                visited.add(current)
                cluster.append(current)
                for neighbor in self._get_neighbors(current, distance_threshold):
                    if neighbor not in visited:
                        stack.append(neighbor)
        return cluster

    def _get_neighbors(self, neuron: Neuron, distance_threshold: float) -> List[Neuron]:
        """
        Finds all neighbors of a neuron within a given distance.

        Parameters
        ----------
        neuron : Neuron
            The neuron for which to find neighbors.
        distance_threshold : float
            The maximum distance to consider a neighbor.

        Returns
        -------
        List[Neuron]
            A list of neighboring neurons.
        """
        neighbors = []
        neuron_position = neuron.cell.position  # Access the correct position attribute

        for other_neuron in self.neurons:
            other_position = other_neuron.cell.position  # Access the position of the other neuron
            if other_neuron != neuron and np.linalg.norm(neuron_position - other_position) <= distance_threshold:
                neighbors.append(other_neuron)
        return neighbors

        # try:
        #     neuron_position = neuron.cell.position  # Correctly access the position attribute
        # except AttributeError:
        #     raise AttributeError("Neuron object does not have a 'cell.position' attribute. Check the Neuron class definition.")

        # for other_neuron in self.neurons:
        #     try:
        #         other_position = other_neuron.cell.position  # Access the position of other neurons
        #     except AttributeError:
        #         continue  # Skip if the neuron doesn't have a valid position

        #     if other_neuron != neuron and np.linalg.norm(neuron_position - other_position) <= distance_threshold:
        #         neighbors.append(other_neuron)

        # return neighbors
    def check_rosette_formation(self, min_time_before_check: float = 150.0) -> bool:
        """
        Checks if any clusters meet the threshold for rosette formation.

        Parameters
        ----------
        min_time_before_check : float
            The minimum time that must pass before checking for rosettes.

        Returns
        -------
        bool
            True if at least one cluster meets the threshold for rosette formation, False otherwise.
        """

        # Ensure enough time has passed before checking for rosettes
        if self.timer.current_time < min_time_before_check:
            print(f"Skipping rosette check at time {self.timer.current_time}. Not enough time has passed.")
            return False  # Do not check too early

        clusters = self.find_clusters(distance_threshold=5.0)  

        # Debugging: Log cluster formation
        print(f"Number of clusters detected: {len(clusters)} at time {self.timer.current_time}")

        for i, cluster in enumerate(clusters):
            # Increase the threshold for detecting real rosettes
            if len(cluster) >= 4:  
                print(f"Rosette formation detected in Cluster {i} (size: {len(cluster)})")
                return True  # A rosette is confirmed

        print("No rosettes detected this cycle.")
        return False  # No valid clusters met the threshold
    
    def elongate_cells_toward_global_center(self, scale_long=2.2, scale_short=0.7, alignment_duration=5.0):
        """
        Gradually elongate neurons toward the global cluster center.
        Caps elongation over time using `alignment_duration`.
        """
        current_time = self.timer.current_time

        if not hasattr(self, "elongation_start_time"):
            self.elongation_start_time = current_time

        elapsed_time = current_time - self.elongation_start_time
        alignment_progress = min(elapsed_time / alignment_duration, 1.0)
        print(f"\nüü° Elongation Debug: Time {current_time / 550 + 1:.2f} days, Progress: {alignment_progress:.2%}")

        if len(self.neurons) < 2:
            return

        global_center = np.mean([neuron.cell.position for neuron in self.neurons], axis=0)

        for neuron in self.neurons:
            if not hasattr(neuron.cell, "sphere"):
                continue

            pos = neuron.cell.position
            direction = global_center - pos
            norm = np.linalg.norm(direction)
            if norm < 1e-6:
                continue

            direction = direction / norm

            # Compute final elongation target scale
            elongation_vector = np.abs(direction)
            elongation_scale = 1 + (scale_long - 1) * elongation_vector
            scale_vector = [
                1 + alignment_progress * (s - 1) for s in elongation_scale
            ]

            neuron.cell.sphere.setScale(scale_vector)
            neuron.cell.sphere.color("cyan")

        if alignment_progress >= 1.0:
            self.has_scaled = True
            print("‚úÖ Global elongation complete.")
    def apply_dynamic_repulsion(neurons, current_time, total_time, base_strength=1.0, repulsion_threshold=15.0):
        """
        Apply a time-scaled repulsion force between nearby neurons.
        - Force ramps up gradually as simulation progresses.
        - Keeps cells from clustering too early.
        """
        repulsion_scale = min(1.0, current_time / (total_time * 0.7))  # Gradually increase
        effective_strength = base_strength * repulsion_scale

        for i, neuron in enumerate(neurons):
            for j, other in enumerate(neurons):
                if i == j:
                    continue

                displacement = neuron.cell.position - other.cell.position
                dist = np.linalg.norm(displacement)

                if dist < repulsion_threshold:
                    direction = displacement / (dist + 1e-6)
                    repulsion = effective_strength * direction * (1.0 - dist / repulsion_threshold)
                    neuron.cell.position += repulsion

    # def elongate_cells_toward_global_center(self, scale_long=2.2, scale_short=0.7):
    #     """
    #     Elongate all neuron spheres so they point toward the global cluster center.
    #     """
    #     if len(self.neurons) < 2:
    #         return

    #     global_center = np.mean([neuron.cell.position for neuron in self.neurons], axis=0)

    #     for neuron in self.neurons:
    #         if not hasattr(neuron.cell, "sphere"):
    #             continue

    #         pos = neuron.cell.position
    #         direction = global_center - pos
    #         norm = np.linalg.norm(direction)
    #         if norm < 1e-6:
    #             continue
    #         direction = direction / norm

    #         # First, scale along Z (default), then rotate Z to face direction
    #         neuron.cell.sphere.scale([scale_short, scale_short, scale_long])
    #         neuron.cell.sphere.orientation(newaxis=direction)
    #         neuron.cell.sphere.color("cyan")
    from vedo import Line

    def draw_neighbor_connections(container, distance_threshold=20.0, color="red", line_width=0.3):
        """
        Draws line segments from each neuron to its nearby neighbors.
        
        Parameters:
        - container: your SimulationContainer object containing `neurons`
        - distance_threshold: max distance to consider two neurons neighbors
        - color: line color (e.g., "red", "blue", "green")
        - line_width: thickness of the lines
        """
        neurons = container.neurons
        plotter = container.plotter  # Your Vedo plotter (assumed available)

        for i, neuron in enumerate(neurons):
            p1 = neuron.cell.position
            for j in range(i + 1, len(neurons)):
                other = neurons[j]
                p2 = other.cell.position
                distance = np.linalg.norm(p1 - p2)

                if distance <= distance_threshold:
                    connection = Line(p1, p2, c=color, lw=line_width)
                    plotter.vp += connection  # Add to Vedo plotter

    def replicate_neuron(self, parent_neuron):
        displacement = np.random.uniform(-1.0, 1.0, size=3)  # Slight offset
        new_position = parent_neuron.cell.position + displacement

        new_neuron = self.neuron_factory.create_neuron(coordinates=new_position)

        new_neuron.cell.sphere.color("green")  # Visualize new cells differently
        new_neuron.cell.sphere.scale([1.0, 1.0, 1.0])
        
        # Reset parent cycle clock
        parent_neuron.clocks.cycle_clock.reset()
        return new_neuron


    def update_sphere_colors_and_deform(self, time_step: float):
        """
        Handles visualization updates based on the current phase.
        NO PHYSICS OR FORCE APPLICATIONS SHOULD BE HERE.
        """

        print(f"üìå Time {self.timer.current_time}: Updating visualization...")

        # **Clustering Phase**: All cells are blue
        if not self.clustering_complete:
            for neuron in self.neurons:
                neuron.cell.sphere.color("blue")
                # **Elongation Phase**: Make neurons light blue
        elif self.polarization_complete and not self.elongation_complete:
            for neuron in self.neurons:
                neuron.cell.sphere.color("blue")

        # **Lumenation Phase**: Make neurons light green
        elif self.elongation_complete and not self.lumenation_complete:
            for neuron in self.neurons:
                neuron.cell.sphere.color("lightgreen")

        # **Render Once at the End**
        self.animator.plotter.render()


    def update_indicator(self):
        """
        Updates the indicator color based on rosette formation status.
        """
        # Re-check clusters to determine rosette formation status
        clusters = self.find_clusters(distance_threshold = 9.0)  # Ensure the threshold matches elsewhere
        self.rosette_formed = any(len(cluster) >= 3 for cluster in clusters)

        if self.rosette_formed:
            self.indicator_color = "green"
        else:
            self.indicator_color = "blue"
        print(f"Indicator updated to: {self.indicator_color}")

        # Ensure the animator reflects this change visually
        self.animator.update_indicator(color=self.indicator_color)

    def jiggle_neurons(self, jiggle_magnitude: float = 0.10) -> None:
        """
        Jiggling with dynamic control based on time or density.
        """
        current_density = len(self.neurons) / self.grid.total_volume
        adjusted_magnitude = max(0.10, jiggle_magnitude * (1 - current_density))  # Reduce jiggle as density increases

        for neuron in self.neurons:
            jiggle_vector = np.random.uniform(-jiggle_magnitude, jiggle_magnitude, size=3)
            neuron.cell.position += jiggle_vector
            neuron.cell.sphere.pos(neuron.cell.position)
        self.animator.plotter.render()
    def check_and_flag_neurons_for_death(self):
        """Flag neurons for removal based on lifetime/age conditions."""
        for neuron in self.neurons:
            if not hasattr(neuron, "lifetime"):
                neuron.lifetime = np.random.uniform(100, 200)  # You can tweak range
            if not hasattr(neuron, "age"):
                neuron.age = 0

            neuron.age += 1

            if neuron.age > neuron.lifetime:
                neuron.ready_to_die = True
                print(f"‚ùå Neuron {neuron} marked for death (age: {neuron.age}, lifetime: {neuron.lifetime})")
                num_flagged = sum(getattr(n, "ready_to_die", False) for n in self.neurons)
                print(f"‚ö†Ô∏è Neurons flagged for death this step: {num_flagged}")

    def perform_neuron_death_cleanup(self):
        """Actually remove neurons flagged for death from the container and plot."""
        neurons_to_keep = []
        for neuron in self.neurons:
            if getattr(neuron, "ready_to_die", False):
                # Visual cleanup
                self.animator.plotter -= neuron.cell.sphere
                for neurite in neuron.neurites:
                    self.animator.plotter -= neurite.cylinder[0]
                    self.animator.plotter -= neurite.cylinder[1]
                print(f"üóëÔ∏è Neuron {neuron} deleted.")
            else:
                neurons_to_keep.append(neuron)
        self.neurons = neurons_to_keep

    def check_and_trigger_neuron_division(self):
        """Check neurons for division and divide if conditions are met."""
        new_neurons = []
        for neuron in self.neurons:
            if not hasattr(neuron.clocks.cycle_clock, "time_to_division"):
                neuron.clocks.cycle_clock.time_to_division = 100  # initial setting

            if not neuron.ready_for_division:
                neuron.clocks.cycle_clock.time_to_division -= 10
                continue

            # You can add density logic here if needed

            # Spread new neurons outward
            spread_factor = np.random.uniform(5.0, 10.0)
            direction = get_random_unit_vector(two_dimensions=self.simulation_2d)
            new_pos = neuron.cell.position + (direction * neuron.cell_radius * spread_factor)

            # Avoid collisions
            while any(np.linalg.norm(new_pos - other.cell.position) < 2.5 * neuron.cell_radius for other in self.neurons):
                direction = get_random_unit_vector(two_dimensions=self.simulation_2d)
                new_pos = neuron.cell.position + (direction * neuron.cell_radius * spread_factor)

            new_neuron = self.create_new_neuron(new_pos)
            new_neuron.cell.sphere.scale([1.0, 1.0, 1.0])

            # Slight jitter
            jitter = np.random.uniform(-0.5, 0.5, size=3)
            new_neuron.cell.position += jitter

            neuron.clocks.cycle_clock.remove_flag()
            new_neurons.append(new_neuron)
            print(f"üß¨ Neuron {neuron} divided. New neuron created at {new_pos}.")

        self.neurons.extend(new_neurons)
        self.update_drawings()

    def update(self): 
        """
        Updates the simulation container by advancing cycles, handling differentiation,
        killing neurons, and updating drawings.
        """
        self.advance_cycles(self.timer.step)
        self.kill()
        self.differentiate()
        self.divide()
        self.solve_mechanics(self.timer.step)
        self.update_drawings()
    
    def apply_fringe_effect(self, core_radius=20.0, edge_softening=0.15):
        colony_center = np.mean([n.cell.position for n in self.neurons], axis=0)
        for neuron in self.neurons:
            dist = np.linalg.norm(neuron.cell.position - colony_center)
            if dist > core_radius:
                # Reduce cohesion or density at edge
                jitter = np.random.normal(scale=edge_softening, size=3)
                neuron.cell.position += jitter
    def trigger_differentiation_window(neurons, current_day, start=8.0, end=12.5):
        """
        Marks neurons as ready to differentiate if current_day falls within a specified window.

        Parameters:
        - neurons: list of Neuron objects
        - current_day: float, simulation time in days
        - start: float, start of differentiation window (default 8.0)
        - end: float, end of differentiation window (default 12.5)
        """
        if start <= current_day <= end:
            for neuron in neurons:
                neuron.cell.ready_to_differentiate = True

    def advance_cycles(self, time_step: float) -> None:
        current_day = self.timer.current_time / 550
        print(f"‚è© Updated time: {current_day:.2f} days")

    #     """Advances the simulation and applies trend-based modifications based on rosette formation parameters."""
    #     self.divide()
    #     self.timer.current_time += time_step
    #     print(f"‚è© Updated time: {self.timer.current_time}")
    # # In `advance_cycles` or simulation loop:
    #     # for neuron in self.neurons:
    #     #     if not hasattr(neuron, "lifetime"):
    #     #         neuron.lifetime = np.random.uniform(100, 200)
    #     #     if not hasattr(neuron, "age"):
    #     #         neuron.age = 0
    #     #     neuron.age += 1
    #     #     if neuron.age > neuron.lifetime:
    #     #         neuron.ready_to_die = True
    #     # for neuron in self.neurons:
    #     #     self.kill()
    #     #     self.divide()
    #     # üîÅ Lifecycle management
    #     # self.check_and_flag_neurons_for_death()
    #     # self.perform_neuron_death_cleanup()
    #     # self.check_and_trigger_neuron_division()

    #           # Compute Phase Parameters
    #     C = compute_clustering_coefficient(self.neurons)
    #     NNV = compute_nearest_neighbor_variance(self.neurons)
    #     H = compute_orientation_entropy(self.neurons)
    #     AR = compute_aspect_ratio(self.neurons)
    #     RSI = compute_radial_symmetry(self.neurons)

    #     # Log parameters
    #     print(f"üìä C: {C:.4f}, NNV: {NNV:.4f}, H: {H:.4f}, AR: {AR:.4f}, RSI: {RSI:.4f}")
    #     print(f"üî¢ Total neurons: {len(self.neurons)}")
    #     # 1Ô∏è‚É£ **Clustering Phase**
    #     if not self.clustering_complete:
    #         # while True:
    #             # üß† Continuous directional jiggle for all neurons
    #         for neuron in self.neurons:
    #             # Random small directional movement (jiggle + slight bias outward from center)
    #             center = np.array([0.0, 0.0, 0.0])
    #             direction = neuron.cell.position - center
    #             if np.linalg.norm(direction) == 0:
    #                 direction = np.random.uniform(-1, 1, size=3)
    #             direction = direction / np.linalg.norm(direction)

    #             jiggle = np.random.uniform(-0.02, 0.02, size=3)
    #             directional_bias = 0.01 * direction  # Adjust magnitude of outward motion

    #             neuron.cell.position += jiggle + directional_bias

    #         print(f"üìå Clustering in progress...")
    #         # self.apply_clustering_force()
    #         self.apply_density_aware_clustering_force
    #         # Inside your simulation loop

    #     # time_interval = 0.25  # seconds
    #     # previous_time = self.timer.current_time - time_step
    #     # current_time = self.timer.current_time

    #     # # Trigger only when crossing an exact multiple of `time_interval`
    #     # # if not hasattr(self, "pre_polarization_scaled"):
    #     # if int(previous_time // time_interval) < int(current_time // time_interval):
    #     #     print(f"‚è±Ô∏è Triggered at time {current_time:.2f}")
    #     #     self.replicate_clusters_balanced(target_cluster_size=random.randint(1, 6))


    #         # Check if clustering is complete
    #         avg_distance = np.mean([
    #             np.linalg.norm(n.cell.position - np.mean([m.cell.position for m in self.neurons], axis=0))
    #             for n in self.neurons
    #         ])
    #         clusters = self.find_clusters(distance_threshold=4.5)
    #         avg_cluster_coef = compute_average_cluster_coefficient(clusters)

    #         if avg_cluster_coef > 0.23 and AR == 1:
    #             self.clustering_complete = True
    #             # before_count = len(self.neurons)
    #             # self.remove_outlier_neurons(removal_threshold=3.5, min_cluster_size=20)
    #             # after_count = len(self.neurons)
    #             # removed = before_count - after_count

    #             # print(f"üß† Neurons before: {before_count}, after: {after_count}, removed: {removed}")
    #             print("‚úÖ Clustering complete! Moving to constriction.")

    #                  # **New Step: Compact clusters before spreading**
    #             self.compact_clusters_spherically(attraction_strength=-0.1, iterations=20)

    #             # self.replicate_clusters_balanced(target_cluster_size=30)

    #             # 2Ô∏è‚É£ Gradual spreading after clustering, added after the spread force added to remove_outlier_neurons
    #     if self.clustering_complete and not hasattr(self, "clusters_spread"):
    #         self.apply_cluster_spread(spread_strength=0.49)  # <‚Äî adjust as needed
    #         self.clusters_spread = True  # ‚Üê use a **separate flag**
    #         self.replicate_clusters_balanced(target_cluster_size=30)
               
    #             # üìä Log current cluster status
    #     # clusters = self.find_clusters(distance_threshold=4.5)
    #     # print(f"üî¢ Total neurons: {len(self.neurons)}")
    #     # for i, cluster in enumerate(clusters):
    #     #     print(f"üß† Cluster {i+1} size: {len(cluster)}")

    #     # üß¨ Scaling Phase - before polarization starts
    #     if self.clustering_complete and hasattr(self, "clusters_spread") and not hasattr(self, "pre_polarization_scaled"):
    #         print("üì¶ Scaling neurons slightly before polarization...")

    #         for neuron in self.neurons:
    #             if hasattr(neuron.cell, "sphere"):
    #                 neuron.cell.sphere.scale([0.8, 0.8, 2.0])  # üîç Adjust scale as needed

    #         self.pre_polarization_scaled = True  # ‚úÖ Only scale once

    #     elif self.clustering_complete and not self.polarization_complete:
    #         print(f"üìå Polarization and Elongation in progress...")    

    #         # üîÑ **Apply polarity with spreading & gradual alignment**
    #         self.apply_polarity_establishment(spread_factor=0.035, alignment_duration=4.0)
    #         # self.split_elongated_cells()

    #         # ‚úÖ **Check if polarization is complete**
    #     if self.polarization_complete:

    #         print(f"‚úÖ Polarization and Elongation **complete**! Moving to lumen formation.")


    #     if self.polarization_complete and not self.elongation_complete:
    #         print(f"üìå Lumenation in progress...")
    #         # self.project_clusters_onto_plane_gradually(projection_duration=2.0)
    #         # self.spread_apart_clusters(spread_distance=30.0, spread_duration=2.0, spread_speed=0.05)
    #         # elongation_factor = 0.025  # Adjust elongation intensity

    #         # self.apply_elongation(elongation_factor=elongation_factor)
            


    #     elif self.elongation_complete and not self.lumenation_complete:
    #         print(f"üí° Lumenation in progress...")
    #         # self.apply_lumenation(lumen_factor=0.1)
    #         # # Get the current cluster center
    #         # cluster_center = np.mean([n.cell.position for n in self.neurons], axis=0)

    #         # # Define duration for smooth lumenation
    #         # lumen_duration = 30.0
    #         # current_time = self.timer.current_time

    #         # if not hasattr(self, "lumenation_start_time"):
    #         #     self.lumenation_start_time = current_time  # Set start time

    #         # elapsed_time = current_time - self.lumenation_start_time
    #         # lumen_progress = min(elapsed_time / lumen_duration, 1.0)  # Normalize (0 ‚Üí 1)

    #         # for neuron in self.neurons:
    #         #     direction_from_center = normalize_vector(neuron.cell.position - cluster_center)
                
    #         #     # **Move outward gradually over time**
    #         #     neuron.cell.position += direction_from_center * 0.241 * lumen_progress  

    #         # # ‚úÖ Mark lumenation complete
    #         # if lumen_progress >= 1.0:
    #         #     self.lumenation_complete = True
    #         #     print(f"‚úÖ Lumenation **complete**! Final structure achieved.")




















































        """Advances the simulation and applies trend-based modifications based on rosette formation parameters."""
        self.divide()
        self.timer.current_time += time_step
        print(f"‚è© Updated time: {self.timer.current_time / 550:.2f} days")
        # print(f"‚è© Updated time: {self.timer.current_time}")
    # In `advance_cycles` or simulation loop:
        # for neuron in self.neurons:
        #     if not hasattr(neuron, "lifetime"):
        #         neuron.lifetime = np.random.uniform(100, 200)
        #     if not hasattr(neuron, "age"):
        #         neuron.age = 0
        #     neuron.age += 1
        #     if neuron.age > neuron.lifetime:
        #         neuron.ready_to_die = True
        # for neuron in self.neurons:
        #     self.kill()
        #     self.divide()
        # üîÅ Lifecycle management
        # self.check_and_flag_neurons_for_death()
        # self.perform_neuron_death_cleanup()
        # self.check_and_trigger_neuron_division()

              # Compute Phase Parameters
        C = compute_clustering_coefficient(self.neurons)
        NNV = compute_nearest_neighbor_variance(self.neurons)
        H = compute_orientation_entropy(self.neurons)
        AR = compute_aspect_ratio(self.neurons)
        RSI = compute_radial_symmetry(self.neurons)

        # Log parameters
        print(f"üìä C: {C:.4f}, NNV: {NNV:.4f}, H: {H:.4f}, AR: {AR:.4f}, RSI: {RSI:.4f}")
        print(f"üî¢ Total neurons: {len(self.neurons)}")
        # 1Ô∏è‚É£ **Clustering Phase**
        if not self.clustering_complete:
            # while True:
            #     # üß† Continuous directional jiggle for all neurons
            # for neuron in self.neurons:
            #     # Random small directional movement (jiggle + slight bias outward from center)
            #     center = np.array([0.0, 0.0, 0.0])
            #     direction = neuron.cell.position - center
            #     if np.linalg.norm(direction) == 0:
            #         direction = np.random.uniform(-1, 1, size=3)
            #     direction = direction / np.linalg.norm(direction)

            #     jiggle = np.random.uniform(-0.02, 0.02, size=3)
            #     directional_bias = 0.01 * direction  # Adjust magnitude of outward motion

            #     neuron.cell.position += jiggle + directional_bias

            print(f"üìå Clustering in progress...")
            # self.apply_clustering_force()
            # self.apply_density_aware_clustering_force
            # Inside your simulation loop

        time_interval = 1
        previous_time = self.timer.current_time - time_step
        current_time = self.timer.current_time
        # Trigger only when crossing an exact multiple of `time_interval`
        # if not hasattr(self, "pre_polarization_scaled"):
        if int(previous_time // time_interval) < int(current_time // time_interval) and int(current_time) <= 1300 and len(self.neurons) <= 420:
            print(f"‚è±Ô∏è Triggered at time {current_time:.2f}")
            # self.replicate_clusters_incrementally(target_cluster_size=random.randint(4, 8))
            if not self.clustering_complete:
                # self.apply_density_aware_clustering_force(
                #     clustering_strength=0.02,
                #     base_attraction_prob=0.3,
                #     resistance_probability=2.0  # Increase this to encourage more repulsion (less clustering)
                # )
                #THESE 4 LINES
                # self.replicate_lone_cells_spread_out(target_lone_count=random.randint(4, 8)) 1
                # self.apply_repulsion_until_separated(min_distance=11.0, repulsion_strength=1.4) 2
                # self.apply_density_aware_repulsion(min_distance=6.0, base_repulsion=1.8, neighbor_threshold=6) 3
                # self.replicate_lone_cells_spread_out(target_lone_count=random.randint(4, 8)) 4
                # self.apply_repulsion_until_separated(min_distance=12.0, repulsion_strength=1.6)
        #         self.apply_continuous_repulsion(min_distance=3.5, strength=1.6)
                # self.replicate_lone_cells_spread_out(target_lone_count=random.randint(1, 3))
                # if int(previous_time // (time_interval*3)) < int(current_time // (time_interval*3)) and int(current_time) <= 1300 and len(self.neurons) <= 420:
                #     # self.apply_repulsion_until_separated(min_distance=11.0, repulsion_strength=1.4)
                # self.replicate_clusters_spread_out(target_cluster_size=random.randint(4, 5))
                # self.apply_spring_repulsion(spring_constant = 0.03)
                # self.apply_spring_dynamics(dt=time_step, spring_constant=8)
                # after all other updates
                self.apply_mechanical_forces(
                    dt=time_step,
                    base_repulsion_k=45.0,
                    base_adhesion_k=1111.0,
                    adhesion_dist=1,
                    damping=0.7
                )
                # every simulation step, try a density-based replication pass
                self.replicate_cells_by_density(
                    base_prob=0.05,    # low base rate
                    radius=4.0,        # local neighborhood
                    jitter_scale=1.2   # how far new cells can land around parent
                )
                # e.g. every frame or every N steps
                self.replicate_cells_by_density_positive(base_prob=0.05, radius=6.0)
        #         import math
        #         """
        #         Applies polarity to one cluster at a time, allowing already polarized neurons to gradually morph.
        #         """
        #         current_time = self.timer.current_time
        #         alignment_duration = 5.0
        #         if not hasattr(self, "polarization_start_time"):
        #             self.polarization_start_time = current_time
        #         if not hasattr(self, "current_cluster_index"):
        #             self.current_cluster_index = 0

        #         elapsed_time = current_time - self.polarization_start_time
        #         clusters_to_polarize = 10
        #         duration_per_cluster = alignment_duration / clusters_to_polarize

        #         cluster_idx = int(elapsed_time / duration_per_cluster)
        #         if cluster_idx >= clusters_to_polarize:
        #             print("‚úÖ All clusters have completed polarity.")
        #             # self.polarization_complete = True
        #             return

        #         if cluster_idx > self.current_cluster_index:
        #             self.current_cluster_index = cluster_idx

        #         alignment_progress = min((elapsed_time - cluster_idx * duration_per_cluster) / duration_per_cluster, 1.0)
        #         print(f"\nüî¥ Polarizing cluster {cluster_idx+1}/10, Progress: {alignment_progress:.2%}")

        #         clusters = self.find_clusters(distance_threshold=2.5)
        # # # sorted_clusters = sorted(clusters, key=len, reverse=True)[:clusters_to_polarize]
        #         import math

        #         # your action radius (same as in find_clusters)
        #         action_radius = 8.5
        #         # area of that circle
        #         circle_area = math.pi * action_radius**2

        # #         # compute density = #cells / area
        #         clusters_with_density = [(cl, len(cl)/circle_area) for cl in clusters]
        # #         # sort by descending density and take the top N
        #         sorted_clusters = [
        #             cl for cl, dens in sorted(clusters_with_density, key=lambda x: x[1], reverse=True)
        #         ][:10]

        #         # cluster_centers = [
        #         #     np.mean([n.cell.position for n in cluster if np.all(np.isfinite(n.cell.position))], axis=0)
        #         #     for cluster in sorted_clusters
        #         # ]
        #         cluster_centers = []
        #         for i, cluster in enumerate(sorted_clusters):
        #             pts = [n.cell.position for n in cluster
        #                 if np.all(np.isfinite(n.cell.position)) and len(n.cell.position)==3]
        #             if not pts:
        #                 # no valid positions in this cluster
        #                 continue
        #             center = np.mean(pts, axis=0)
        #             # make sure it's really a 3-vector
        #             if center.shape == (3,) and np.all(np.isfinite(center)):
        #                 cluster_centers.append(center)
        #             else:
        #                 # skip any weird ones
        #                 continue

                # self.mark_cluster_centers(cluster_centers)
                # inside apply_sequential_polarity, after you have:
                #   sorted_clusters = [...]
                #   cluster_centers = [...]
                # cluster_sizes   = [len(cl) for cl in sorted_clusters]

                # self.mark_cluster_centers(cluster_centers, cluster_sizes, action_radius=7.5)

                # self.replicate_largest_clusters(target_cluster_size=30, min_dist=3.5, base_jitter=2.0, time_scaling=True, num_clusters=5)
                # self.apply_mechanical_forces(
                #     dt = time_step,
                #     repulsion_k = 2.0,
                #     adhesion_k = 2.0,
                #     adhesion_dist = 15.0)

                # if len(self.neurons) >= 420:
                #     self.clustering_complete = True
                # self.replicate_clusters_spread_out(target_cluster_size=random.randint(3, 6))
                # self.replicate_clusters_spread_out(target_cluster_size=random.randint(3, 6))
        #         self.apply_continuous_repulsion(min_distance=3.5, strength=6.9)
        #         self.expand_largest_clusters(top_k=6, expansion_strength=0.05)
            

        #         # Trigger final expansion setup at t = 4200 (or nearby)
        new_time_interval = 5

        if int(previous_time // new_time_interval) < int(current_time // new_time_interval) and int(current_time) <= 2200 and len(self.neurons) <= 1017 and len(self.neurons) >= 420:
            # new_time_interval = 5
            previous_time = self.timer.current_time - time_step
            current_time = self.timer.current_time

            # self.replicate_lone_cells_spread_out(target_lone_count=random.randint(2, 4))
            # self.replicate_largest_clusters(target_cluster_size=30, min_dist=3.5, base_jitter=2.0, time_scaling=True, num_clusters=10)
            # self.apply_density_aware_repulsion(min_distance=6.0, base_repulsion=1.8, neighbor_threshold=6)
            # e.g. every frame or every N steps
            self.apply_mechanical_forces(
                    dt=time_step,
                    base_repulsion_k=135.0,
                    base_adhesion_k=111.0,
                    adhesion_dist=1,
                    damping=0.7
                )
            self.replicate_cells_by_density_positive(base_prob=0.15, radius=12.0)

        # Trigger only when crossing an exact multiple of `time_interval`
        # if not hasattr(self, "pre_polarization_scaled"):
            # if int(previous_time // time_interval) < int(current_time // time_interval):
            #     self.replicate_once_in_top_8_clusters(target_cluster_size=45)
            # if int(previous_time // time_interval) < int(current_time // time_interval):
            #     self.replicate_top_10_clusters_stepwise(target_cluster_size=45)
        if len(self.neurons) >= 1017:
            self.apply_sequential_polarity(spread_factor=-0.12, alignment_duration=5.0)
            if self.polarization_complete:
                                    # # After replication steps
                # First, find and sort clusters by size
                clusters = self.find_clusters(distance_threshold=2.5)
                clusters = sorted(clusters, key=len, reverse=True)[:20]
                for cluster in clusters:
                    for neuron in cluster:
                        neuron.cell.sphere.scale([1, 1, 1]) # üîç Adjust scale as needed
                        self.pre_polarization_scaled = True  # ‚úÖ Only scale once
                # self.apply_polarity_establishment_n()
                # self.apply_polarity_to_nearest_100(spread_factor=-0.12, alignment_duration=5.0)
                    # clusters = self.find_clusters(distance_threshold=2.5)
            # # sorted_clusters = sorted(clusters, key=len, reverse=True)[:clusters_to_polarize]
            #         import math

            #         # your action radius (same as in find_clusters)
            #         action_radius = 8.5
            #         # area of that circle
            #         circle_area = math.pi * action_radius**2

            # #         # compute density = #cells / area
            #         clusters_with_density = [(cl, len(cl)/circle_area) for cl in clusters]
            # #         # sort by descending density and take the top N
            #         sorted_clusters = [
            #             cl for cl, dens in sorted(clusters_with_density, key=lambda x: x[1], reverse=True)
            #         ][:10]
            #         cluster_sizes   = [len(cl) for cl in sorted_clusters]
            #         # cluster_centers = [
            #         #     np.mean([n.cell.position for n in cluster if np.all(np.isfinite(n.cell.position))], axis=0)
            #         #     for cluster in sorted_clusters
            #         # ]
            #         cluster_centers = []
            #         for i, cluster in enumerate(sorted_clusters):
            #             pts = [n.cell.position for n in cluster
            #                 if np.all(np.isfinite(n.cell.position)) and len(n.cell.position)==3]
            #             if not pts:
            #                 # no valid positions in this cluster
            #                 continue
            #             center = np.mean(pts, axis=0)
            #             # make sure it's really a 3-vector
            #             if center.shape == (3,) and np.all(np.isfinite(center)):
            #                 cluster_centers.append(center)
            #             else:
            #                 # skip any weird ones
            #                 continue
                # self.mark_cluster_centers(cluster_centers, cluster_sizes, action_radius=2.5)
            # self.apply_mechanical_forces(
            #         dt=time_step,
            #         base_repulsion_k=11135.0,
            #         base_adhesion_k=111.0,
            #         adhesion_dist=1,
            #         damping=0.7
            #     )
                self.plotter.render()
            # self.mark_cluster_centers(cluster_centers, cluster_sizes, action_radius=2.5)
            # self.apply_global_polarity_establishment(scale_dims=(0.7, 0.7, 2.2))
            # self.has_scaled = True

            # Wait 0.3 seconds between each cluster scale
            # if current_time - self.last_scaling_time >= 0.3:
            #     if self.scaling_cluster_index < len(top_10_clusters):
            #         current_cluster = top_10_clusters[self.scaling_cluster_index]
            #         print(f"üü¶ Scaling cluster {self.scaling_cluster_index + 1} at time {current_time:.2f}")

            #         for neuron in current_cluster:
            #             if hasattr(neuron.cell, "sphere"):
            #                 neuron.cell.sphere.scale([0.7, 0.7, 2.2])  # Apply gradual elongation
            #                 neuron.cell.sphere.color((0.5, 0.8, 1.0))  # Light blue

            #         self.scaling_cluster_index += 1
            #         self.last_scaling_time = current_time  # Reset the timer for next cluster
            #     else:
            #         print("‚úÖ All top 10 clusters have been scaled.")
            #         self.scaling_complete = True
            #         # Apply fringe effect only before polarization
            # if hasattr(self, "clusters_spread") and not self.polarization_complete:
            #     self.apply_fringe_effect(self.neurons)

            # # Analyze for rosette only before polarization
            # if not self.polarization_complete and int(self.timer.current_time) % 500 == 0:
            #     rosette, hole_count = analyze_and_visualize_rosette(self.neurons, visualize=True)
            #     print(f"üîç Rosette-like holes detected: {rosette} ({hole_count} persistent H1 features)")


            # # Then, iterate over all neurons in those clusters
            # if self.scaling_cluster_index < len(top_10_clusters):
            #     current_cluster = top_10_clusters[self.scaling_cluster_index]
            #     print(f"üîß Scaling cluster {self.scaling_cluster_index + 1} / {len(top_10_clusters)}")
            
            #     for neuron in current_cluster:
            #         if hasattr(neuron.cell, "sphere"):
            #             neuron.cell.sphere.scale([0.7, 0.7, 2.2])  # Apply gradual elongation
            #             neuron.cell.sphere.color((0.5, 0.8, 1.0))

            #     self.scaling_cluster_index += 1  # Move to the next cluster for next interval
            # else:
            #     print("‚úÖ All top 10 clusters have been scaled.")







            # for neuron in self.neurons:
            #         neuron.cell.sphere.scale([0.8, 0.8, 1.1]) # üîç Adjust scale as needed
            # #     self.pre_polarization_scaled = True  # ‚úÖ Only scale once
            # self.apply_polarity_establishment_n()

                # clusters = self.find_clusters(distance_threshold=4.5)
                # top_clusters = sorted(clusters, key=len, reverse=True)[:6]
                # cluster_centers = [np.mean([n.cell.position for n in cluster], axis=0) for cluster in top_clusters]

                # assigned_neurons = set()
                # excluded_neurons = set()

                # for neuron in self.neurons:
                #     if any(neuron in cluster for cluster in top_clusters):
                #         continue  # already in a top cluster

                #     position = neuron.cell.position
                #     distances = [np.linalg.norm(position - center) for center in cluster_centers]
                #     nearest_cluster_idx = np.argmin(distances)

                #     if distances[nearest_cluster_idx] <= 18.0:
                #         # attract to cluster center
                #         direction = cluster_centers[nearest_cluster_idx] - position
                #         neuron.cell.position += 0.1 * direction  # attraction strength
                #         assigned_neurons.add(neuron)
                #     else:
                #         # check for isolation
                #         is_lone = all(np.linalg.norm(position - other.cell.position) > 2.0 for other in self.neurons if other is not neuron)
                #         if is_lone:
                #             if hasattr(neuron.cell, "sphere"):
                #                 neuron.cell.sphere.color("green")
                #             neuron.excluded_from_dynamics = True
                #             excluded_neurons.add(neuron)

            # # freeze anything else not part of a top cluster
            # for neuron in self.neurons:
            #     if neuron not in assigned_neurons and neuron not in excluded_neurons and not any(neuron in c for c in top_clusters):
            #         if hasattr(neuron.cell, "sphere"):
            #             neuron.cell.sphere.color("green")
            #         neuron.excluded_from_dynamics = True

        # if not self.final_expansion_started and current_time >= 4200:
        #     print("üöÄ Final expansion initialized at t=4200")

        #     # Step 1: Find 6 largest clusters
        #     all_clusters = self.find_clusters(distance_threshold=4.5)
        #     top_clusters = sorted(all_clusters, key=len, reverse=True)[:6]

        #     self.final_expansion_clusters = top_clusters
        #     self.final_expansion_progress = {i: 0 for i in range(6)}  # 0 replicated so far
        #     self.final_expansion_started = True
        # # Perform incremental replication every timestep after 4200
        # if self.final_expansion_started:
        #     for i, cluster in enumerate(self.final_expansion_clusters):
        #         if self.final_expansion_progress[i] < 6:  # Cap at 6 replications per cluster
        #             source_neuron = np.random.choice(cluster)
        #             jitter = np.random.normal(scale=2.0, size=3)
        #             new_pos = source_neuron.cell.position + jitter
        #             new_neuron = self.create_new_neuron(new_pos)
        #             cluster.append(new_neuron)
        #             self.final_expansion_progress[i] += 1
        #             print(f"üÜï Final expansion: cluster {i} now has {len(cluster)} neurons")
        # if current_time >= 4200 and not hasattr(self, "excluded_clusters_flagged"):
        #     all_clusters = self.find_clusters(distance_threshold=4.5)
        #     for cluster in all_clusters:
        #         if len(cluster) < 2:
        #             for neuron in cluster:
        #                 if hasattr(neuron.cell, "sphere"):
        #                     neuron.cell.sphere.color("green")
        #                 neuron.excluded_from_dynamics = True
        #     self.excluded_clusters_flagged = True

            # # Check if clustering is complete
            # avg_distance = np.mean([
            #     np.linalg.norm(n.cell.position - np.mean([m.cell.position for m in self.neurons], axis=0))
            #     for n in self.neurons
            # ])
            # clusters = self.find_clusters(distance_threshold=4.5)
            # avg_cluster_coef = compute_average_cluster_coefficient(clusters)

        #     if avg_cluster_coef > 0.16 and current_time >= 5 and AR == 1:
        #         self.clustering_complete = True
        #         # before_count = len(self.neurons)
        #         # self.remove_outlier_neurons(removal_threshold=3.5, min_cluster_size=20)
        #         # after_count = len(self.neurons)
        #         # removed = before_count - after_count

        #         # print(f"üß† Neurons before: {before_count}, after: {after_count}, removed: {removed}")
        #         print("‚úÖ Clustering complete! Moving to constriction.")

        #              # **New Step: Compact clusters before spreading**
        #         self.compact_clusters_spherically(attraction_strength=-0.1, iterations=20)

        #         # self.replicate_clusters_balanced(target_cluster_size=30)

        # #         # 2Ô∏è‚É£ Gradual spreading after clustering, added after the spread force added to remove_outlier_neurons
        # # if self.clustering_complete and not hasattr(self, "clusters_spread"):
        #         self.apply_cluster_spread(spread_strength=0.49)  # <‚Äî adjust as needed
        #         self.clusters_spread = True  # ‚Üê use a **separate flag**
        #         # self.replicate_clusters_balanced(target_cluster_size=30)
               
        #         # üìä Log current cluster status
        #         clusters = self.find_clusters(distance_threshold=4.5)
        #         print(f"üî¢ Total neurons: {len(self.neurons)}")
        #         for i, cluster in enumerate(clusters):
        #             print(f"üß† Cluster {i+1} size: {len(cluster)}")

        # # # üß¨ Scaling Phase - before polarization starts
        # # if self.clustering_complete and hasattr(self, "clusters_spread") and not hasattr(self, "pre_polarization_scaled"):
        #     print("üì¶ Scaling neurons slightly before polarization...")

        #     for neuron in self.neurons:
        #         if hasattr(neuron.cell, "sphere"):
        #             neuron.cell.sphere.scale([0.9, 0.9, 1.1])  # üîç Adjust scale as needed
        #     self.pre_polarization_scaled = True  # ‚úÖ Only scale once

        # # elif self.clustering_complete and not self.polarization_complete:
        #     print(f"üìå Polarization and Elongation in progress...")    

        # #     # üîÑ **Apply polarity with spreading & gradual alignment**
        #     self.apply_polarity_establishment(spread_factor=0.035, alignment_duration=444.0)
        # #     # self.split_elongated_cells()

        # #     # ‚úÖ **Check if polarization is complete**
        # # if self.polarization_complete:
            
        #     # for neuron in self.neurons:
        #     #     if hasattr(neuron.cell, "sphere"):
        #     #         neuron.cell.sphere.scale([0.8, 0.8, 2.0])  # üîç Adjust scale as needed
        #     # self.pre_polarization_scaled = True  # ‚úÖ Only scale once
        #     print(f"‚úÖ Polarization and Elongation **complete**! Moving to lumen formation.")


        # if self.polarization_complete and not self.elongation_complete:
        #     print(f"üìå Lumenation in progress...")
        #     # self.project_clusters_onto_plane_gradually(projection_duration=2.0)
        #     # self.spread_apart_clusters(spread_distance=30.0, spread_duration=2.0, spread_speed=0.05)
        #     # elongation_factor = 0.025  # Adjust elongation intensity

        #     # self.apply_elongation(elongation_factor=elongation_factor)
            


        # elif self.elongation_complete and not self.lumenation_complete:
        #     print(f"üí° Lumenation in progress...")
        #     # self.apply_lumenation(lumen_factor=0.1)
        #     # # Get the current cluster center
        #     # cluster_center = np.mean([n.cell.position for n in self.neurons], axis=0)

        #     # # Define duration for smooth lumenation
        #     # lumen_duration = 30.0
        #     # current_time = self.timer.current_time

        #     # if not hasattr(self, "lumenation_start_time"):
        #     #     self.lumenation_start_time = current_time  # Set start time

        #     # elapsed_time = current_time - self.lumenation_start_time
        #     # lumen_progress = min(elapsed_time / lumen_duration, 1.0)  # Normalize (0 ‚Üí 1)

        #     # for neuron in self.neurons:
        #     #     direction_from_center = normalize_vector(neuron.cell.position - cluster_center)
                
        #     #     # **Move outward gradually over time**
        #     #     neuron.cell.position += direction_from_center * 0.241 * lumen_progress  

        #     # # ‚úÖ Mark lumenation complete
        #     # if lumen_progress >= 1.0:
        #     #     self.lumenation_complete = True
        #     #     print(f"‚úÖ Lumenation **complete**! Final structure achieved.")













        # """
        # Checks if conditions for rosette formation are met.
        # Returns True if rosette formation is detected, otherwise False.
        # """
        # # Define criteria for rosette formation. For example:
        # # - Check if a certain number of neurons are in close proximity (clustered)
        # # - Check if all neurons have extended a minimum number of neurites
        # # Example: assuming rosette formation means each neuron has at least 2 neurites and forms a cluster
        # cluster_threshold = 3  # Example threshold for cluster size
        # clustered_neurons = [
        #     neuron for neuron in self.neurons
        #     if len(neuron.neurites) >= 2  # Example neurite condition
        #     and self.is_neuron_clustered(neuron, cluster_threshold)  # Check clustering
        # ]
        
        # # If enough neurons meet the criteria, we consider the rosette formed
        # return len(clustered_neurons) >= cluster_threshold

    # def is_neuron_clustered(self, neuron: Neuron, distance_threshold: float) -> bool:
    #     """
    #     Checks if a neuron is within a certain distance of enough neighboring neurons.

    #     Parameters
    #     ----------
    #     neuron : Neuron
    #         The neuron to check for clustering.
    #     distance_threshold : float
    #         The distance threshold for considering a neuron as part of a cluster.

    #     Returns
    #     -------
    #     bool
    #         True if the neuron is clustered with others, False otherwise.
    #     """
    #     nearby_neurons = [
    #         other_neuron for other_neuron in self.neurons
    #         if other_neuron is not neuron
    #         and np.linalg.norm(neuron.cell.position - other_neuron.cell.position) < distance_threshold
    #     ]
    #     return len(nearby_neurons) >= 3  # Example condition: clustered if at least 3 nearby neurons

    # def update_indicator(self):
    #     """Updates the color of the indicator based on rosette formation status."""
    #     if self.rosette_formed:
    #         self.indicator_color = "green"  # Rosette formation detected
    #     else:
    #         self.indicator_color = "blue"  # No rosette formation detected
    #     self.animator.update_indicator(color=self.indicator_color)  # Update indicator in animator

    # def advance_cycles(self, time_step: float) -> None:
    #     """
    #     Updates the biological clocks of every object in the simulation and checks for rosette formation.

    #     Parameters
    #     ----------
    #     time_step
    #         The time between simulation time points.
    #     """
    #     for neuron in self.neurons:
    #         neuron.clocks.advance_clocks(time_step)

    #     # Check rosette formation
    #     if not self.rosette_formed and self.check_rosette_formation():
    #         self.rosette_formed = True  # Update the rosette formation status

    #     # Update the indicator color based on rosette status
    #     self.update_indicator()

    def set_density_check(self, density_check: CellDensityCheck) -> None:
        """
        Sets the contact inhibition function to be used before proliferation.

        Parameters
        ----------
        density_check
            The contact inhibition function to be used.
        """
        self.density_check = density_check

    def register_neuron(self, neuron: Neuron, color="blue") -> None:
        """
        Registers a neuron and its representation into the container.

        Parameters
        ----------
        neuron
            The new neuron to be registered.
        color
            The color of the sphere that will represent the new neuron
            in the renderings of the simulation.
        """
        neuron.cell.set_sphere_representation(self.animator, color=color)
        self.grid.register_cell(neuron.cell)
        for neurite in neuron.neurites:
            neurite.create_neurite_representation(self.animator)
            self.grid.register_neurite(neurite)

        self.neurons.append(neuron)
    


    def update_drawings(self) -> None:
        """Updates the representations of the neurons"""
        for neuron in self.neurons:
            neuron.cell.update_representation()
            for neurite in neuron.neurites:
                neurite.update_representation()

        self.animator.plotter.show()

    # def advance_cycles(self, time_step: float) -> None:
    #     """
    #     Updates the biological clocks of every object in the simulation.

    #     Parameters
    #     ----------
    #     time_step
    #         The time between simulation time points.
    #     """
    #     for neuron in self.neurons:
    #         neuron.clocks.advance_clocks(time_step)

    # def create_new_neuron(
    #     self,
    #     coordinates: Union[np.ndarray, List[float]],
    #     outgrowth_axis: Optional[Union[List[float], np.ndarray]] = None,
    #     color="darkblue",
    # ) -> Neuron:
    def create_new_neuron(self, coordinates: List[float]) -> Neuron:
        if self.simulation_2d:
            coordinates[2] = 0.0
        outgrowth_axis = np.subtract([0,0,0], coordinates)
        """Creates a new neuron at the given coordinates."""
        neuron = self.neuron_factory.create_neuron(coordinates, outgrowth_axis)
        neuron.cell.set_sphere_representation(self.animator, color="green")  # Initial color
        neuron.cell.sphere.scale([0.5, 0.5, 0.5])  # Ensure spherical shape
        self.neurons.append(neuron)
        return neuron
    def create_and_resolve_neuron(self, coordinates: List[float], spring_constant: float = 0.001) -> Neuron:
        """
        Creates a new neuron at the given coordinates, adds spring-based repulsion against existing neurons
        if their X/Y projections overlap, and ensures 2D confinement.

        Parameters
        ----------
        coordinates : List[float]
            Initial [x, y, z] coordinates for the new neuron.
        spring_constant : float
            Hookean spring constant to resolve overlap forces.

        Returns
        -------
        Neuron
            The newly created neuron after repulsion resolution.
        """
        # Flatten to XY plane if in 2D
        if self.simulation_2d:
            coordinates[2] = 0.0

        # Create neuron and set visuals
        neuron = self.neuron_factory.create_neuron(coordinates, np.subtract([0,0,0], coordinates))
        neuron.cell.set_sphere_representation(self.animator, color="blue")
        neuron.cell.sphere.scale([0.5, 0.5, 0.5])

        # Assign physics properties
        neuron.spring_constant = spring_constant
        neuron.radius = getattr(neuron, 'cell_radius', 4.0)

        # Register and append
        self.grid.register_cell(neuron.cell)
        self.neurons.append(neuron)

        # Resolve overlap with existing neurons (axis-aligned X/Y overlap)
        for other in self.neurons:
            if other is neuron:
                continue
            # Compute axis overlaps
            other.radius = getattr(other, 'radius', getattr(other, 'cell_radius', getattr(other.cell, 'radius', 4.0)))
            dx = neuron.cell.position[0] - other.cell.position[0]
            dy = neuron.cell.position[1] - other.cell.position[1]
            overlap_x = (neuron.radius + other.radius) - abs(dx)
            overlap_y = (neuron.radius + other.radius) - abs(dy)
            # If both axes overlap, resolve
            if overlap_x > 0 and overlap_y > 0:
                # Determine repulsion forces along each axis
                fx = spring_constant * overlap_x * np.sign(dx)
                fy = spring_constant * overlap_y * np.sign(dy)
                # Apply half to each neuron
                disp = np.array([fx*0.009, fy*0.009, 0.0])
                neuron.cell.position += disp
                other.cell.position -= disp
        return neuron

        """
        Creates a new neuron and registers it to the container's grid.

        The new neuron is created as an undifferentiated cell body centred at
        the passed coordinates. An outgrowth axis vector can be passed to model
        neurite outgrowth along this direction.

        Parameters
        ----------
        coordinates
            The center position of the neuron's cell body.
        outgrowth_axis
            The direction of growth of the neuron's neurites.
        color
            The color of the new neurite in the simulation renders.
        """
        if isinstance(coordinates, list):
            coordinates = np.array(coordinates)

        if not isinstance(outgrowth_axis, np.ndarray):
            if isinstance(outgrowth_axis, list):
                outgrowth_axis = np.array(outgrowth_axis)
            else:
                outgrowth_axis = get_random_unit_vector(
                    two_dimensions=self.simulation_2d
                )

        new_neuron = self.neuron_factory.create_neuron(coordinates, outgrowth_axis)
        self.register_neuron(new_neuron, color=color)

        return new_neuron
    def replicate_clusters_balanced(self, target_cluster_size=30):
        """
        Replicates neurons in smaller clusters to balance cluster sizes.
        - `target_cluster_size`: Desired number of neurons in each cluster.
        """
        if len(self.neurons) < 2:
            return

        # Find clusters
        clusters = self.find_clusters(distance_threshold=4.5)
        print(f"üîç Found {len(clusters)} clusters for replication check.")

        # Sort clusters by size (ascending to prioritize smaller ones)
        clusters_sorted = sorted(clusters, key=len)

        total_new_neurons = 0

        for cluster in clusters_sorted:
            current_size = len(cluster)
            if current_size >= target_cluster_size:
                continue  # Skip well-populated clusters

            # Compute how many neurons to replicate
            neurons_needed = target_cluster_size - current_size
            print(f"üìà Replicating {neurons_needed} neurons for a cluster of size {current_size}")

            for _ in range(neurons_needed):
                # Randomly pick a neuron from the cluster to replicate
                source_neuron = np.random.choice(cluster)
                jitter = np.random.normal(scale=2.0, size=3)  # Add small spatial noise
                new_position = source_neuron.cell.position + jitter
                if self.simulation_2d:
                    new_position[2] = 0.0
                self.create_new_neuron(new_position)
                total_new_neurons += 1

        print(f"‚úÖ Replicated {total_new_neurons} new neurons across underpopulated clusters.")

    def replicate_largest_clusters(self, target_cluster_size=30, min_dist=3.5, base_jitter=2.0, time_scaling=True, num_clusters=10):
        """
        Replicates one neuron in each of the `num_clusters` largest clusters (if they are below `target_cluster_size`),
        spreading them out and avoiding crowding.

        - `target_cluster_size`: Max size for cluster to continue replicating
        - `min_dist`: Min spacing required to place a new neuron
        - `base_jitter`: Random offset around a source neuron
        - `time_scaling`: Scale jitter with time
        - `num_clusters`: Number of largest clusters to process
        """
        if len(self.neurons) < 2:
            return

        clusters = self.find_clusters(distance_threshold=4.5)
        print(f"üîç Found {len(clusters)} clusters for replication.")

        # Sort by size descending and take top N
        largest_clusters = sorted(clusters, key=len, reverse=True)[:num_clusters]

        for cluster in largest_clusters:
            if len(cluster) >= target_cluster_size:
                continue  # Already full

            source = np.random.choice(cluster)
            jitter_scale = base_jitter + 0.002 * self.timer.current_time if time_scaling else base_jitter

            attempts = 0
            max_attempts = 20
            while attempts < max_attempts:
                jitter = np.random.normal(scale=jitter_scale, size=3)
                jitter[2] = 0.0
                candidate_position = source.cell.position + jitter
                if self.simulation_2d:
                    candidate_position[2] = 0.0
                too_close = any(
                    np.linalg.norm(candidate_position - n.cell.position) < min_dist
                    for n in self.neurons
                )
                if not too_close:
                    new_neuron = self.create_new_neuron(candidate_position)
                    if new_neuron and hasattr(new_neuron.cell, "sphere"):
                        # new_neuron.cell.sphere.scale([0.75, 0.75, 0.75])
                        print(f"‚úÖ New neuron in cluster placed at {candidate_position}, jitter={jitter_scale:.2f}")
                    break
                attempts += 1

            if attempts >= max_attempts:
                print(f"‚ö†Ô∏è Could not place neuron for one of the largest clusters after {max_attempts} attempts.")

    # def replicate_clusters_spread_out(self, target_cluster_size=30, min_dist=3.5, base_jitter=2.0, time_scaling=True):
    #     """
    #     Replicates a single neuron in the smallest cluster with spreading and crowding avoidance.
        
    #     - `target_cluster_size`: Max cluster size to stop replication.
    #     - `min_dist`: Minimum allowed distance between the new neuron and existing ones.
    #     - `base_jitter`: Base amount of randomness for new position.
    #     - `time_scaling`: Whether to increase spread over time.
    #     """
    #     if len(self.neurons) < 2:
    #         return

    #     clusters = self.find_clusters(distance_threshold=4.5)
    #     print(f"üîç Found {len(clusters)} clusters for spread-out replication.")

    #     clusters_sorted = sorted(clusters, key=len)
    #     num_clusters_to_replicate = random.randint(1, 5) #added
    #     clusters_to_replicate = clusters_sorted[:num_clusters_to_replicate] #added

    #     for cluster in clusters_to_replicate: #added
    #         if len(cluster) >= target_cluster_size:
    #             continue

    #         source = np.random.choice(cluster)
    #         jitter_scale = base_jitter + 0.002 * self.timer.current_time if time_scaling else base_jitter

    #         attempts = 0
    #         max_attempts = 20
    #         while attempts < max_attempts:
    #             jitter = np.random.normal(scale=jitter_scale, size=3)
    #             jitter[2] = 0.0  # Stay in 2D
    #             candidate_position = source.cell.position + jitter
    #             if self.simulation_2d:
    #                 candidate_position[2] = 0.0
    #             too_close = any(
    #                 np.linalg.norm(candidate_position - n.cell.position) < min_dist
    #                 for n in self.neurons
    #             )

    #             if not too_close:
    #                 # new_neuron = self.create_new_neuron(candidate_position)
    #                 new_neuron = self.create_and_resolve_neuron(candidate_position, spring_constant=25.0)
    #                 if new_neuron and hasattr(new_neuron.cell, "sphere"):
    #                     new_neuron.cell.sphere.scale([1, 1, 1])
    #                 print(f"‚úÖ New neuron placed at {candidate_position}, jitter={jitter_scale:.2f}")
    #                 return

    #             attempts += 1

    #         print(f"‚ö†Ô∏è Skipped cluster after {max_attempts} failed spacing attempts.")
    #         return
    def replicate_clusters_spread_out(
        self,
        target_cluster_size: int = 30,
        min_dist: float = 3.5,
        base_jitter: float = 2.0,
        time_scaling: bool = True
    ):
        """
        Replicates one neuron in a random 1‚Äì5 of the smallest clusters each cycle,
        spreading them out and avoiding crowding.
        """
        if len(self.neurons) < 2:
            return

        clusters = self.find_clusters(distance_threshold=4.5)
        print(f"üîç Found {len(clusters)} clusters for spread-out replication.")

        # Sort clusters by ascending size
        clusters_sorted = sorted(clusters, key=len)

        # Pick between 1 and 5 smallest clusters this cycle
        import random
        num_to_replicate = random.randint(1, 5)
        clusters_to_replicate = clusters_sorted[:num_to_replicate]
        print(f"üåÄ Replicating in {num_to_replicate} clusters this cycle")

        # For each selected cluster, try to spawn one neuron
        for idx, cluster in enumerate(clusters_to_replicate, start=1):
            size = len(cluster)
            if size >= target_cluster_size:
                print(f"‚è≠Ô∏è Cluster {idx} size {size} ‚â• {target_cluster_size}, skipped")
                continue

            source = random.choice(cluster)
            jitter_scale = base_jitter + (0.002 * self.timer.current_time if time_scaling else 0)

            placed = False
            for attempt in range(20):
                # Candidate position around the source
                offset = np.random.normal(scale=jitter_scale, size=3)
                offset[2] = 0.0
                candidate = source.cell.position + offset
                if self.simulation_2d:
                    candidate[2] = 0.0

                # Check minimum distance
                if all(np.linalg.norm(candidate - n.cell.position) >= min_dist for n in self.neurons):
                    new_neuron = self.create_and_resolve_neuron(candidate, spring_constant=25.0)
                    if hasattr(new_neuron.cell, "sphere"):
                        new_neuron.cell.sphere.scale([1.0, 1.0, 1.0])
                    print(f"‚úÖ Cluster {idx}: placed at {candidate}, jitter={jitter_scale:.2f}")
                    placed = True
                    break

            if not placed:
                print(f"‚ö†Ô∏è Cluster {idx}: failed after 20 attempts")

        # End of function ‚Äî no early return so multiple clusters get processed

    # def replicate_clusters_incrementally(self, target_cluster_size=30):
    #     """
    #     Replicates a single neuron in the most underpopulated cluster (if needed).
    #     - `target_cluster_size`: Desired number of neurons in each cluster.
    #     """
    #     if len(self.neurons) < 2:
    #         return

    #     # Find clusters
    #     clusters = self.find_clusters(distance_threshold=4.5)
    #     print(f"üîç Found {len(clusters)} clusters for incremental replication check.")

    #     # Sort clusters by size (ascending to prioritize smaller ones)
    #     clusters_sorted = sorted(clusters, key=len)

    #     for cluster in clusters_sorted:
    #         current_size = len(cluster)
    #         if current_size >= target_cluster_size:
    #             continue  # Skip large enough clusters

    #         print(f"üìà Replicating 1 neuron for cluster of size {current_size}")
    #         source_neuron = np.random.choice(cluster)
    #         jitter = np.random.normal(scale=2.0, size=3)
    #         new_position = source_neuron.cell.position + jitter
    #         self.create_new_neuron(new_position)
    #         print(f"‚úÖ Replicated 1 neuron in cluster. New total: {len(self.neurons)}")
    #         return  # Stop after replicating one neuron

    #     print("üì≠ No underpopulated cluster found needing replication.")
    def replicate_once_in_top_8_clusters(self, target_cluster_size=30):
        """
        Replicates 1 neuron in each of the top 8 largest clusters (if below the target size).

        Parameters:
        - target_cluster_size (int): Maximum size allowed for a cluster.
        """
        if len(self.neurons) < 2:
            print("‚ö†Ô∏è Not enough neurons to form clusters.")
            return

        # üîç Find and sort clusters by size (descending order)
        clusters = self.find_clusters(distance_threshold=9)
        largest_clusters = sorted(clusters, key=len, reverse=True)[:10]
        print(f"üèÜ Identified {len(largest_clusters)} largest clusters for replication.")

        replicated_count = 0

        for cluster in largest_clusters:
            current_size = len(cluster)
            if current_size >= target_cluster_size:
                continue  # Skip fully populated clusters

            source_neuron = np.random.choice(cluster)
            jitter = np.random.normal(scale=2.0, size=3)
            new_position = source_neuron.cell.position + jitter
            if self.simulation_2d:
                new_position[2] = 0.0

            self.create_new_neuron(new_position)
            replicated_count += 1
            print(f"üß¨ Replicated 1 neuron in cluster of size {current_size} ‚Üí {current_size + 1}")

        if replicated_count == 0:
            print("üì≠ All top clusters already at or above target size.")
        else:
            print(f"‚úÖ Replicated {replicated_count} neurons across top 8 clusters.")

    def replicate_top_10_clusters_stepwise(self, target_cluster_size=30):
        """
        Replicates all needed neurons in the current top-10 cluster (one cluster at a time) if it's below the target size.
        Advances to the next cluster in the top-10 list on each call.
        """
        if len(self.neurons) < 2:
            print("‚ö†Ô∏è Not enough neurons to form clusters.")
            return

        # Step 1: Get clusters once and cache them
        if not hasattr(self, "_top10_clusters"):
            clusters = self.find_clusters(distance_threshold=4.5)
            self._top10_clusters = sorted(clusters, key=len, reverse=True)[:15]
            self._top10_cluster_idx = 0
            print(f"üéØ Cached top 10 clusters for stepwise replication.")

        # Step 2: Check if done
        if self._top10_cluster_idx >= len(self._top10_clusters):
            print("‚úÖ Finished replicating all top 10 clusters.")
            del self._top10_clusters
            del self._top10_cluster_idx
            return

        # Step 3: Replicate current cluster
        cluster = self._top10_clusters[self._top10_cluster_idx]
        current_size = len(cluster)

        if current_size >= target_cluster_size:
            print(f"‚è© Cluster {self._top10_cluster_idx+1} already at size {current_size}. Skipping.")
        else:
            neurons_needed = target_cluster_size - current_size
            print(f"üìà Replicating {neurons_needed} neurons for cluster {self._top10_cluster_idx+1} (size: {current_size})")
            for _ in range(neurons_needed):
                source_neuron = np.random.choice(cluster)
                jitter = np.random.normal(scale=2.0, size=3)
                new_position = source_neuron.cell.position + jitter
                if self.simulation_2d:
                    new_position[2] = 0.0
                self.create_new_neuron(new_position)

        self._top10_cluster_idx += 1
    def replicate_clusters_incrementally_excluding_top_n(self, target_cluster_size=random.randint(4, 8), exclude_top_n=20):
        """
        Replicates 1 neuron incrementally in clusters excluding the top `exclude_top_n` largest ones.

        Parameters:
        - target_cluster_size (int): Desired number of neurons in each cluster.
        - exclude_top_n (int): Number of largest clusters to skip during replication.
        """
        # if len(self.neurons) < 2:
        #     print("‚ö†Ô∏è Not enough neurons to form clusters.")
        #     return

        # Find and sort clusters by size (descending)
        clusters = self.find_clusters(distance_threshold=4.5)
        clusters_sorted_desc = sorted(clusters, key=len, reverse=True)

        # Exclude the top N largest clusters
        clusters_to_check = clusters_sorted_desc[exclude_top_n:]

        print(f"üîç Checking {len(clusters_to_check)} clusters (excluding top {exclude_top_n}) for replication.")

        for cluster in clusters_to_check:
            current_size = len(cluster)
            if current_size >= target_cluster_size:
                continue  # Skip fully populated clusters

            print(f"üìà Replicating 1 neuron for cluster of size {current_size}")
            source_neuron = np.random.choice(cluster)
            jitter = np.random.normal(scale=3.0, size=3)
            new_position = source_neuron.cell.position + jitter
            if self.simulation_2d:
                new_position[2] = 0.0
            self.create_new_neuron(new_position)
            print(f"‚úÖ Replicated 1 neuron. New total: {len(self.neurons)}")
            return  # Stop after one replication

        print("üì≠ No underpopulated clusters (excluding top ones) found for replication.")


    def expand_largest_clusters(self, top_k=6, expansion_strength=0.1, influence_radius=10.0):
        """
        Slightly pushes the top K largest clusters outward from their center after replication.
        
        Parameters:
        - top_k: Number of largest clusters to expand.
        - expansion_strength: Magnitude of outward movement.
        - influence_radius: Only apply expansion to neurons within this radius from cluster center.
        """
        clusters = self.find_clusters(distance_threshold=4.5)

        # Sort clusters by size, descending
        largest_clusters = sorted(clusters, key=len, reverse=True)[:top_k]

        for i, cluster in enumerate(largest_clusters):
            # Compute the center of the cluster
            center = np.mean([n.cell.position for n in cluster], axis=0)

            for neuron in cluster:
                if getattr(neuron, "excluded_from_dynamics", False):
                    continue  # Skip excluded (green) neurons

                # Check if neuron is within influence radius
                dist_to_center = np.linalg.norm(neuron.cell.position - center)
                if dist_to_center > influence_radius:
                    continue

                # Compute outward direction from center
                direction = neuron.cell.position - center
                if np.linalg.norm(direction) > 0:
                    unit_dir = direction / np.linalg.norm(direction)
                    neuron.cell.position += unit_dir * expansion_strength

    def differentiate(self) -> None:
        """Checks for neurons that are flagged for differentiation and deals with differentiation"""
        new_neurons = []
        for neuron in self.neurons:
            if not neuron.ready_for_differentiation:
                neuron.clocks.advance() #fast forwards cell cycle
                continue
            neuron.clocks.differentiation_clock.differentiation_signal = False
            # if (
            #     not neuron.ready_for_differentiation
            #     or len(neuron.neurites) >= neuron.max_number_of_neurites
            # ):
            #     continue
            # # Decide whether to create a new neurite or extend an existing one
            # if neuron.neurites:
            #     neurite = neuron.create_secondary_neurite(self.object_factory)
            #     neurite = neuron.neurites[-1]

            #     nearby_neurites = [
            #         nearby_object
            #         for nearby_object in self.grid.get_close_objects(
            #             neurite.distal_point
            #         )
            #         if isinstance(nearby_object, Neurite)
            #     ]

            #     nearby_neurites = [
            #         neurite
            #         for neurite in nearby_neurites
            #         if neurite not in neuron.neurites
            #     ]

            #     keep_going = True
            #     clear = [False for _ in nearby_neurites]

            #     while not all(clear) and keep_going:
            #         for i, neighbor in enumerate(nearby_neurites):

            #             neurite_axis = neurite.spring_axis

            #             if intersect(
            #                 neurite.proximal_point,
            #                 neurite.distal_point,
            #                 neighbor.proximal_point,
            #                 neighbor.distal_point,
            #             ):
            #                 good_point = get_cylinder_intersection(
            #                     neurite.proximal_point,
            #                     neurite.distal_point,
            #                     neighbor.proximal_point,
            #                     neighbor.distal_point,
            #                 )[0]

            #                 length = np.linalg.norm(
            #                     np.subtract(good_point, neurite.proximal_point)
            #                 )

            #                 if length < 5.0:
            #                     keep_going = False
            #                     neuron.neurites.pop(-1)
            #                     break

            #                 fraction = length / neurite.current_length

            #                 neurite.distal_point = (
            #                     neurite.proximal_point + fraction * neurite_axis
            #                 )
            #                 neurite.mechanics.default_length = np.linalg.norm(
            #                     neurite.spring_axis
            #                 )
            #                 clear[i] = True

            #             else:
            #                 clear[i] = True

            #     if all(clear):
            #         neurite.create_neurite_representation(self.animator)
            #         self.grid.register_neurite(neurite)

            # else:
            #     neuron.create_first_neurite(self.object_factory)
            #     neurite = neuron.neurites[0]

            #     nearby_neurites = [
            #         nearby_object
            #         for nearby_object in self.grid.get_close_objects(
            #             neurite.distal_point
            #         )
            #         if isinstance(nearby_object, Neurite)
            #     ]

            #     keep_going = True
            #     clear = [False for _ in nearby_neurites]

            #     while not all(clear) and keep_going:
            #         for i, neighbor in enumerate(nearby_neurites):

            #             neurite_axis = neurite.spring_axis

            #             if intersect(
            #                 neurite.proximal_point,
            #                 neurite.distal_point,
            #                 neighbor.proximal_point,
            #                 neighbor.distal_point,
            #             ):
            #                 good_point = get_cylinder_intersection(
            #                     neurite.proximal_point,
            #                     neurite.distal_point,
            #                     neighbor.proximal_point,
            #                     neighbor.distal_point,
            #                 )[0]

            #                 length = np.linalg.norm(
            #                     np.subtract(good_point, neurite.proximal_point)
            #                 )

            #                 if length < 5.0:
            #                     keep_going = False
            #                     neuron.neurites.pop(-1)
            #                     break

            #                 fraction = length / neurite.current_length

            #                 neurite.distal_point = (
            #                     neurite.proximal_point + fraction * neurite_axis
            #                 )
            #                 neurite.mechanics.default_length = np.linalg.norm(
            #                     neurite.spring_axis
            #                 )
            #                 clear[i] = True

            #             else:
            #                 clear[i] = True

            #     if all(clear):
            #         neurite.create_neurite_representation(self.animator)
            #         self.grid.register_neurite(neurite)

            # neuron.clocks.differentiation_clock.differentiation_signal = False

    def kill(self) -> None:
        """
        Removes neurons flagged for death and their visual + grid representations.
        """
        neurons_to_kill = [n for n in self.neurons if getattr(n, "ready_to_die", False)]

        for neuron in neurons_to_kill:
            if hasattr(neuron.cell, "sphere"):
                self.animator.plotter -= neuron.cell.sphere

            for neurite in neuron.neurites:
                for cylinder in neurite.cylinder:
                    self.animator.plotter -= cylinder

            self.neurons.remove(neuron)

        if neurons_to_kill:
            print(f"üíÄ Removed {len(neurons_to_kill)} neurons marked for death.")

   
   
   
   
    # def kill(self) -> None:
    #     """Checks for neurons that are flagged for death and removes them from the container"""
    #     initial_count = len(self.neurons)
    #     killed_count = 0
    #     for neuron in self.neurons[:]:
    #         if not neuron.ready_to_die:
    #             continue
    #         # Remove neuron and its representation
    #         self.animator.plotter -= neuron.cell.sphere
    #         for neurite in neuron.neurites:
    #             self.animator.plotter -= neurite.cylinder[0]
    #             self.animator.plotter -= neurite.cylinder[1]
    #         self.neurons.remove(neuron)
    #         killed_count += 1
    #     if killed_count > 0:
    #         print(f"üíÄ Killed {killed_count} neurons. Remaining: {len(self.neurons)}")

    def divide(self) -> None:
        """
        Handles neuron division logic, creating new neurons while maintaining constraints
        on density and cell cycle state. Now optimized for frequent division.
        """
        new_neurons = []
        division_count = 0

        for neuron in self.neurons:
            # Initialize or decrement time_to_division
            if not hasattr(neuron.clocks.cycle_clock, "time_to_division"):
                neuron.clocks.cycle_clock.time_to_division = 50  # LOWERED from 100
            else:
                neuron.clocks.cycle_clock.time_to_division -= 5  # FASTER progression

            if neuron.clocks.cycle_clock.time_to_division > 0:
                continue  # Still not ready

            if not neuron.ready_for_division:
                continue

            if self.density_check:
                if self.density_check.check_max_density(neuron.cell, self.grid):
                    print("‚ö†Ô∏è Skipping division due to density constraint")
                    neuron.clocks.cycle_clock.remove_flag()
                    neuron.clocks.cycle_clock.trigger_block()
                    continue

            # Calculate position for new neuron
            spread_factor = np.random.uniform(4.0, 8.0)
            direction_vector = get_random_unit_vector(two_dimensions=self.simulation_2d)
            new_position = neuron.cell.position + (direction_vector * neuron.cell_radius * spread_factor)

            # Ensure no overlap with neighbors
            while any(np.linalg.norm(new_position - other.cell.position) < 2.5 * neuron.cell_radius for other in self.neurons):
                direction_vector = get_random_unit_vector(self.simulation_2d)
                new_position = neuron.cell.position + (direction_vector * neuron.cell_radius * spread_factor)

            # Create and configure new neuron
            new_neuron = self.create_new_neuron(new_position)
            new_neuron.cell.sphere.scale([1.0, 1.0, 1.0])
            new_neuron.cell.sphere.color("limegreen")
            new_neuron.division_born_time = self.timer.current_time

            # ‚úÖ Add new division clock to child
            new_neuron.clocks.cycle_clock.time_to_division = 50

            jitter = np.random.uniform(-0.5, 0.5, size=3)
            new_neuron.cell.position += jitter

            new_neurons.append(new_neuron)
            neuron.clocks.cycle_clock.remove_flag()
            division_count += 1

        if new_neurons:
            print(f"üß¨ {division_count} neurons divided. Total: {len(self.neurons) + len(new_neurons)}")
            self.neurons.extend(new_neurons)
            self.update_drawings()


    # def divide(self) -> None:
    #     """
    #     Handles neuron division logic, creating new neurons while maintaining constraints
    #     on density and cell cycle state. Adds visual feedback for division.
    #     """
    #     new_neurons = []
    #     division_count = 0

    #     for neuron in self.neurons:
    #         if not neuron.ready_for_division:
    #             if not hasattr(neuron.clocks.cycle_clock, "time_to_division"):
    #                 neuron.clocks.cycle_clock.time_to_division = 100
    #             neuron.clocks.cycle_clock.time_to_division -= 10
    #             continue

    #         if self.density_check:
    #             if self.density_check.check_max_density(neuron.cell, self.grid):
    #                 neuron.clocks.cycle_clock.remove_flag()
    #                 neuron.clocks.cycle_clock.trigger_block()
    #                 continue

    #         spread_factor = np.random.uniform(5.0, 10.0)
    #         direction_vector = get_random_unit_vector(two_dimensions=self.simulation_2d)
    #         new_position = neuron.cell.position + (direction_vector * neuron.cell_radius * spread_factor)

    #         while any(np.linalg.norm(new_position - other.cell.position) < 2.5 * neuron.cell_radius for other in self.neurons):
    #             direction_vector = get_random_unit_vector(self.simulation_2d)
    #             new_position = neuron.cell.position + (direction_vector * neuron.cell_radius * spread_factor)

    #         new_neuron = self.create_new_neuron(new_position)
    #         new_neuron.cell.sphere.scale([1.0, 1.0, 1.0])

    #         jitter = np.random.uniform(-0.5, 0.5, size=3)
    #         new_neuron.cell.position += jitter

    #         # üåü Visual Feedback: flash green when created
    #         new_neuron.cell.sphere.color("limegreen").alpha(1.0)

    #         # Optional: store creation time to fade color later
    #         new_neuron.division_born_time = self.timer.current_time

    #         new_neurons.append(new_neuron)
    #         division_count += 1
    #         neuron.clocks.cycle_clock.remove_flag()

    #     self.neurons.extend(new_neurons)
    #     self.update_drawings()

    #     if division_count > 0:
    #         print(f"üß¨ {division_count} neurons divided. Total neurons: {len(self.neurons)}")

    # def divide(self) -> None:
    #     """
    #     Handles neuron division logic, creating new neurons while maintaining constraints
    #     on density and cell cycle state.
    #     """
    #     new_neurons = []
    #     for neuron in self.neurons:
    #         if not neuron.ready_for_division:
    #             # Decrement or initialize division timer safely
    #             if not hasattr(neuron.clocks.cycle_clock, "time_to_division"):
    #                 neuron.clocks.cycle_clock.time_to_division = 100  # Default threshold

    #             neuron.clocks.cycle_clock.time_to_division -= 10
    #             continue


    #         if self.density_check:
    #             # Check if the maximum density is reached
    #             if self.density_check.check_max_density(neuron.cell, self.grid):
    #                 # Prevent division and block further cycling
    #                 neuron.clocks.cycle_clock.remove_flag()
    #                 neuron.clocks.cycle_clock.trigger_block()
    #                 continue
    #         # Spread new neurons outward with a random direction and increased spacing
    #         spread_factor = np.random.uniform(5.0, 10.0)  # Increase for more spreading
    #         direction_vector = get_random_unit_vector(two_dimensions=self.simulation_2d)

    #         new_position = neuron.cell.position + (direction_vector * neuron.cell_radius * spread_factor)

    #         # Ensure new cell is not too close to existing ones
    #         while any(np.linalg.norm(new_position - other.cell.position) < 2.5 * neuron.cell_radius for other in self.neurons):
    #             new_position = neuron.cell.position + (get_random_unit_vector(self.simulation_2d) * neuron.cell_radius * spread_factor)

    #         # Create new neuron at spread-out position
    #         new_neuron = self.create_new_neuron(new_position)
    #         new_neuron.cell.sphere.scale([1.0, 1.0, 1.0])  # Keep new cells spherical

    #         # Apply slight random jitter to avoid rigid structures
    #         jitter = np.random.uniform(-0.5, 0.5, size=3)
    #         new_neuron.cell.position += jitter

    #         new_neurons.append(new_neuron)  # Add to the list of new neurons

    #         # Remove flag for next division cycle
    #         neuron.clocks.cycle_clock.remove_flag()

    #     # Extend the neuron list only after loop completes
    #     self.neurons.extend(new_neurons)
    #     self.update_drawings()  # Ensure visualization updates


            #     else:
            #         # Create a new neuron nearby
            #         position = (
            #             get_random_unit_vector(two_dimensions=self.simulation_2d)
            #             * neuron.cell_radius
            #             * np.random.uniform(2.0,3.5)  # Reduced spacing for tighter clusters
            #         )
            #         position += neuron.cell.position
            #         new_neuron = self.create_new_neuron(position)
            #         new_neuron.cell.sphere.scale([1.0, 1.0, 1.0])  # Ensure spherical shape

            #         # Apply a small random jiggle after division
            #         jiggle_vector = np.random.uniform(-0.1, 0.1, size=3)
            #         new_neuron.cell.position += jiggle_vector   

            #         # Update the original neuron's cycle state
            #         neuron.clocks.cycle_clock.remove_flag()
            #         self.update_drawings()
            # else:
            #     # Create a new neuron nearby without density check
            #     position = (
            #         get_random_unit_vector(two_dimensions=self.simulation_2d)
            #         * neuron.cell_radius
            #         * np.random.uniform(2.0,3.5)  # Reduced spacing for tighter clusters
            #     )
            #     position += neuron.cell.position
            #     new_neuron = self.create_new_neuron(position)
            #     new_neuron.cell.sphere.scale([1.0, 1.0, 1.0])  # Ensure spherical shape

            #     # Apply a small random jiggle after division
            #     jiggle_vector = np.random.uniform(-0.1, 0.1, size=3)
            #     new_neuron.cell.position += jiggle_vector 

            #     # Update the original neuron's cycle state
            #     neuron.clocks.cycle_clock.remove_flag()
            #     self.update_drawings()


    def get_displacement_from_force(
        self, force: np.ndarray, time_step: float
    ) -> np.ndarray:
        """
        Returns the displacemnt value that a force originates, based on the equation of motion.

        Parameters
        ----------
        force
            The force value to be converted to a displacement
        time_step
            The time passed between simulation time points.
        """
        velocity = force / self.drag_coefficient
        return velocity * time_step

    def move_cell(
        self, neuron: Neuron, new_coordinates: Union[np.ndarray, List[float]]
    ) -> None:
        """
        Moves the cell to a new position and updates the proximal point of the first neurite.

        Parameters
        ----------
        neuron
            The neuron object to be moved.
        new_coordinates
            The new coordinates to be assigned to the cell body's centre.
        """
        if isinstance(new_coordinates, list):
            new_coordinates = np.array(new_coordinates)

        self.grid.remove_cell(neuron.cell)
        neuron.cell.set_center_position(new_coordinates)
        self.grid.register_cell(neuron.cell)

        if neuron.neurites:
            neuron.place_neurite_on_cell_surface(neuron.neurites[0])

    def move_neurite(self, neurite: Neurite, new_coordinates: np.ndarray) -> None:
        """
        Deals with moving a neurite's distal point and updating it on the grid.

        Parameters
        ----------
        neurite
            The neurite object to be moved.
        new_coordinates
            The new coordinates to be assigned to the neurite's distal point.
        """
        self.grid.remove_neurite(neurite)
        neurite.move_distal_point(new_coordinates)
        self.grid.register_neurite(neurite)

    def compute_displacements(self, time_step) -> None:
        """
        Computes the displacement for each object based on the resulting force.

        Parameters
        ----------
        time_step
            The time passed between simulation time points.
        """
        for i, neuron in enumerate(self.neurons):
            reversed_order = range(len(neuron.neurites) - 1, -1, -1)

            for j, neurite in zip(reversed_order, reversed(neuron.neurites)):

                # Get force from spring
                force_spring = neurite.get_spring_force()
                neurite.force += force_spring
                # Transmit the opposite force to the mother neurite/cell
                # (Going through the neurites in reverse, once we arrive at 0 it is the last)
                if j > 0:
                    # Transmit to the mother neurite
                    neuron.neurites[j - 1].force_from_daughter -= force_spring
                else:
                    # Transmit to the cell
                    neuron.cell.force_from_daughter -= force_spring

                # Get force from daughter
                # Will contain force from spring and object interactions (the mother fraction)
                neurite.force += neurite.force_from_daughter

                # Get objects in the surrounding voxels
                nearby_objects = self.grid.get_close_objects(neurite.distal_point)
                nearby_cells = [
                    nearby_object
                    for nearby_object in nearby_objects
                    if isinstance(nearby_object, CellBody)
                ]
                nearby_neurites = [
                    nearby_object
                    for nearby_object in nearby_objects
                    if isinstance(nearby_object, Neurite)
                ]

                # Get forces from neighbor cells
                for neighbor in nearby_cells:
                    if neighbor is neuron.cell:
                        continue

                    # Cell force and fraction to be transmitted to the distal point
                    cell_force, fraction = neurite.get_cell_neighbor_force(
                        neighbor, self.sphere_cylinder_int
                    )

                    # Apply force to the distal point
                    neurite.force += cell_force * fraction
                    # Transmit force to the neighbor
                    neighbor.force_from_neighbors -= cell_force

                    # Transmit the force from cell to proximal part of the neurite
                    # (Going through the neurites in reverse, once we arrive at 0 it is the last)
                    if j > 0:
                        neuron.neurites[j - 1].force_from_daughter += cell_force * (
                            1 - fraction
                        )
                    else:
                        neuron.cell.force_from_daughter += cell_force * (1 - fraction)

                # Get forces from neighbor neurites
                for neighbor in nearby_neurites:
                    if neighbor in neuron.neurites:
                        continue

                    neurite_force, fraction = neurite.get_neurite_neighbor_force(
                        neighbor, self.cylinder_int
                    )
                    neurite.force += neurite_force * fraction

                    # Transmit the force from cell to proximal part of the neurite
                    # (Going through the neurites in reverse, once we arrive at 0 it is the last)
                    if j > 0:
                        neuron.neurites[j - 1].force_from_daughter += neurite_force * (
                            1 - fraction
                        )
                    else:
                        neuron.cell.force_from_daughter += neurite_force * (
                            1 - fraction
                        )

            # Get cell bodies close to the cell
            nearby_objects = self.grid.get_close_objects(neuron.cell.position)
            nearby_cells = [
                nearby_object
                for nearby_object in nearby_objects
                if isinstance(nearby_object, CellBody)
            ]

            for neighbor in nearby_cells:
                if neuron.cell is neighbor:
                    continue
                neuron.cell.force += neuron.cell.get_neighbor_force(
                    neighbor, self.sphere_int
                )

        for i, neuron in enumerate(self.neurons):
            for j, neurite in enumerate(neuron.neurites):
                neurite.force += neurite.force_from_daughter
                displacement = self.get_displacement_from_force(
                    neurite.force, time_step
                )
                self.neurons[i].neurites[j].displacement = displacement

            # Add the forces that were already calculated from other neurites
            neuron.cell.force += neuron.cell.force_from_daughter
            neuron.cell.force += neuron.cell.force_from_neighbors

            # Convert force value to displacement to assign new position
            displacement = self.get_displacement_from_force(
                neuron.cell.force, time_step
            )
            neuron.cell.displacement = displacement

    def update_cell_positions(self) -> None:
        """Updates the positions of all the simulation objects based on their velocity."""
        for neuron in self.neurons:
            neuron.cell.force = np.zeros(3)
            neuron.cell.force_from_neighbors = np.zeros(3)
            neuron.cell.force_from_daughter = np.zeros(3)

            for j, neurite in enumerate(neuron.neurites):
                neurite.force = np.zeros(3)
                neurite.force_from_daughter = np.zeros(3)
                self.move_neurite(neurite, neurite.distal_point + neurite.displacement)

                if j < len(neuron.neurites) - 1:
                    neuron.neurites[j + 1].move_proximal_point(
                        neuron.neurites[j].distal_point
                    )

            # Update the proximal position of the first neurite
            self.move_cell(neuron, neuron.cell.position + neuron.cell.displacement)
            # Flatten Z after movement
            neuron.cell.position[2] = 0.0
            if hasattr(neuron.cell, "sphere"):
                neuron.cell.sphere.pos(neuron.cell.position)


    def solve_mechanics(self, time_step) -> None:
        """
        Solves the mechanical interactions and updates the neurons' positions.

        Goes through each object and computes the resulting force acting on
        it, then gets the object's velocity based on the equation of motion.
        When all of the objects are checked, the positions are updated based
        on the calculated velocity.

        Parameters
        ----------
        time_step
            The time passed between simulation time points.
        """
        self.compute_displacements(time_step * 2)
        self.update_cell_positions()



class Simulation:
    """
    Class to create and run a simulation.

    Parameters
    ----------
    timer
        The structure to store the time data of the simulation.
    container
        The structure to store the spatial data of the simulation.
    """
    directory = r"C:\Users\16785\Desktop\neurorosette_code\tests\sim_images_newest"
    os.makedirs(directory, exist_ok=True)

    def __init__(self, timer: Timer, container: SimulationContainer):
        self.timer = timer
        self.container = container
        self.grid = container.grid
        self.neruon_factory = container.neuron_factory
        self.contact_factory = container.contact_factory

    def run(self, total_steps: int = 500) -> None:
        """Runs the entire simulation and ensures frames are saved correctly."""
        frame_count = 0
        
        # Ensure the correct save directory
        directory = r'C:\Users\16785\Desktop\neurorosette_code\tests\sim_images_newest'
        os.makedirs(directory, exist_ok=True)  # Force folder creation

        # üî¥ PRINT DEBUG INFO BEFORE STARTING
        print("\n‚úÖ DEBUG: Checking File Save Path")
        print(f"   Current Working Directory: {os.getcwd()}")
        print(f"   Target Save Directory: {directory}")

        # üî¥ TEST WRITING TO DIRECTORY
        test_file_path = os.path.join(directory, "debug_test.txt")
        try:
            with open(test_file_path, 'w') as f:
                f.write("Test Write Success!")
            print(f"‚úÖ SUCCESS: Test file written to {test_file_path}")
            os.remove(test_file_path)  # Cleanup
        except Exception as e:
            print(f"‚ùå ERROR: Cannot write to directory. Check folder permissions! {str(e)}")
            return  # Stop execution if folder is not writable

        print("\nüöÄ Starting simulation loop...")
        
        for step in range(total_steps):
            self.container.advance_cycles(self.timer.step)
            frame_count += 1

            if frame_count % 5 == 0:  # Save every 5 frames
                frame_path = os.path.abspath(os.path.join(directory, f'frame_{frame_count:05d}.png'))

                # üî¥ PRINT DEBUG INFO BEFORE SAVING
                print(f"\nüì∏ Attempting to save frame {frame_count} to: {frame_path}")

                try:
                    self.container.animator.plotter.render()  # Force render before saving
                    self.container.animator.save_screenshot(frame_path)

                    # üî¥ VERIFY IF FILE EXISTS AFTER SAVING
                    if os.path.exists(frame_path):
                        print(f"‚úÖ SUCCESS: Frame {frame_count} saved at {frame_path}")
                    else:
                        print(f"‚ùå ERROR: Frame {frame_count} was NOT saved. File missing!")

                except Exception as e:
                    print(f"‚ùå ERROR: Failed to save frame {frame_count}: {str(e)}")

        print("\nüéâ Simulation complete! Checking final saved files...")
        try:
            saved_files = os.listdir(directory)
            print(f"üìÇ Final contents of {directory}: {saved_files}")
        except Exception as e:
            print(f"‚ùå ERROR: Could not list directory contents: {str(e)}")


  


    def save_meshes(self, file_name: str) -> None:
        """
        Saves the neurons as PLY objects. Cell bodies are saved as spheres.
        Neurites are saved as cylinders.
        """
        # Save the cell bodies as one mesh (spheres)
        meshes = merge([neuron.cell.sphere for neuron in self.container.neurons])
        
        # Save the neurites as one mesh (cylinders)
        cylinders = []

        for neuron in self.container.neurons:
            for neurite in neuron.neurites:
                # Create a cylinder from the neurite's geometry
                cylinder = Cylinder(pos=neurite.proximal_point+0.5*neurite.spring_axis, 
                                    height=neurite.current_length, 
                                    axis=neurite.spring_axis/neurite.current_length,
                                    r=neurite.mechanics.radius)
                cylinders.append(cylinder)

        cylinder_meshes = merge(cylinders)

        # Save the result
        write(meshes, f"{file_name}_cells.ply")
        if cylinder_meshes:
            write(cylinder_meshes, f"{file_name}_neurites.ply")

    @classmethod
    def from_file(cls, config_path: Union[Path, str]) -> "Simulation":
        """
        Initializes a Simulation object from a YAML config file.

        Parameters
        ----------
        config_path : str or Path
            The path to the YAML file config file.

        Returns
        -------
        Simulation
            The initialized simulation object.
        """
        if not isinstance(config_path, Path):
            config_path = Path(config_path)

        print(f"config path loaded: {config_path}")  # Debugging output

        parser = ConfigParser(config_path)
        domain_data = parser.get_domain_data()
        if "boundaries" not in domain_data:
            raise KeyError("The 'boundaries' key is misising in configuration")
        
        grid_data = {
            "boundaries": domain_data["boundaries"],
            "step": domain_data["boundaries"]["step"]
        }
        print(f"DEBUG: Grid initialized with data -> {grid_data}")  # Debugging line

        grid = UniformGrid(boundaries=grid_data["boundaries"], step=grid_data["step"])

        timer = Timer(**parser.get_time_data())

        # # Ensure 'boundaries' exists
        # if "boundaries" not in domain_data:
        #     raise KeyError("The 'boundaries' key is missing in the configuration file. Check config.yml.")

        # boundaries = {"min": domain_data["boundaries"]["min"], "max": domain_data["boundaries"]["max"]}

        # grid = UniformGrid(boundaries, step=domain_data["step"])  # Corrected

        status_2d = parser.get_2d_status()
        drag = parser.get_drag_coefficient()

        number_of_neurites = parser.get_max_number_of_neurites()
        objects = ObjectFactory(**parser.get_objects_data())
        clocks = ClocksFactory(**parser.get_clocks_data())

        interactions_data = parser.get_interactions_data()
        interactions_type = interactions_data.pop("type")
        if interactions_type == "potentials":
            interactions = PotentialsFactory(**interactions_data)
        else:
            interactions = SimpleFactory(**interactions_data)
        plotter = Plotter()
        container = SimulationContainer(
            grid=grid,
            simulation_2d=status_2d,
            neuron_factory=NeuronFactory(number_of_neurites, objects, clocks),
            contact_factory=interactions,
            timer=timer,
            drag_coefficient=drag,
            plotter=Plotter() 
        )

        
        return cls(timer, container)


import yaml  # Ensure YAML is imported
from typing import Union

class ConfigParser:
    """Parses configuration files to load simulation parameters."""

    def __init__(self, config_path: Union[Path,str]):
        """Loads YAML config file into a dictionary."""
        with open(config_path, "r") as file:
            self.config = yaml.safe_load(file)

        print(f"DEBUG: Loaded YAML configuration")



    def get_domain_data(self):
        """Retrieves domain (grid) configuration from the YAML file and ensures 'step' is inside 'boundaries'."""
        
        # Load the domain section from the config file
        domain_data = self.config.get("domain", {})

        # Debugging: Print domain data
        print(f"DEBUG: domain_data -> {domain_data}")

        # Ensure 'boundaries' exists
        if "boundaries" not in domain_data:
            raise KeyError("The 'boundaries' key is missing in the configuration file. Check config.yml.")

        # Extract boundaries data
        boundaries = domain_data["boundaries"]

        # Ensure 'step' is inside 'boundaries'
        if "step" not in boundaries:
            boundaries["step"] = domain_data.get("step", 20.0)  # Move step inside boundaries if missing

        # Return the corrected structure
        return {"boundaries": boundaries}



    def get_time_data(self):
        """Retrieves simulation time settings."""
        return self.config.get("time", {"total_time": 100000, "step": 0.3})

    def get_2d_status(self):
        """Retrieves whether the simulation should run in 2D."""
        return self.config.get("simulation_2d", True)

    def get_drag_coefficient(self):
        """Retrieves drag coefficient settings."""
        return self.config.get("drag_coefficient", 10.0)

    def get_max_number_of_neurites(self):
        """Retrieves max neurites allowed per neuron."""
        return self.config.get("neurons", {}).get("max_number_of_neurites", 3)

    def get_objects_data(self):
        """Retrieves object properties."""

        objects_data = self.config.get("objects", {})

        return {
        "cell_radius": objects_data.get("cell_radius", 7.0),
        "cell_interaction_factor": objects_data.get("cell_interaction_factor", 1.0),
        "neurite_radius": objects_data.get("neurite_radius", 0.5),
        "neurite_interaction_factor": objects_data.get("neurite_interaction_factor", 0.8),
        "neurite_spring_constant": objects_data.get("neurite_spring_constant", 10.0),
        "neurite_default_length": objects_data.get("neurite_default_length", 20.0),
        }

    def get_clocks_data(self):
        """Retrieves clock properties for neuron proliferation, death, and differentiation rates."""
        clocks_data = self.config.get("clocks", {})

        return {
            "proliferation_rate": clocks_data.get("proliferation_rate", 1.5),  # Default: slow division
            "death_rate": clocks_data.get("death_rate", 0.0),  # Default: No death
            "differentiation_rate": clocks_data.get("differentiation_rate", 0.4),  # Default: slow differentiation
        }


    def get_interactions_data(self):
        """Retrieves interaction settings for neuron mechanics."""
        return self.config.get("interactions", {})
    
    def set(self, key: str, value):
        keys = key.split(".")
        config_section = self.config
        for k in keys[:-1]:
            config_section = config_section[k]
        config_section[keys[-1]] = value

    def save(self, config_path: Union[str, Path]):
        if not isinstance(config_path, Path):
            config_path = Path(config_path)
        with open(config_path, "w") as file:
            yaml.safe_dump(self.config, file)


# class ConfigParser:
#     def __init__(self, config_path: Union[str, Path]):
#         if not isinstance(config_path, Path):
#             config_path = Path(config_path)
#         with open(config_path, "r") as file:
#             self.config = yaml.safe_load(file)

#     def get_time_data(self):
#         return self.config["time"]

#     def get_domain_data(self):
#         return self.config["domain"]

#     def get_2d_status(self):
#         return self.config["domain"].get("use_2d", True)

#     def get_drag_coefficient(self):
#         return self.config["domain"].get("drag_coefficient", 10.0)

#     def get_max_number_of_neurites(self):
#         return self.config["neurons"].get("max_number_of_neurites", 3)

#     def get_objects_data(self):
#         return self.config["neurons"].get("objects", {})

#     def get_clocks_data(self):
#         return self.config["neurons"].get("clocks", {})

#     def get_interactions_data(self):
#         return self.config["interactions", {}]

    # def set(self, key: str, value):
    #     keys = key.split(".")
    #     config_section = self.config
    #     for k in keys[:-1]:
    #         config_section = config_section[k]
    #     config_section[keys[-1]] = value

    # def save(self, config_path: Union[str, Path]):
    #     if not isinstance(config_path, Path):
    #         config_path = Path(config_path)
    #     with open(config_path, "w") as file:
    #         yaml.safe_dump(self.config, file)

# Adjust configuration for testing rosette formation
CONFIG_PATH = r'C:\Users\16785\Desktop\neurorosette_code\config.yml'
parser = ConfigParser(CONFIG_PATH)
parser.set("neurons.clocks.proliferation_rate", 0.001)
parser.set("neurons.clocks.differentiation_rate", 0.001)
parser.set("interactions.sphere_sphere_adhesion", 10.0)
parser.set("time.total_time", 100000.0)  # Increased total simulation time
parser.save(CONFIG_PATH)

# Run simulation to validate indicator functionality
simulation = Simulation.from_file(CONFIG_PATH)
simulation.run()



#    def __init__(
#         self,
#         grid: UniformGrid,
#         simulation_2d: bool,
#         neuron_factory: NeuronFactory,
#         contact_factory: ContactFactory,
#         drag_coefficient: float = 10.0,
#         density_check: Optional[CellDensityCheck] = None,
#     ) -> None:

#         self.grid = grid
#         self.simulation_2d = simulation_2d
#         self.sphere_int = contact_factory.get_sphere_sphere_interactions()
#         self.sphere_cylinder_int = contact_factory.get_sphere_cylinder_interactions()
#         self.cylinder_int = contact_factory.get_cylinder_cylinder_interactions()
#         self.neuron_factory = neuron_factory
#         self.object_factory = self.neuron_factory.objects_factory
#         self.drag_coefficient = drag_coefficient
#         self.density_check = density_check
#         self.animator = Animator()
#         self.neurons = []

#         if self.simulation_2d:
#             self.animator.add_grid(
#                 self.grid.representation_grid_values,
#                 self.grid.representation_grid_values,
#             )






